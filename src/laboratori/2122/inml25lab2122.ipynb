{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c2e1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only in docker\n",
    "%cd ~/src/laboratori/2122"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e4429e",
   "metadata": {},
   "source": [
    "# **Introduzione al Machine Learning -- laboratorio 19 e 20**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a854eaf",
   "metadata": {},
   "source": [
    "- Regressione logistica\n",
    "- Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44d5194f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73160b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Biacromial diameter (see Fig. 2)</th>\n",
       "      <th>Biiliac diameter, or \"pelvic breadth\" (see Fig. 2)</th>\n",
       "      <th>Bitrochanteric diameter (see Fig. 2)</th>\n",
       "      <th>Chest depth between spine and sternum at nipple level,</th>\n",
       "      <th>Chest diameter at nipple level, mid-expiration</th>\n",
       "      <th>Elbow diameter, sum of two elbows</th>\n",
       "      <th>Wrist diameter, sum of two wrists</th>\n",
       "      <th>Knee diameter, sum of two knees</th>\n",
       "      <th>Ankle diameter, sum of two ankles</th>\n",
       "      <th>Shoulder girth over deltoid muscles</th>\n",
       "      <th>...</th>\n",
       "      <th>Bicep girth, flexed, average of right and left girths</th>\n",
       "      <th>Forearm girth, extended, palm up, average of right and</th>\n",
       "      <th>Knee girth over patella, slightly flexed position, average</th>\n",
       "      <th>Calf maximum girth, average of right and left girths</th>\n",
       "      <th>Ankle minimum girth, average of right and left girths</th>\n",
       "      <th>Wrist minimum girth, average of right and left girths</th>\n",
       "      <th>Age (years)</th>\n",
       "      <th>Weight (kg)</th>\n",
       "      <th>Height (cm)</th>\n",
       "      <th>Gender (1 - male, 0 - female)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42.9</td>\n",
       "      <td>26.0</td>\n",
       "      <td>31.5</td>\n",
       "      <td>17.7</td>\n",
       "      <td>28.0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>10.4</td>\n",
       "      <td>18.8</td>\n",
       "      <td>14.1</td>\n",
       "      <td>106.2</td>\n",
       "      <td>...</td>\n",
       "      <td>32.5</td>\n",
       "      <td>26.0</td>\n",
       "      <td>34.5</td>\n",
       "      <td>36.5</td>\n",
       "      <td>23.5</td>\n",
       "      <td>16.5</td>\n",
       "      <td>21</td>\n",
       "      <td>65.6</td>\n",
       "      <td>174.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>43.7</td>\n",
       "      <td>28.5</td>\n",
       "      <td>33.5</td>\n",
       "      <td>16.9</td>\n",
       "      <td>30.8</td>\n",
       "      <td>14.0</td>\n",
       "      <td>11.8</td>\n",
       "      <td>20.6</td>\n",
       "      <td>15.1</td>\n",
       "      <td>110.5</td>\n",
       "      <td>...</td>\n",
       "      <td>34.4</td>\n",
       "      <td>28.0</td>\n",
       "      <td>36.5</td>\n",
       "      <td>37.5</td>\n",
       "      <td>24.5</td>\n",
       "      <td>17.0</td>\n",
       "      <td>23</td>\n",
       "      <td>71.8</td>\n",
       "      <td>175.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40.1</td>\n",
       "      <td>28.2</td>\n",
       "      <td>33.3</td>\n",
       "      <td>20.9</td>\n",
       "      <td>31.7</td>\n",
       "      <td>13.9</td>\n",
       "      <td>10.9</td>\n",
       "      <td>19.7</td>\n",
       "      <td>14.1</td>\n",
       "      <td>115.1</td>\n",
       "      <td>...</td>\n",
       "      <td>33.4</td>\n",
       "      <td>28.8</td>\n",
       "      <td>37.0</td>\n",
       "      <td>37.3</td>\n",
       "      <td>21.9</td>\n",
       "      <td>16.9</td>\n",
       "      <td>28</td>\n",
       "      <td>80.7</td>\n",
       "      <td>193.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44.3</td>\n",
       "      <td>29.9</td>\n",
       "      <td>34.0</td>\n",
       "      <td>18.4</td>\n",
       "      <td>28.2</td>\n",
       "      <td>13.9</td>\n",
       "      <td>11.2</td>\n",
       "      <td>20.9</td>\n",
       "      <td>15.0</td>\n",
       "      <td>104.5</td>\n",
       "      <td>...</td>\n",
       "      <td>31.0</td>\n",
       "      <td>26.2</td>\n",
       "      <td>37.0</td>\n",
       "      <td>34.8</td>\n",
       "      <td>23.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>23</td>\n",
       "      <td>72.6</td>\n",
       "      <td>186.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>42.5</td>\n",
       "      <td>29.9</td>\n",
       "      <td>34.0</td>\n",
       "      <td>21.5</td>\n",
       "      <td>29.4</td>\n",
       "      <td>15.2</td>\n",
       "      <td>11.6</td>\n",
       "      <td>20.7</td>\n",
       "      <td>14.9</td>\n",
       "      <td>107.5</td>\n",
       "      <td>...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>28.4</td>\n",
       "      <td>37.7</td>\n",
       "      <td>38.6</td>\n",
       "      <td>24.4</td>\n",
       "      <td>18.0</td>\n",
       "      <td>22</td>\n",
       "      <td>78.8</td>\n",
       "      <td>187.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Biacromial diameter (see Fig. 2)  \\\n",
       "Id                                     \n",
       "1                               42.9   \n",
       "2                               43.7   \n",
       "3                               40.1   \n",
       "4                               44.3   \n",
       "5                               42.5   \n",
       "\n",
       "    Biiliac diameter, or \"pelvic breadth\" (see Fig. 2)  \\\n",
       "Id                                                       \n",
       "1                                                26.0    \n",
       "2                                                28.5    \n",
       "3                                                28.2    \n",
       "4                                                29.9    \n",
       "5                                                29.9    \n",
       "\n",
       "    Bitrochanteric diameter (see Fig. 2)  \\\n",
       "Id                                         \n",
       "1                                   31.5   \n",
       "2                                   33.5   \n",
       "3                                   33.3   \n",
       "4                                   34.0   \n",
       "5                                   34.0   \n",
       "\n",
       "    Chest depth between spine and sternum at nipple level,  \\\n",
       "Id                                                           \n",
       "1                                                17.7        \n",
       "2                                                16.9        \n",
       "3                                                20.9        \n",
       "4                                                18.4        \n",
       "5                                                21.5        \n",
       "\n",
       "    Chest diameter at nipple level, mid-expiration  \\\n",
       "Id                                                   \n",
       "1                                             28.0   \n",
       "2                                             30.8   \n",
       "3                                             31.7   \n",
       "4                                             28.2   \n",
       "5                                             29.4   \n",
       "\n",
       "    Elbow diameter, sum of two elbows  Wrist diameter, sum of two wrists  \\\n",
       "Id                                                                         \n",
       "1                                13.1                               10.4   \n",
       "2                                14.0                               11.8   \n",
       "3                                13.9                               10.9   \n",
       "4                                13.9                               11.2   \n",
       "5                                15.2                               11.6   \n",
       "\n",
       "    Knee diameter, sum of two knees  Ankle diameter, sum of two ankles  \\\n",
       "Id                                                                       \n",
       "1                              18.8                               14.1   \n",
       "2                              20.6                               15.1   \n",
       "3                              19.7                               14.1   \n",
       "4                              20.9                               15.0   \n",
       "5                              20.7                               14.9   \n",
       "\n",
       "    Shoulder girth over deltoid muscles  ...  \\\n",
       "Id                                       ...   \n",
       "1                                 106.2  ...   \n",
       "2                                 110.5  ...   \n",
       "3                                 115.1  ...   \n",
       "4                                 104.5  ...   \n",
       "5                                 107.5  ...   \n",
       "\n",
       "    Bicep girth, flexed, average of right and left girths  \\\n",
       "Id                                                          \n",
       "1                                                32.5       \n",
       "2                                                34.4       \n",
       "3                                                33.4       \n",
       "4                                                31.0       \n",
       "5                                                32.0       \n",
       "\n",
       "    Forearm girth, extended, palm up, average of right and  \\\n",
       "Id                                                           \n",
       "1                                                26.0        \n",
       "2                                                28.0        \n",
       "3                                                28.8        \n",
       "4                                                26.2        \n",
       "5                                                28.4        \n",
       "\n",
       "    Knee girth over patella, slightly flexed position, average  \\\n",
       "Id                                                               \n",
       "1                                                34.5            \n",
       "2                                                36.5            \n",
       "3                                                37.0            \n",
       "4                                                37.0            \n",
       "5                                                37.7            \n",
       "\n",
       "    Calf maximum girth, average of right and left girths  \\\n",
       "Id                                                         \n",
       "1                                                36.5      \n",
       "2                                                37.5      \n",
       "3                                                37.3      \n",
       "4                                                34.8      \n",
       "5                                                38.6      \n",
       "\n",
       "    Ankle minimum girth, average of right and left girths  \\\n",
       "Id                                                          \n",
       "1                                                23.5       \n",
       "2                                                24.5       \n",
       "3                                                21.9       \n",
       "4                                                23.0       \n",
       "5                                                24.4       \n",
       "\n",
       "    Wrist minimum girth, average of right and left girths  Age (years)  \\\n",
       "Id                                                                       \n",
       "1                                                16.5               21   \n",
       "2                                                17.0               23   \n",
       "3                                                16.9               28   \n",
       "4                                                16.6               23   \n",
       "5                                                18.0               22   \n",
       "\n",
       "    Weight (kg)  Height (cm)  Gender (1 - male, 0 - female)  \n",
       "Id                                                           \n",
       "1          65.6        174.0                              1  \n",
       "2          71.8        175.3                              1  \n",
       "3          80.7        193.5                              1  \n",
       "4          72.6        186.5                              1  \n",
       "5          78.8        187.2                              1  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('body.xlsx', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4b0f191",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/machine-learning/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:14: UserWarning: A NumPy version >=1.22.4 and <2.3.0 is required for this version of SciPy (detected version 2.3.0)\n",
      "  from scipy.sparse import csr_matrix, issparse\n"
     ]
    }
   ],
   "source": [
    "# Libreria standard per regressione logistica\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X = df.drop(columns='Gender (1 - male, 0 - female)')\n",
    "y = df['Gender (1 - male, 0 - female)']\n",
    "\n",
    "model = LogisticRegression(penalty=None).fit(X=X, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "230976b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"â–¸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"â–¾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(penalty=None)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(penalty=None)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(penalty=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf76d6d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 43.59678536, -30.09449892,   6.72451241,   7.09659872,\n",
       "        -14.1415722 ,  40.2960209 ,  21.17580908, -29.42871815,\n",
       "         47.23879814,  -7.23057728,   3.8137938 ,  18.95284988,\n",
       "        -24.7519052 ,  -8.34131365, -62.27705814,  19.30499548,\n",
       "         37.70358756, -37.01592358, -67.23993937,  25.17149844,\n",
       "          8.10819819,  -1.24373387,  42.05595735,  15.1620603 ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6151f29e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.80752457])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc7a89c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 42.9,  26. ,  31.5,  17.7,  28. ,  13.1,  10.4,  18.8,  14.1,\n",
       "       106.2,  89.5,  71.5,  74.5,  93.5,  51.5,  32.5,  26. ,  34.5,\n",
       "        36.5,  23.5,  16.5,  21. ,  65.6, 174. ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.iloc[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbac1ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([958.46154513])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_ @ X.iloc[0].values + model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1ec6d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/machine-learning/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1458: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(self.predict_proba(X))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-inf,   0.],\n",
       "       [-inf,   0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_log_proba(X.iloc[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b36e58e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d938fe1c",
   "metadata": {},
   "source": [
    "Forse siamo in overfitting, proviamo ad usare meno variabili"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "838bd983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ankle minimum girth, average of right and left girths</th>\n",
       "      <th>Wrist minimum girth, average of right and left girths</th>\n",
       "      <th>Age (years)</th>\n",
       "      <th>Weight (kg)</th>\n",
       "      <th>Height (cm)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23.5</td>\n",
       "      <td>16.5</td>\n",
       "      <td>21</td>\n",
       "      <td>65.6</td>\n",
       "      <td>174.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24.5</td>\n",
       "      <td>17.0</td>\n",
       "      <td>23</td>\n",
       "      <td>71.8</td>\n",
       "      <td>175.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21.9</td>\n",
       "      <td>16.9</td>\n",
       "      <td>28</td>\n",
       "      <td>80.7</td>\n",
       "      <td>193.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>23</td>\n",
       "      <td>72.6</td>\n",
       "      <td>186.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24.4</td>\n",
       "      <td>18.0</td>\n",
       "      <td>22</td>\n",
       "      <td>78.8</td>\n",
       "      <td>187.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Ankle minimum girth, average of right and left girths  \\\n",
       "Id                                                          \n",
       "1                                                23.5       \n",
       "2                                                24.5       \n",
       "3                                                21.9       \n",
       "4                                                23.0       \n",
       "5                                                24.4       \n",
       "\n",
       "    Wrist minimum girth, average of right and left girths  Age (years)  \\\n",
       "Id                                                                       \n",
       "1                                                16.5               21   \n",
       "2                                                17.0               23   \n",
       "3                                                16.9               28   \n",
       "4                                                16.6               23   \n",
       "5                                                18.0               22   \n",
       "\n",
       "    Weight (kg)  Height (cm)  \n",
       "Id                            \n",
       "1          65.6        174.0  \n",
       "2          71.8        175.3  \n",
       "3          80.7        193.5  \n",
       "4          72.6        186.5  \n",
       "5          78.8        187.2  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xr = X.iloc[:, -5:]\n",
    "Xr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73de2d8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9191321499013807"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelr = LogisticRegression(penalty=None).fit(X=Xr, y=y)\n",
    "modelr.score(Xr, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "400d2318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.44170967, -0.26987233],\n",
       "       [-2.32176132, -0.10325235]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = modelr.predict_log_proba(Xr.iloc[0:2])\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "495375aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.17183734])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = modelr.coef_ @ Xr.iloc[0].values + modelr.intercept_\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "675aab9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.1718373414890806)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0,1] - t[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30c77043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.23652304, 0.76347696],\n",
       "       [0.09810065, 0.90189935]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelr.predict_proba(Xr.iloc[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb89bddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.76347696])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Funzione sigmoide logistica\n",
    "(1 / (1 + np.exp(-z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfe135ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Come cambia con la regolarizzazione di default?\n",
    "model_l2 = LogisticRegression(max_iter=5000).fit(X=X, y=y)\n",
    "model_l2.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1983444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.58341916, -0.37266708,  0.13580994,  0.23812767, -0.19283536,\n",
       "         1.05692744,  0.33055334, -0.29209734,  1.12530681,  0.03684634,\n",
       "         0.19953749,  0.56707925, -0.28765108, -0.17780219, -0.53921704,\n",
       "         0.55497559,  1.00904623, -0.08204791, -0.70592404,  0.495081  ,\n",
       "         0.04081308, -0.03666437, -0.2365196 ,  0.47582062]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_l2.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e34aa8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proviamo a fare la validazione\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb52719b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.987012987012987)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_val = LogisticRegression(max_iter=1000).fit(X=X_train, y=y_train)\n",
    "model_val.score(X_train, y_train), model_val.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13bdf93b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.78500544, -0.33852448,  0.01392559,  0.27873131, -0.1900081 ,\n",
       "         0.79603085,  0.27092994, -0.30645025,  1.0710608 ,  0.1030269 ,\n",
       "         0.13907863,  0.55320308, -0.20925664, -0.13043679, -0.53623912,\n",
       "         0.36772577,  0.90773851,  0.04599768, -0.68145819,  0.24175277,\n",
       "        -0.12088041, -0.08413462, -0.22203225,  0.44152789]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_val.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84e98048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylin</th>\n",
       "      <th>disp</th>\n",
       "      <th>hp</th>\n",
       "      <th>wt</th>\n",
       "      <th>accel</th>\n",
       "      <th>year</th>\n",
       "      <th>orig</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Chevrolet Chevelle Malibu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Buick Skylark 320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Plymouth Satellite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Amc Rebel Sst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Ford Torino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403.0</th>\n",
       "      <td>32.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>2295.0</td>\n",
       "      <td>11.6</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Dodge Rampage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404.0</th>\n",
       "      <td>28.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>2625.0</td>\n",
       "      <td>18.6</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Ford Ranger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405.0</th>\n",
       "      <td>31.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>2720.0</td>\n",
       "      <td>19.4</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Chevy S-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1825.0</td>\n",
       "      <td>18.6</td>\n",
       "      <td>77.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Renault 5 Gtl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>407 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        mpg  cylin   disp     hp      wt  accel  year  orig  \\\n",
       "id                                                            \n",
       "1.0    18.0    8.0  307.0  130.0  3504.0   12.0  70.0   1.0   \n",
       "2.0    15.0    8.0  350.0  165.0  3693.0   11.5  70.0   1.0   \n",
       "3.0    18.0    8.0  318.0  150.0  3436.0   11.0  70.0   1.0   \n",
       "4.0    16.0    8.0  304.0  150.0  3433.0   12.0  70.0   1.0   \n",
       "5.0    17.0    8.0  302.0  140.0  3449.0   10.5  70.0   1.0   \n",
       "...     ...    ...    ...    ...     ...    ...   ...   ...   \n",
       "403.0  32.0    4.0  135.0   84.0  2295.0   11.6  82.0   1.0   \n",
       "404.0  28.0    4.0  120.0   79.0  2625.0   18.6  82.0   1.0   \n",
       "405.0  31.0    4.0  119.0   82.0  2720.0   19.4  82.0   1.0   \n",
       "NaN     NaN    NaN    NaN    NaN     NaN    NaN   NaN   NaN   \n",
       "406.0   NaN    4.0   79.0    NaN  1825.0   18.6  77.0   2.0   \n",
       "\n",
       "                            name  \n",
       "id                                \n",
       "1.0    Chevrolet Chevelle Malibu  \n",
       "2.0            Buick Skylark 320  \n",
       "3.0           Plymouth Satellite  \n",
       "4.0                Amc Rebel Sst  \n",
       "5.0                  Ford Torino  \n",
       "...                          ...  \n",
       "403.0              Dodge Rampage  \n",
       "404.0                Ford Ranger  \n",
       "405.0                 Chevy S-10  \n",
       "NaN                          NaN  \n",
       "406.0              Renault 5 Gtl  \n",
       "\n",
       "[407 rows x 9 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multireg_filename = 'stin22-lab1718.xlsx'\n",
    "\n",
    "dfm = pd.read_excel(multireg_filename, sheet_name='mtst0906.2', usecols='A:J', index_col=0)\n",
    "dfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85d4d8dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylin</th>\n",
       "      <th>disp</th>\n",
       "      <th>hp</th>\n",
       "      <th>wt</th>\n",
       "      <th>accel</th>\n",
       "      <th>year</th>\n",
       "      <th>orig</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Chevrolet Chevelle Malibu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Buick Skylark 320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Plymouth Satellite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Amc Rebel Sst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Ford Torino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401.0</th>\n",
       "      <td>27.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>2790.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Ford Mustang Gl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402.0</th>\n",
       "      <td>44.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>2130.0</td>\n",
       "      <td>24.6</td>\n",
       "      <td>82.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Vw Pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403.0</th>\n",
       "      <td>32.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>2295.0</td>\n",
       "      <td>11.6</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Dodge Rampage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404.0</th>\n",
       "      <td>28.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>2625.0</td>\n",
       "      <td>18.6</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Ford Ranger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405.0</th>\n",
       "      <td>31.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>2720.0</td>\n",
       "      <td>19.4</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Chevy S-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>391 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        mpg  cylin   disp     hp      wt  accel  year  orig  \\\n",
       "id                                                            \n",
       "1.0    18.0    8.0  307.0  130.0  3504.0   12.0  70.0   1.0   \n",
       "2.0    15.0    8.0  350.0  165.0  3693.0   11.5  70.0   1.0   \n",
       "3.0    18.0    8.0  318.0  150.0  3436.0   11.0  70.0   1.0   \n",
       "4.0    16.0    8.0  304.0  150.0  3433.0   12.0  70.0   1.0   \n",
       "5.0    17.0    8.0  302.0  140.0  3449.0   10.5  70.0   1.0   \n",
       "...     ...    ...    ...    ...     ...    ...   ...   ...   \n",
       "401.0  27.0    4.0  140.0   86.0  2790.0   15.6  82.0   1.0   \n",
       "402.0  44.0    4.0   97.0   52.0  2130.0   24.6  82.0   2.0   \n",
       "403.0  32.0    4.0  135.0   84.0  2295.0   11.6  82.0   1.0   \n",
       "404.0  28.0    4.0  120.0   79.0  2625.0   18.6  82.0   1.0   \n",
       "405.0  31.0    4.0  119.0   82.0  2720.0   19.4  82.0   1.0   \n",
       "\n",
       "                            name  \n",
       "id                                \n",
       "1.0    Chevrolet Chevelle Malibu  \n",
       "2.0            Buick Skylark 320  \n",
       "3.0           Plymouth Satellite  \n",
       "4.0                Amc Rebel Sst  \n",
       "5.0                  Ford Torino  \n",
       "...                          ...  \n",
       "401.0            Ford Mustang Gl  \n",
       "402.0                  Vw Pickup  \n",
       "403.0              Dodge Rampage  \n",
       "404.0                Ford Ranger  \n",
       "405.0                 Chevy S-10  \n",
       "\n",
       "[391 rows x 9 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfm = dfm.dropna().copy()\n",
    "dfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ff152b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converto orig a intero per evitare problemi\n",
    "\n",
    "dfm['orig'] = dfm['orig'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6435e4f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylin</th>\n",
       "      <th>disp</th>\n",
       "      <th>hp</th>\n",
       "      <th>wt</th>\n",
       "      <th>accel</th>\n",
       "      <th>year</th>\n",
       "      <th>orig</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Chevrolet Chevelle Malibu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Buick Skylark 320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Plymouth Satellite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Amc Rebel Sst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Ford Torino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401.0</th>\n",
       "      <td>27.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>2790.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Ford Mustang Gl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402.0</th>\n",
       "      <td>44.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>2130.0</td>\n",
       "      <td>24.6</td>\n",
       "      <td>82.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Vw Pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403.0</th>\n",
       "      <td>32.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>2295.0</td>\n",
       "      <td>11.6</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Dodge Rampage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404.0</th>\n",
       "      <td>28.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>2625.0</td>\n",
       "      <td>18.6</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Ford Ranger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405.0</th>\n",
       "      <td>31.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>2720.0</td>\n",
       "      <td>19.4</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Chevy S-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>391 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        mpg  cylin   disp     hp      wt  accel  year  orig  \\\n",
       "id                                                            \n",
       "1.0    18.0    8.0  307.0  130.0  3504.0   12.0  70.0     1   \n",
       "2.0    15.0    8.0  350.0  165.0  3693.0   11.5  70.0     1   \n",
       "3.0    18.0    8.0  318.0  150.0  3436.0   11.0  70.0     1   \n",
       "4.0    16.0    8.0  304.0  150.0  3433.0   12.0  70.0     1   \n",
       "5.0    17.0    8.0  302.0  140.0  3449.0   10.5  70.0     1   \n",
       "...     ...    ...    ...    ...     ...    ...   ...   ...   \n",
       "401.0  27.0    4.0  140.0   86.0  2790.0   15.6  82.0     1   \n",
       "402.0  44.0    4.0   97.0   52.0  2130.0   24.6  82.0     2   \n",
       "403.0  32.0    4.0  135.0   84.0  2295.0   11.6  82.0     1   \n",
       "404.0  28.0    4.0  120.0   79.0  2625.0   18.6  82.0     1   \n",
       "405.0  31.0    4.0  119.0   82.0  2720.0   19.4  82.0     1   \n",
       "\n",
       "                            name  \n",
       "id                                \n",
       "1.0    Chevrolet Chevelle Malibu  \n",
       "2.0            Buick Skylark 320  \n",
       "3.0           Plymouth Satellite  \n",
       "4.0                Amc Rebel Sst  \n",
       "5.0                  Ford Torino  \n",
       "...                          ...  \n",
       "401.0            Ford Mustang Gl  \n",
       "402.0                  Vw Pickup  \n",
       "403.0              Dodge Rampage  \n",
       "404.0                Ford Ranger  \n",
       "405.0                 Chevy S-10  \n",
       "\n",
       "[391 rows x 9 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "224317d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xm = dfm.iloc[:, :7]\n",
    "ym = dfm['orig']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e0f8e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizzo Xm per avere dei coefficienti confrontabili\n",
    "Xm = (Xm - Xm.mean()) / Xm.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d48101b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xm_train, Xm_test, ym_train, ym_test = train_test_split(Xm, ym, test_size=0.15, stratify=ym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2a98c98e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7650602409638554, 0.7627118644067796)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_m = LogisticRegression(max_iter=5000).fit(X=Xm_train, y=ym_train)\n",
    "model_m.score(Xm_train, ym_train), model_m.score(Xm_test, ym_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a2f738b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.76212527, -0.86827825, -0.89490384],\n",
       "       [-2.243757  , -0.74839912, -0.86555639]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_m.predict_log_proba(Xm_test.iloc[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9b771360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17167961, 0.4196735 , 0.40864689],\n",
       "       [0.10605929, 0.47312336, 0.42081735]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_m.predict_proba(Xm_test.iloc[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "153ce227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.50181181,  0.31457722,  0.18723459],\n",
       "       [-0.3008016 , -0.06105337,  0.36185497],\n",
       "       [ 3.42624508, -1.89608196, -1.53016312],\n",
       "       [-0.79507134, -0.42440831,  1.21947965],\n",
       "       [-0.46891162,  1.67015628, -1.20124466],\n",
       "       [-0.04418876, -0.09074793,  0.13493668],\n",
       "       [ 0.36181025, -0.51255036,  0.15074011]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_m.coef_.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bc645bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfwAAAGdCAYAAAD3+sHsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAhb0lEQVR4nO3dfXBU1f348c/dQBIiJIg8hEwWAXlQQB7kaZDWBhUpUgb6h1qLFfi1WBmwUmw75h8pdWi000GdNgPoV8FppSh1Ah0VGEUD40MQgswgKjVKJVoCgpgnS4K55zfnRFaCJJDcu7t373m/mDNhL3fPPXvZ7Gc/55x7rqOUUgIAAEItkuwGAACA+CPgAwBgAQI+AAAWIOADAGABAj4AABYg4AMAYAECPgAAFiDgAwBggU6JPqDruvLf//5XunXrJo7jJPrwAIAUodeFq62tlby8PIlE4pefnjp1ShobG32pKz09XTIzMyWIEh7wdbCPRqOJPiwAIEVVVlZKfn5+3IJ9XpeuclKafKkvNzdXDh06FMign/CArzN7bfnaw5KZlZ3owyOAxvSvS3YTECCnmhL+sYSA+qq+Vn4ydXAsbsRDY2OjCfZPZw6ULI+j3F+JK3OrPjZ1EvBFYt34OtgT8KFd0pWpJPhWGgEf50jE8G+WRCTLSfNWScDvTMNvFgDAek4nRyIev1g4Ktjz0gj4AADrOZ0j4jjeehudgN98loAPALBeJM2RSMRbhh5xg53hM3gKAIAFyPABANZzOjvieMzwnYBn+AR8AID1InrSHl36AAAg1ZHhAwCs59ClDwCAJbP00zx26TcFO+DTpQ8AgAXI8AEA1nPSHFM81SHBzvAJ+AAA60X86NIPeMCnSx8AAAuQ4QMArOdEfJilz81zAAAINictYoqnOgJ+f1wCPgDAehHG8AEAQBiQ4QMArOc4rLQHAEDoOWnN3fqe6gj2EH7HuvSLi4ulf//+kpmZKRMnTpS3337b/5YBAIDkBfxnn31Wli5dKsuWLZO9e/fKqFGjZNq0aXLs2DH/WgUAQBJW2nM8llAF/JUrV8qCBQtk/vz5MmzYMFm9erVkZWXJU089FZ8WAgAQZ04k4ksJsna1rrGxUcrLy+XGG2/8toJIxDx+6623zvuchoYGqampaVEAALDZqlWrZOTIkZKdnW3KpEmTZMuWLcEJ+MePH5empibp06dPi+36cVVV1XmfU1RUJDk5ObESjUa9tRgAgDittOd4LBcrPz9fHnroIZNE79mzR66//nqZNWuWHDhwIG6vMe79D4WFhVJdXR0rlZWV8T4kAAAdWngn4rFcrJkzZ8rNN98sgwcPliFDhsiKFSuka9euUlZWFozL8nr27ClpaWly9OjRFtv149zc3PM+JyMjwxQAAGxQc87Q9YXioO4537hxo9TX15uu/UBk+Onp6TJ27FjZvn17bJvruuZxPBsJAECqdOlHo9EWQ9l6aPt89u/fb7J6/WXg7rvvlpKSEjMZPjAL7+hL8ubOnSvjxo2TCRMmyKOPPmq+lehZ+wAApCLH8T7LXteh6aFrPRHvjNay+6FDh8q+ffvMcPc///lPE1t37NgRt6Df7oB/2223yeeffy4PPPCAmag3evRo2bp163cm8gEAYNXtcSPNzz8z8/5ies0HDRpk/q57z3fv3i2PPfaYrFmzRgKztO7ixYtNAQAA/tBD5PpS9nhhLX0AgPUiftwetx03z9FXsE2fPl369esntbW1sn79eiktLZVt27ZJvBDwAQDWc3zs0r8Yejn6O++8U44cOWIm9ulFeHSwnzp1qsQLAR8AgAR78sknE31IAj4AAI4Pa+EHfS19Aj4AwHpOgrv0kyHYX0cAAIAvyPABANZzLMjwCfgAAOs5FgR8uvQBALAAGT4AwHqOyfAjoc7wCfgAAOs5Ee8r7TlNBHwAAALNYQwfAACEARk+AMB6DivtAQAQfg5d+gAAIAzI8AEA1nMsyPAJ+AAA6zkWjOEHu3UAACC1M/xLuoh0yUrW0REkR+t4I+BbfbvVJbsJCAjVqTFhx3Lo0gcAIPwcuvQBAEAYkOEDAOA4zcVrHQFGwAcAWM9xfBjDJ+ADABBsDmP4AAAgDMjwAQDWc7gsDwCA8HPo0gcAAGFAhg8AsJ4T8d4lr+sIMgI+AMB6jgVj+AH/PgIAAPxAhg8AQCTSXLzWEWAEfACA9Ry90p4T7pX2gv11BAAA+IIMHwBgPceC6/AJ+AAA6zkWzNIn4AMA4PgwaS/gF+IHu3UAAMAXZPgAAES8d+nrOoKMgA8AsJ7jREzxWkeQBbt1AAAgOQF/586dMnPmTMnLyzOLDGzatMmflgAAkCwRx58SpoBfX18vo0aNkuLi4vi0CACAJF2H73gsoRrDnz59uikAACB1xH3SXkNDgyln1NTUxPuQAAC0i2PBwjtx738oKiqSnJycWIlGo/E+JAAA7eM4zQvneCqWB/zCwkKprq6OlcrKyngfEgAAJLpLPyMjwxQAAILKsaBLn4V3AACI+LCWfthm6dfV1UlFRUXs8aFDh2Tfvn3So0cP6devn9/tAwAg7hzHMcVrHaEK+Hv27JEpU6bEHi9dutT8nDt3rqxbt87f1gEAgOQE/IKCAlFK+XN0AACCwAn/7XEZwwcAWM+xYNJesL+OAAAAX5DhAwDgfLN4jtc6AoyADwBAxIe73dGlDwAAko0MHwBgPceJmOK1jiAj4AMAEKFLHwAAhAAZPgDAek4kYorXOoIs2K0DACARHMefcpGKiopk/Pjx0q1bN+ndu7fMnj1bDh48GNeXSMAHACCix/AjHsvFB/wdO3bIokWLpKysTF5++WU5ffq03HTTTVJfXx+3l0iXPgAACbZ169YWj/XN53SmX15eLtddd11cjknABwDAaV+XfKt1iEhNTU2LzRkZGaa0pbq62vzUt5qPF7r0AQDWc76ZtOe1aNFoVHJycmJFj9e3xXVdWbJkiUyePFlGjBgRt9dIhg8AgI8qKyslOzs79vhC2b0ey3/33Xfl9ddfl3gi4AMA4Ph38xwd7M8O+G1ZvHixvPDCC7Jz507Jz8+XeCLgAwDg+LDSXjvmACil5J577pGSkhIpLS2VAQMGSLwR8AEASDDdjb9+/XrZvHmzuRa/qqrKbNdj/l26dInLMQn4AADrOQm+ec6qVavMz4KCghbb165dK/PmzZNQBfxes8dKlpOWrMMjQIqmP57sJiBAekZzk90EBMTpxrrQ3jxHKSWJxmV5AABYgC59AAAc/2bpBxUBHwAAx7+V9oKKgA8AQOSbG+B4rSPAgt06AADgCzJ8AAAcxvABAAi/SGIvy0uGYH8dAQAAviDDBwDAcXzo0g92hk/ABwDACf9leXTpAwBgATJ8AAAi4b8On4APAABd+gAAIAzI8AEAcFh4BwCA8HN8GMMn4AMAEHAOY/gAACAEyPABAHAYwwcAIPwcuvQBAEAIkOEDABAJ/0p77WpdUVGRjB8/Xrp16ya9e/eW2bNny8GDB+PXOgAAEkA5ji8lNAF/x44dsmjRIikrK5OXX35ZTp8+LTfddJPU19fHr4UAACCxXfpbt25t8XjdunUm0y8vL5frrrvOe2sAAEjapL2I9zrCOoZfXV1tfvbo0aPVfRoaGkw5o6amxsshAQDwnxP+y/I63DrXdWXJkiUyefJkGTFiRJvj/jk5ObESjUY7ekgAAJDogK/H8t99913ZsGFDm/sVFhaanoAzpbKysqOHBAAgLpQFk/Y61KW/ePFieeGFF2Tnzp2Sn5/f5r4ZGRmmAAAQWE74u/TbFfCVUnLPPfdISUmJlJaWyoABA+LXMgAAEsUJ/0p7ndrbjb9+/XrZvHmzuRa/qqrKbNdj8126dIlXGwEAQCID/qpVq8zPgoKCFtvXrl0r8+bN89oWAACSIxL+lfba3aUPAEDYKB8m3QV90l6wv44AAABfcPMcAAAcZukDABB6yomY4rWOIAt26wAAgC/I8AEAcLgOHwCA0FPiQ5d+wDvNCfgAADjhz/CD/XUEAAD4ggwfAADH8eGyvGBn+AR8AID1FCvtAQCAMCDDBwDAYaU9AABCT4ljitc6gizYX0cAAIAvyPABANZTFqylT8AHAMAJ/xh+sFsHAAB8QYYPALCesuA6/KQF/D3FZZLRJTtZh0eAzOnTOdlNQID8tM+ryW4CAqKmrl4ufzIxx1KM4QMAYAGHm+cAAIAQIMMHAMDx3qUf9Fn6BHwAgPUUK+0BAIAwIMMHAFhPWTBLP9itAwAgEZyzZup3uLTvkDt37pSZM2dKXl6eOI4jmzZtkngi4AMAkAT19fUyatQoKS4uTsjx6NIHAFhPScQUr3W0x/Tp001JFAI+AMB6yseldWtqalpsz8jIMCXZ6NIHAMBH0WhUcnJyYqWoqEiCgAwfAGA95eMs/crKSsnO/vZeMUHI7jUCPgDAesrHhXd0sD874AcFAR8AYD1lwXX4BHwAAJKgrq5OKioqYo8PHTok+/btkx49eki/fv18Px4BHwBgPeXjLP2LtWfPHpkyZUrs8dKlS83PuXPnyrp168RvBHwAgPVUEm6eU1BQIEopSZRgDzgAAABfkOEDAKynmLQHAED4qSR06SdasL+OAAAAX5DhAwCsp8SHLv2A59Dtat2qVatk5MiRsVWEJk2aJFu2bIlf6wAASGCXvvJYQhPw8/Pz5aGHHpLy8nJz/eD1118vs2bNkgMHDsSvhQAAILFd+jNnzmzxeMWKFSbrLysrk+HDh3tvDQAASVt4J+K5jlCO4Tc1NcnGjRulvr7edO23pqGhwZQzzr1PMAAAyaYsmKXf7oC/f/9+E+BPnTolXbt2lZKSEhk2bFir++v7AC9fvtxrOwEACNXSuonW7v6LoUOHmsX9d+3aJQsXLjRr/r733nut7l9YWCjV1dWxou8TDAAAAp7hp6eny6BBg8zfx44dK7t375bHHntM1qxZc979MzIyTAEAIKiUckzxWkeor8N3XbfFGD0AAKkn4sN19JHwBHzdPT99+nRzn97a2lpZv369lJaWyrZt2+LXQgAAkNiAf+zYMbnzzjvlyJEjkpOTYxbh0cF+6tSp3lsCAECSKGbpt/Tkk0/GryUAACSJsiDgB3vAAQAA+IKb5wAArKcsyPAJ+AAA6ykLAj5d+gAAWIAMHwBgPcXCOwAAhJ+yoEufgA8AsJ6yIOAzhg8AgAXI8AEA1lMWZPgEfACA9ZT4MGkv4AGfLn0AACxAhg8AsJ4rjile6wgyAj4AwHrKgjF8uvQBALAAGT4AwHqKlfYAAAg/5UOXvK4jyOjSBwDAAmT4AADrKbr0AQAIP2XBLP2kBfyel3WWLlmdk3V4BMh89X/JbgIC5J4XZye7CQiIxlO1CTuWsiDDZwwfAAAL0KUPALCeMivlea8jyAj4AADrKbr0AQBAGJDhAwCsp5ilDwBA+Cm69AEAQBiQ4QMArKfo0gcAIPxc1Vy81hFkdOkDAGABMnwAgPUUXfoAAISfsmCWPgEfAGA9pZqL1zqCjDF8AAAsQIYPALCeK44pXusIMgI+AMB6yoIxfLr0AQCwABk+AMB6yoJJewR8AID1lAXX4dOlDwCABcjwAQDWcy1YS5+ADwCA8j5LX9cR2i79hx56SBzHkSVLlvjXIgAALFFcXCz9+/eXzMxMmThxorz99tvBC/i7d++WNWvWyMiRI/1tEQAASZqlrzyW9nj22Wdl6dKlsmzZMtm7d6+MGjVKpk2bJseOHQtOwK+rq5M5c+bIE088IZdeeqn/rQIAIAkr7bkeS3usXLlSFixYIPPnz5dhw4bJ6tWrJSsrS5566qngBPxFixbJjBkz5MYbb7zgvg0NDVJTU9OiAABgc4bf2Ngo5eXlLeJoJBIxj996661gTNrbsGGD6XrQXfoXo6ioSJYvX96RtgEAkHJqzklsMzIyTDnb8ePHpampSfr06dNiu378wQcfJD/Dr6yslHvvvVeeeeYZM8HgYhQWFkp1dXWs6DoAAAjiWvrKY9Gi0ajk5OTEik58g6BdGb7uftCTCa655prYNv0NZefOnfLXv/7VdN+npaVd8JsNAABhvQ6/srJSsrOzY9vPFwN79uxp4uXRo0dbbNePc3NzJekZ/g033CD79++Xffv2xcq4cePMBD7993ODPQAAtsnOzm5Rzhfw09PTZezYsbJ9+/bYNtd1zeNJkyYlP8Pv1q2bjBgxosW2Sy65RC677LLvbAcAIFWoJNw8R1+SN3fuXJM4T5gwQR599FGpr683s/bjgZX2AADWU0m4ec5tt90mn3/+uTzwwANSVVUlo0ePlq1bt35nIl9gAn5paak/LQEAwDKLFy82JRHI8AEA1nN9uPmNriPICPgAAOupJIzhp9TNcwAAQGogwwcAWE9ZkOET8AEA1nOVY4rXOoKMgA8AsJ6yIMNnDB8AAAuQ4QMArKcsyPAJ+AAA6ykfbp4T9IBPlz4AABYgwwcAWE+ddT97L3UEGQEfAGA9ZcEYPl36AABYgAwfAGA914dJe16fH28EfACA9RRd+gAAIAzI8AEA1lMWZPgEfACA9VzG8AEACD9Fhh8/+T2bJKtrU7IOjwB5sen/JbsJCJBbZ36d7CYgIOrrRP62ItmtCA8yfACA9Vy3uXitI8gI+AAA6ykLuvS5LA8AAAuQ4QMArKcsyPAJ+AAA67k+XFYX8CF8uvQBALABGT4AwHpKKVO81hFkBHwAgPWUBWP4dOkDAGABMnwAgPWUDwvv6DqCjIAPALCesqBLn4APALCea8Hd8hjDBwDAAmT4AADrKbr0AQAIP+UqU7zWEWR06QMAYAEyfACA9VwLJu0R8AEA1lMWjOHTpQ8AgAXI8AEA1nNdZYrXOoKMgA8AsJ6iSx8AAIRBuwL+73//e3Ecp0W58sor49c6AAASmOErjyVUXfrDhw+XV1555dsKOjEqAABIba5SpnitI8jaHa11gM/NzY1PawAASALler+9bdBvj9vuMfwPP/xQ8vLyZODAgTJnzhw5fPhwm/s3NDRITU1NiwIAAAIc8CdOnCjr1q2TrVu3yqpVq+TQoUPy/e9/X2pra1t9TlFRkeTk5MRKNBr1o90AAPhG6T/KYxEVnoA/ffp0ueWWW2TkyJEybdo0eemll+TLL7+U5557rtXnFBYWSnV1daxUVlb60W4AAHyjXH0dvbcS9C59TzPuunfvLkOGDJGKiopW98nIyDAFAACk6HX4dXV18tFHH0nfvn39axEAAAmmvHbnf1NCE/B/85vfyI4dO+Q///mPvPnmm/LjH/9Y0tLS5Pbbb49fCwEASNDd8lyPJTRd+p9++qkJ7idOnJBevXrJ9773PSkrKzN/BwAAIQn4GzZsiF9LAABIEuUqU7zWEWQskwcAsJ7i5jkAACAMyPABANZzXeX5fvZenx9vBHwAgPWUD5fVBf2yPAI+AMB6ipvnAACAMCDgAwCs5yrlS4mXFStWyLXXXitZWVlmWfuOIOADAKynAr60bmNjo7l53cKFCztcB2P4AAAE3PLly81PfYv6jiLgAwCs5/p4WV5NTU0g7xpLlz4AwHpK+VO0aDQqOTk5sVJUVCRBQMAHAMBHlZWVUl1dHSuFhYXn3e/+++8Xx3HaLB988IFv7aJLHwBgPaV8uHnONyl+dna2KRdy3333ybx589rcZ+DAgeIXAj4AwHrKh8vq2jtLX99aPpG3lyfgAwAQcIcPH5YvvvjC/GxqapJ9+/aZ7YMGDZKuXbteVB0EfACA9ZTrQ5d+HG+e88ADD8jTTz8dezxmzBjz87XXXpOCgoKLqoNJewAA66lvAr7XEi/6+vvzLfRzscFeI8MHAFjPVc3Fax1BRoYPAIAFkpbh9+laL5d0TUvW4REguyoufPkK7NH98oCnSUiYiJO494IK+Bi+H+jSBwBYT/lw85t43jzHD3TpAwBgATJ8AID1XPfbm994qSPICPgAAOspuvQBAEAYkOEDAKynmKUPAED4KQsCPl36AABYgAwfAGA9V7zfHlfXEWQEfACA9ZQFXfoEfACA9RSX5QEAgDAgwwcAWE+5yvNKe3TpAwAQcMqCMXy69AEAsAAZPgDAesqCSXsEfACA9ZTrmuK1jiCjSx8AAAuQ4QMArOf6MEvf6/PjjYAPALCesmAMny59AAAsQIYPALCe4jr87/rss8/kjjvukMsuu0y6dOkiV199tezZsyc+rQMAIIEBX3ksocnwT548KZMnT5YpU6bIli1bpFevXvLhhx/KpZdeGr8WAgAQZ67+o1zPdYQm4D/88MMSjUZl7dq1sW0DBgyIR7sAAECyuvT/9a9/ybhx4+SWW26R3r17y5gxY+SJJ55o8zkNDQ1SU1PTogAAECTK9aNbX8IT8D/++GNZtWqVDB48WLZt2yYLFy6UX/3qV/L000+3+pyioiLJycmJFd1DAABAkCgLxvDbFfBd15VrrrlG/vjHP5rs/q677pIFCxbI6tWrW31OYWGhVFdXx0plZaUf7QYAAPEaw+/bt68MGzasxbarrrpKnn/++Vafk5GRYQoAAEGlLFh4p10BX8/QP3jwYItt//73v+Xyyy/3u10AACSM67qmeK0jNF36v/71r6WsrMx06VdUVMj69evl8ccfl0WLFsWvhQAAILEBf/z48VJSUiL/+Mc/ZMSIEfLggw/Ko48+KnPmzPHeEgAAkkRZMGmv3Uvr/uhHPzIFAICwUMo1xWsdQcbNcwAAsAA3zwEAWE9ZcPMcAj4AAK4PY/AEfAAAgs1VPtw8hzF8AACQbGT4AADrKcbwAQCw5LI8l8vyAABAiiPDBwBYT9GlDwBA+ClW2gMAAGFAhg8AsJ7r6uKtSz7gd8cl4AMAoFwfZukHPOLTpQ8AgAXI8AEA1lPM0gcAIPyUBbP0CfgAAOspCzJ8xvABALBAwjN8pZq/AdXX1Sb60AioU18luwUIkvq6hmQ3AQHxVX1ti7gRT1831nqeZd/0db0EWcID/okTJ8zPWdcPT/ShAQApSMeNnJycuNSdnp4uubm5smf7rb7Up+vSdQaRoxLx1eksX375pVx66aVy+PDhuP0HpoKamhqJRqNSWVkp2dnZYivOQzPOQzPOQzPOQ7Pq6mrp16+fnDx5Urp37x6345w6dUoaGxt9qUsH+8zMTAmihGf4kUjztAEd7G1+I5+hzwHngfNwBuehGeehGeehZdyIl8zMzMAGaT8xaQ8AAAsQ8AEAsEDCA35GRoYsW7bM/LQZ56EZ56EZ56EZ56EZ56EZ5yHFJ+0BAIDEo0sfAAALEPABALAAAR8AAAsQ8AEAsEDcA/4XX3whc+bMMYtH6JWSfv7zn0tdXV2bzykoKBDHcVqUu+++W1JNcXGx9O/f3yzoMHHiRHn77bfb3H/jxo1y5ZVXmv2vvvpqeemllyQM2nMe1q1b953/+zAsiLFz506ZOXOm5OXlmde0adOmCz6ntLRUrrnmGjNDedCgQebc2HYe9Dk49/2gS1VVlaSqoqIiGT9+vHTr1k169+4ts2fPloMHD17weWH7fOjIeQjr50NoAr4O9gcOHJCXX35ZXnjhBfMLf9ddd13weQsWLJAjR47Eyp/+9CdJJc8++6wsXbrUXFKyd+9eGTVqlEybNk2OHTt23v3ffPNNuf32280Xonfeece8+XV59913JZW19zxo+svh2f/3n3zyiaS6+vp689r1l5+LcejQIZkxY4ZMmTJF9u3bJ0uWLJFf/OIXsm3bNrHpPJyhA8HZ7wkdIFLVjh07ZNGiRVJWVmY+F0+fPi033XSTOTetCePnQ0fOQ1g/HxJGxdF7772nL/lTu3fvjm3bsmWLchxHffbZZ60+7wc/+IG69957VSqbMGGCWrRoUexxU1OTysvLU0VFRefd/9Zbb1UzZsxosW3ixInql7/8pbLpPKxdu1bl5OSoMNO/EyUlJW3u87vf/U4NHz68xbbbbrtNTZs2Tdl0Hl577TWz38mTJ1VYHTt2zLzGHTt2tLpPWD8f2nsebPh8iKe4ZvhvvfWW6cYfN25cbNuNN95o1kXetWtXm8995plnpGfPnjJixAgpLCyUr75KnXuo6pswlJeXm9d6hn7N+rE+J+ejt5+9v6Yz4db2D+t50PSQz+WXX25uHjJr1izTQ2SbML4fvBg9erT07dtXpk6dKm+88YaE7QYxWo8ePax+P1zMedD4fOi4uAZ8Pc52btdbp06dzH9oW2NwP/3pT+Xvf/+7vPbaaybY/+1vf5M77rhDUsXx48elqalJ+vTp02K7ftza69bb27N/WM/D0KFD5amnnpLNmzeb94DrunLttdfKp59+KjZp7f2g76L2v//9T2yhg/zq1avl+eefN0V/yOs5Pnp4KAz0+1sP10yePNkkN60J4+dDR84Dnw9JuFve/fffLw8//HCb+7z//vsdbVOLMX49OUX/0t9www3y0UcfyRVXXNHhehF8kyZNMuUM/ct81VVXyZo1a+TBBx9MatuQePoDXpez3w/6c+CRRx4xiUCq02PYehz+9ddfF5td7Hng8yEJAf++++6TefPmtbnPwIEDJTc39zuTs77++mszc1//28XSM7u1ioqKlAj4eigiLS1Njh492mK7ftza69bb27N/KujIeThX586dZcyYMeb/3iatvR/0hKUuXbqIzSZMmBCKALl48eLYROb8/Pw29w3j50NHzsO5bP18SGiXfq9evczlIW2V9PR0803syy+/NOO4Z7z66qumG+ZMEL8YepaypjP9VKBf+9ixY2X79u2xbfo168dnfzs9m95+9v6anrna2v5hPQ/n0kMC+/fvT5n/e7+E8f3gF/15kMrvBz1fUQe5kpIS83k4YMAAK98PHTkP57L186HD4jolUCn1wx/+UI0ZM0bt2rVLvf7662rw4MHq9ttvj/37p59+qoYOHWr+XauoqFB/+MMf1J49e9ShQ4fU5s2b1cCBA9V1112nUsmGDRtURkaGWrdunbla4a677lLdu3dXVVVV5t9/9rOfqfvvvz+2/xtvvKE6deqk/vznP6v3339fLVu2THXu3Fnt379fpbL2nofly5erbdu2qY8++kiVl5ern/zkJyozM1MdOHBApbLa2lr1zjvvmKJ/7VauXGn+/sknn5h/1+dAn4szPv74Y5WVlaV++9vfmvdDcXGxSktLU1u3blU2nYdHHnlEbdq0SX344Yfmd0FfvROJRNQrr7yiUtXChQvNTPPS0lJ15MiRWPnqq69i+9jw+dCR8xDWz4dEiXvAP3HihAnwXbt2VdnZ2Wr+/Pnml/4MHdT1L76+/EY7fPiwCe49evQwgWLQoEHmQ6+6ulqlmr/85S+qX79+Kj093VyeVlZW1uLSw7lz57bY/7nnnlNDhgwx++tLsl588UUVBu05D0uWLInt26dPH3XzzTervXv3qlR35vKyc8uZ165/6nNx7nNGjx5tzoX+0qsvSbLtPDz88MPqiiuuMB/q+jOhoKBAvfrqqyqVne/163L2/68Nnw8dOQ9h/XxIFG6PCwCABVhLHwAACxDwAQCwAAEfAAALEPABALAAAR8AAAsQ8AEAsAABHwAACxDwAQCwAAEfAAALEPABALAAAR8AAAsQ8AEAkPD7/wlZjLf+ZVkoAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(model_m.coef_.T, cmap='coolwarm', aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8622e833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adesso rifacciamo in pytorch\n",
    "import torch\n",
    "\n",
    "zm_train = pd.get_dummies(ym_train, dtype=int)\n",
    "zm_test = pd.get_dummies(ym_test, dtype=int)\n",
    "\n",
    "Xt_train, Xt_test, zt_train, zt_test =  torch.tensor(Xm_train.values, dtype=torch.float), \\\n",
    "                                        torch.tensor(Xm_test.values, dtype=torch.float), \\\n",
    "                                        torch.tensor(zm_train.values, dtype=torch.float), \\\n",
    "                                        torch.tensor(zm_test.values, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7fb5ae00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9518,  1.4794,  1.9624,  ...,  1.5292, -2.1876, -0.8072],\n",
       "        [-1.3370,  1.4794,  1.4844,  ...,  2.0253, -0.3747, -0.5360],\n",
       "        [ 2.9768, -0.8648, -1.0391,  ..., -1.0260,  0.8580,  1.0908],\n",
       "        ...,\n",
       "        [ 0.3834, -0.8648, -0.5229,  ..., -0.1303,  0.9305,  1.0908],\n",
       "        [-0.7592,  0.3073,  0.6050,  ...,  0.2504,  0.8218,  0.0062],\n",
       "        [-0.6951,  0.3073,  0.2896,  ...,  0.1656,  0.3504, -0.8072]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xt_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3131d2fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 3)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xt_train.shape[1], zt_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5815af56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=7, out_features=3, bias=True)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin = torch.nn.Linear(Xt_train.shape[1], zt_train.shape[1])\n",
    "lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "75c7780d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__constants__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_backward_pre_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_compiled_call_impl',\n",
       " '_forward_hooks',\n",
       " '_forward_hooks_always_called',\n",
       " '_forward_hooks_with_kwargs',\n",
       " '_forward_pre_hooks',\n",
       " '_forward_pre_hooks_with_kwargs',\n",
       " '_get_backward_hooks',\n",
       " '_get_backward_pre_hooks',\n",
       " '_get_name',\n",
       " '_is_full_backward_hook',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_post_hooks',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_state_dict_pre_hooks',\n",
       " '_version',\n",
       " '_wrapped_call_impl',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'bfloat16',\n",
       " 'bias',\n",
       " 'buffers',\n",
       " 'call_super_init',\n",
       " 'children',\n",
       " 'compile',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'get_buffer',\n",
       " 'get_extra_state',\n",
       " 'get_parameter',\n",
       " 'get_submodule',\n",
       " 'half',\n",
       " 'in_features',\n",
       " 'ipu',\n",
       " 'load_state_dict',\n",
       " 'modules',\n",
       " 'mtia',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'out_features',\n",
       " 'parameters',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_full_backward_pre_hook',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_load_state_dict_pre_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'register_state_dict_post_hook',\n",
       " 'register_state_dict_pre_hook',\n",
       " 'requires_grad_',\n",
       " 'reset_parameters',\n",
       " 'set_extra_state',\n",
       " 'set_submodule',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'weight',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(lin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c2f77e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1614,  0.0835,  0.0257, -0.1700,  0.1196,  0.1592, -0.1880],\n",
       "        [-0.0785, -0.3674,  0.1575,  0.0652,  0.1922, -0.2052,  0.1571],\n",
       "        [ 0.3437, -0.0634,  0.1354,  0.1736,  0.0570, -0.2224, -0.1661]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2d6da662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.0425,  0.0981, -0.0179], requires_grad=True)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "26b7afc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Per ora il gradiente e' vuoto\n",
    "lin.weight.grad is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0bb32881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.9762e-01,  7.6680e-01,  1.1004e+00],\n",
      "        [ 5.0266e-01,  3.5216e-01,  1.2241e-01],\n",
      "        [-5.5300e-01, -2.5037e-01,  3.1031e-01],\n",
      "        [-1.0562e-01,  3.0644e-01, -2.2170e-01],\n",
      "        [ 3.8497e-01,  4.9084e-02, -3.0022e-01],\n",
      "        [ 4.1475e-01,  6.1613e-01,  6.5058e-01],\n",
      "        [ 1.1646e-01,  1.9027e-01, -9.7257e-02],\n",
      "        [ 3.5520e-01,  3.1472e-01,  5.3803e-01],\n",
      "        [ 3.3412e-01,  3.8212e-01,  4.5323e-01],\n",
      "        [-7.2307e-03, -3.1571e-02, -2.6362e-01],\n",
      "        [ 2.4831e-01,  4.3833e-02, -5.2883e-01],\n",
      "        [ 1.1462e-01,  4.0652e-01,  4.2727e-01],\n",
      "        [ 1.1910e-01,  2.0720e-01,  4.0921e-01],\n",
      "        [-3.5379e-02, -6.5750e-02,  3.6193e-01],\n",
      "        [ 5.8134e-02,  3.7585e-01, -1.4774e-01],\n",
      "        [-1.5607e-01,  4.4060e-02, -5.7943e-02],\n",
      "        [-4.6661e-01,  3.5835e-01,  7.2063e-02],\n",
      "        [-7.6144e-02, -9.7372e-02, -2.4441e-02],\n",
      "        [ 3.4314e-01, -2.6863e-01, -5.0749e-01],\n",
      "        [ 1.0215e-01,  2.2573e-01, -1.9850e-01],\n",
      "        [ 5.2367e-01, -8.7436e-02, -8.2428e-01],\n",
      "        [-1.1231e-01, -6.1927e-01, -1.2425e-01],\n",
      "        [-2.8950e-01, -3.1722e-01,  4.4817e-02],\n",
      "        [ 2.1086e-01, -4.7645e-02, -1.0116e-01],\n",
      "        [ 5.8961e-01,  4.4641e-02, -4.8750e-01],\n",
      "        [ 3.8658e-01, -1.8863e-01, -3.0106e-01],\n",
      "        [ 6.8805e-02,  3.7540e-01, -2.1216e-01],\n",
      "        [ 8.8192e-02,  2.0270e-01, -4.8428e-02],\n",
      "        [-2.2756e-01,  9.8936e-02, -3.0903e-01],\n",
      "        [ 4.4321e-01,  3.5533e-01,  5.1109e-01],\n",
      "        [ 2.8403e-01,  2.9611e-02, -5.1967e-01],\n",
      "        [ 3.8340e-01, -8.1459e-02, -1.5461e-01],\n",
      "        [-3.9891e-01,  1.9725e-01,  1.8827e-01],\n",
      "        [-4.7446e-01,  1.1809e-01,  3.9694e-02],\n",
      "        [ 2.3206e-01, -7.1067e-02, -6.2892e-01],\n",
      "        [-2.9568e-01,  4.9409e-01,  3.0332e-02],\n",
      "        [-2.3782e-01, -1.9476e-01, -3.3737e-02],\n",
      "        [-2.5043e-01,  3.2119e-01, -1.0746e-01],\n",
      "        [ 1.2056e-01, -4.7626e-01, -1.8149e-01],\n",
      "        [ 9.6546e-02, -2.3421e-01,  7.0492e-02],\n",
      "        [ 1.8549e-01, -1.1833e-01, -2.2409e-01],\n",
      "        [-5.8181e-01,  4.9974e-01,  1.0354e-01],\n",
      "        [ 7.0380e-01, -1.9855e-01, -7.4804e-01],\n",
      "        [-1.7866e-01, -3.7757e-02, -2.1860e-01],\n",
      "        [ 3.3169e-01,  1.6981e-01, -5.3364e-02],\n",
      "        [ 1.8725e-01,  3.3104e-01,  6.0584e-02],\n",
      "        [ 1.9898e-02,  7.7605e-01,  1.2143e+00],\n",
      "        [ 7.6973e-02, -1.0929e-01,  2.8151e-01],\n",
      "        [ 1.9183e-01, -6.7419e-01, -3.5146e-01],\n",
      "        [ 5.2047e-01,  1.2159e-01,  2.7814e-01],\n",
      "        [-3.2216e-01, -2.6782e-01, -1.6556e-01],\n",
      "        [-5.1211e-01,  2.5633e-01,  2.0077e-01],\n",
      "        [-2.5023e-01,  2.9881e-01, -6.0026e-02],\n",
      "        [ 6.2317e-02, -2.3984e-02,  1.2687e-01],\n",
      "        [ 7.0975e-02,  2.0372e-01, -2.9461e-01],\n",
      "        [ 6.8191e-01, -1.2487e-01, -5.8358e-01],\n",
      "        [-3.8084e-01, -4.7805e-03, -4.3337e-01],\n",
      "        [ 8.2899e-01, -2.4809e-01, -8.5059e-01],\n",
      "        [ 3.6107e-01, -5.7259e-01, -2.6473e-01],\n",
      "        [ 2.3568e-01, -2.3428e-02, -1.5853e-01],\n",
      "        [ 4.8594e-01,  5.6389e-02,  4.6686e-02],\n",
      "        [-4.0529e-01,  5.2884e-01, -1.2369e-01],\n",
      "        [ 1.9787e-01,  7.4976e-01,  7.6596e-01],\n",
      "        [ 1.1038e-01,  4.1031e-01,  8.2983e-01],\n",
      "        [ 8.4130e-02,  5.0405e-02, -1.6323e-01],\n",
      "        [ 5.4370e-01,  5.2043e-04, -6.5776e-02],\n",
      "        [ 1.9252e-01,  1.4801e-01,  5.0241e-01],\n",
      "        [ 4.5947e-01,  1.0503e-01,  2.1376e-01],\n",
      "        [-6.4584e-01,  1.8818e-01,  1.1533e-01],\n",
      "        [-2.2622e-01,  9.7037e-02, -4.2875e-01],\n",
      "        [ 1.2197e-02,  5.5924e-01,  4.9422e-01],\n",
      "        [ 1.2921e-01,  3.9672e-01, -5.4493e-02],\n",
      "        [-2.5962e-01,  4.0741e-01, -3.4296e-01],\n",
      "        [-4.0775e-02, -3.7053e-01, -4.0691e-01],\n",
      "        [-1.3335e-01, -2.6248e-01, -4.0989e-01],\n",
      "        [-4.6315e-01,  8.6052e-02,  1.5006e-02],\n",
      "        [ 5.4352e-01, -5.1905e-02,  1.3060e-02],\n",
      "        [-2.7294e-01,  4.7218e-01, -4.1369e-01],\n",
      "        [ 2.5894e-01,  1.8121e-01,  3.1365e-01],\n",
      "        [ 1.3270e-01,  5.5213e-02, -3.7438e-01],\n",
      "        [ 4.3735e-01,  1.5774e-02, -5.0141e-01],\n",
      "        [-5.0770e-01,  5.7655e-02,  2.2887e-01],\n",
      "        [-5.6180e-01, -7.4517e-02,  3.0554e-02],\n",
      "        [-3.3779e-01, -2.5474e-01, -2.8967e-01],\n",
      "        [ 4.3975e-01, -6.3881e-01, -3.8433e-01],\n",
      "        [-7.2147e-02, -2.6227e-01, -2.4456e-01],\n",
      "        [ 3.6700e-01,  3.4468e-01,  1.6598e-01],\n",
      "        [-2.4662e-02, -1.5125e-01,  2.0899e-02],\n",
      "        [-1.1432e-01,  9.5042e-02,  5.5737e-01],\n",
      "        [ 8.9282e-02, -1.7406e-01,  3.9208e-02],\n",
      "        [-4.5956e-01,  5.5406e-01, -1.7033e-01],\n",
      "        [ 5.8132e-01, -7.4022e-01, -4.4786e-01],\n",
      "        [-1.2571e-01,  3.5057e-01,  1.3481e-01],\n",
      "        [-4.4632e-01,  2.9684e-01, -1.7597e-01],\n",
      "        [-1.1701e-02,  2.9342e-01, -4.4251e-01],\n",
      "        [-6.2783e-01,  5.4509e-01,  1.5070e-01],\n",
      "        [ 9.7105e-03, -1.6031e-01, -1.4482e-01],\n",
      "        [ 3.8224e-01,  3.1351e-01,  1.8951e-01],\n",
      "        [-3.2938e-01,  1.2348e-01,  1.1968e-01],\n",
      "        [ 3.5177e-01,  9.7003e-02, -3.9080e-01],\n",
      "        [ 2.4763e-01, -2.3963e-02,  2.0063e-01],\n",
      "        [ 9.1629e-02,  8.2373e-01,  8.6184e-01],\n",
      "        [ 2.7216e-01,  3.2784e-01,  3.6097e-01],\n",
      "        [ 1.9474e-01,  4.4400e-01,  2.1340e-01],\n",
      "        [ 4.9701e-01,  3.0521e-01,  3.0247e-01],\n",
      "        [ 2.8387e-01,  1.8237e-01,  6.3287e-01],\n",
      "        [ 3.2609e-01,  4.6918e-02, -2.1077e-01],\n",
      "        [ 5.4833e-01,  4.4172e-02, -3.2428e-01],\n",
      "        [ 3.8852e-01,  5.1761e-01,  4.1797e-01],\n",
      "        [ 3.7643e-01,  4.4770e-01,  1.9158e-01],\n",
      "        [-2.8104e-02,  1.5811e-01, -3.5801e-01],\n",
      "        [ 4.4976e-01, -2.5065e-01, -5.1203e-01],\n",
      "        [ 2.4112e-01, -6.5738e-01, -1.0341e-01],\n",
      "        [-5.9158e-02, -2.6538e-01, -2.2750e-01],\n",
      "        [ 3.0564e-01,  7.9283e-02,  2.1698e-01],\n",
      "        [-5.7165e-02,  4.6450e-01,  2.2280e-01],\n",
      "        [ 2.9269e-01, -5.9956e-01, -3.9183e-01],\n",
      "        [-5.9793e-02, -2.3951e-01,  3.7986e-02],\n",
      "        [-5.8703e-01,  2.3093e-01,  3.0682e-01],\n",
      "        [ 1.0092e-01,  2.4564e-01,  8.0971e-02],\n",
      "        [ 7.0303e-02, -1.6588e-01,  2.8939e-01],\n",
      "        [-2.4569e-01,  2.8705e-01, -1.5248e-01],\n",
      "        [-4.9809e-01, -5.3254e-02, -1.4959e-01],\n",
      "        [ 8.1903e-02,  7.2956e-01,  1.1655e+00],\n",
      "        [ 2.9356e-01, -1.0327e-01, -2.5591e-01],\n",
      "        [-5.3750e-02,  2.7539e-01, -1.3553e-01],\n",
      "        [ 3.5878e-01, -1.8178e-01, -4.9040e-01],\n",
      "        [ 1.8732e-02, -7.7869e-03,  2.5528e-01],\n",
      "        [-1.9197e-01, -3.0824e-02, -1.8713e-01],\n",
      "        [-1.0680e-01,  4.2627e-01,  1.0755e+00],\n",
      "        [ 2.4300e-02, -1.2435e-01, -4.1800e-01],\n",
      "        [ 3.2118e-01,  1.4511e-01, -3.4621e-01],\n",
      "        [-5.6367e-01,  2.5382e-01,  5.7259e-02],\n",
      "        [ 2.1269e-01,  1.1639e-01, -3.4277e-01],\n",
      "        [-3.1045e-01,  5.2199e-02,  4.4424e-03],\n",
      "        [ 1.8255e-01, -2.1385e-02, -5.7345e-01],\n",
      "        [-3.1118e-01,  4.3899e-02, -1.7288e-01],\n",
      "        [-4.2746e-01,  1.8077e-01, -2.3407e-02],\n",
      "        [ 1.3043e-01,  8.4653e-02,  1.8453e-02],\n",
      "        [-1.7106e-01,  2.8743e-01,  2.2129e-01],\n",
      "        [-6.7290e-01,  8.3508e-02,  4.1074e-01],\n",
      "        [-3.5411e-01,  2.7274e-02, -1.1181e-03],\n",
      "        [ 2.0370e-01,  2.2083e-01,  3.2499e-01],\n",
      "        [-2.6055e-02,  1.2873e-01,  3.0988e-02],\n",
      "        [-4.6275e-01, -9.7417e-02, -2.1330e-01],\n",
      "        [ 4.5992e-01,  1.4667e-01,  1.3914e-01],\n",
      "        [-5.1909e-02,  6.8423e-01,  5.0311e-01],\n",
      "        [ 5.3573e-01, -5.3089e-03, -1.9240e-02],\n",
      "        [ 1.7826e-01, -2.5317e-02, -1.5597e-01],\n",
      "        [-3.2598e-01, -4.6505e-02, -3.3746e-01],\n",
      "        [ 5.6881e-01,  6.9417e-02,  6.6573e-02],\n",
      "        [-3.4468e-01,  1.4830e-01,  5.9532e-02],\n",
      "        [ 3.5572e-01, -1.2030e-01, -2.0774e-01],\n",
      "        [ 2.5661e-01,  3.9643e-01,  7.9574e-01],\n",
      "        [ 1.7276e-01,  1.2639e-01,  1.9332e-02],\n",
      "        [-2.7020e-01,  3.5429e-01, -2.9122e-01],\n",
      "        [-6.3222e-02,  8.4652e-02,  7.9123e-02],\n",
      "        [ 2.3662e-01, -1.1305e-01, -1.9696e-01],\n",
      "        [ 4.0015e-01, -3.7427e-02, -3.0846e-01],\n",
      "        [ 2.4419e-01,  8.0040e-01,  7.0103e-01],\n",
      "        [-1.1575e-01,  2.5173e-01,  1.9854e-03],\n",
      "        [ 4.3731e-01, -2.7333e-01, -8.8632e-01],\n",
      "        [ 1.8018e-01, -1.2676e-01, -2.2047e-01],\n",
      "        [-2.3408e-01,  9.9510e-02, -2.1445e-01],\n",
      "        [ 4.1807e-01,  1.4529e-01,  3.0210e-01],\n",
      "        [ 4.8354e-01, -4.3446e-03,  3.8622e-02],\n",
      "        [-2.1345e-01, -5.3839e-01, -1.7080e-01],\n",
      "        [ 1.6501e-01,  3.2268e-01,  8.1036e-01],\n",
      "        [ 3.0370e-01, -4.2388e-02, -8.4449e-02],\n",
      "        [ 1.9864e-01,  2.7281e-02, -2.1361e-01],\n",
      "        [-5.0229e-01,  1.8433e-01,  4.6743e-03],\n",
      "        [-6.5720e-01,  2.6468e-01,  1.0531e-01],\n",
      "        [ 2.1086e-01,  3.1400e-01,  4.4757e-02],\n",
      "        [ 2.4759e-01, -6.6406e-02, -4.9679e-01],\n",
      "        [ 3.8372e-01, -4.8880e-01, -8.2679e-01],\n",
      "        [-1.4010e-01,  9.2563e-02,  8.9531e-03],\n",
      "        [-5.2117e-01,  2.3912e-01,  2.4036e-01],\n",
      "        [-3.4740e-01,  3.4340e-01, -2.3875e-01],\n",
      "        [-5.7444e-01,  3.8427e-01,  1.4523e-01],\n",
      "        [ 3.0552e-01, -5.8899e-02, -1.5848e-01],\n",
      "        [ 2.8567e-01,  4.9606e-01,  6.0035e-01],\n",
      "        [ 2.5038e-01,  7.1240e-02, -4.4214e-01],\n",
      "        [ 1.8584e-01,  3.0932e-01,  2.1598e-01],\n",
      "        [ 3.2121e-01,  8.3906e-02, -4.8969e-01],\n",
      "        [ 4.7350e-01, -6.2357e-01, -6.1559e-01],\n",
      "        [ 1.6784e-01,  2.6867e-01, -4.5752e-03],\n",
      "        [ 1.4771e-01, -5.3391e-02, -1.6963e-01],\n",
      "        [ 2.4910e-01, -5.7766e-02, -3.9333e-01],\n",
      "        [ 8.9342e-02, -3.7426e-02, -7.8375e-02],\n",
      "        [-1.9805e-02,  2.1693e-01, -5.9912e-02],\n",
      "        [-2.3795e-01,  3.4609e-01,  2.4575e-01],\n",
      "        [ 5.9117e-01, -2.4315e-01, -5.3894e-01],\n",
      "        [-5.0750e-01,  1.2369e-01, -2.4089e-01],\n",
      "        [-5.5409e-02, -1.6863e-01, -6.6378e-01],\n",
      "        [ 3.8459e-01,  3.6012e-01,  3.0323e-01],\n",
      "        [ 4.2526e-01,  6.1940e-01,  4.3991e-01],\n",
      "        [-3.2569e-01, -5.6753e-02,  1.2304e-01],\n",
      "        [-5.4749e-01,  2.7237e-01,  3.9849e-01],\n",
      "        [-4.3540e-03,  3.3517e-01, -2.5564e-02],\n",
      "        [-2.0418e-01, -2.0571e-02,  8.9498e-02],\n",
      "        [ 1.0102e-01,  8.8515e-03,  1.3753e-01],\n",
      "        [ 7.0938e-01, -1.9339e-01, -7.2665e-01],\n",
      "        [ 4.8730e-01,  1.7183e-01,  3.6054e-01],\n",
      "        [ 1.2477e-01, -3.0774e-02, -2.6857e-01],\n",
      "        [ 4.6604e-01,  9.8318e-02, -3.3157e-03],\n",
      "        [-4.5556e-01,  4.4974e-02, -1.2484e-01],\n",
      "        [ 6.2049e-01,  5.0968e-01,  2.0424e-01],\n",
      "        [ 4.3162e-01,  7.3181e-02, -1.8161e-01],\n",
      "        [-5.5960e-01,  2.6652e-01,  3.4054e-02],\n",
      "        [ 3.7255e-01,  2.5799e-01,  5.0298e-01],\n",
      "        [ 5.1183e-01,  3.2623e-02, -3.6312e-01],\n",
      "        [ 3.5335e-01,  2.7084e-01,  1.5483e-01],\n",
      "        [-2.4360e-01,  7.1490e-03,  2.8325e-02],\n",
      "        [-2.6292e-01,  3.1005e-01, -1.6250e-01],\n",
      "        [ 5.8854e-02, -9.9810e-02, -2.6488e-01],\n",
      "        [-1.9697e-01,  8.5961e-02, -5.6697e-01],\n",
      "        [-4.5767e-01, -6.4672e-02,  1.0355e-02],\n",
      "        [-4.4565e-02,  5.9836e-01,  4.0193e-01],\n",
      "        [ 1.9576e-01, -1.3637e-01,  7.3270e-02],\n",
      "        [-2.9687e-02, -3.8075e-01, -1.0116e-01],\n",
      "        [-8.6083e-02,  2.5870e-01, -9.4814e-02],\n",
      "        [-4.0888e-01,  1.0309e-01, -9.7249e-02],\n",
      "        [ 2.1505e-01,  5.7549e-02,  6.7922e-01],\n",
      "        [-2.4952e-01,  3.0663e-01, -4.1385e-01],\n",
      "        [-5.6905e-01,  7.6570e-02,  2.2670e-01],\n",
      "        [ 9.1998e-03,  3.0953e-01, -1.6813e-02],\n",
      "        [ 3.3040e-01,  5.0189e-01,  7.0530e-01],\n",
      "        [ 5.4972e-02,  1.1289e-01, -5.9443e-01],\n",
      "        [-7.1779e-01,  2.9608e-01,  1.7776e-01],\n",
      "        [-3.6532e-01,  4.7432e-01, -2.4891e-01],\n",
      "        [ 4.6061e-02,  3.3700e-02, -1.7286e-01],\n",
      "        [ 4.4181e-01,  5.3016e-01,  6.1724e-01],\n",
      "        [-4.8700e-01,  1.6200e-02, -1.6005e-01],\n",
      "        [ 8.2356e-01, -6.4489e-02, -5.9769e-02],\n",
      "        [-3.8787e-01,  1.3779e-01,  2.1771e-01],\n",
      "        [ 5.8370e-01,  8.9082e-02, -4.2475e-01],\n",
      "        [ 3.6075e-01,  2.5279e-01,  3.1267e-01],\n",
      "        [-1.6543e-01,  3.6681e-01, -2.5020e-01],\n",
      "        [ 3.6888e-01, -2.2831e-01,  7.9402e-03],\n",
      "        [ 2.9816e-01,  4.7397e-01,  4.0652e-01],\n",
      "        [ 3.6534e-02,  2.1693e-01, -3.5712e-01],\n",
      "        [-6.5906e-02,  2.2280e-01,  2.2810e-01],\n",
      "        [ 3.2231e-02,  5.4371e-01,  1.0499e+00],\n",
      "        [ 3.4923e-01,  2.1965e-02, -5.2403e-01],\n",
      "        [ 4.2251e-01, -2.9738e-01,  1.2327e-01],\n",
      "        [ 1.6485e-01,  1.2335e-01, -2.2863e-01],\n",
      "        [ 1.0849e-01,  3.3143e-01,  7.7203e-02],\n",
      "        [-3.7718e-02,  2.4340e-02,  3.8765e-02],\n",
      "        [ 1.5646e-01,  3.0057e-01, -6.2282e-01],\n",
      "        [-4.8812e-02,  2.6536e-01, -1.4635e-01],\n",
      "        [ 4.4289e-01, -2.5110e-01, -4.2943e-01],\n",
      "        [-1.9148e-02,  3.0369e-01, -1.5140e-01],\n",
      "        [ 1.7317e-01,  1.6171e-01, -2.0505e-01],\n",
      "        [-1.6656e-01, -1.0769e-01, -1.5125e-01],\n",
      "        [ 3.2267e-01,  1.4358e-01,  3.4830e-01],\n",
      "        [ 1.0631e-01, -4.7405e-01, -2.0728e-01],\n",
      "        [ 3.6716e-02,  3.2172e-01,  2.1131e-01],\n",
      "        [-3.4039e-01,  2.4490e-01,  2.2130e-03],\n",
      "        [-1.7595e-01,  3.7254e-01, -2.0849e-01],\n",
      "        [-5.9870e-01,  3.2831e-02, -1.4426e-02],\n",
      "        [-2.1013e-01,  1.6169e-01, -5.1524e-01],\n",
      "        [-6.7219e-01,  1.1532e-01,  8.0322e-02],\n",
      "        [ 4.0395e-01,  2.4597e-01,  3.5413e-01],\n",
      "        [-1.6114e-02, -3.7288e-03, -1.7090e-01],\n",
      "        [-1.3447e-01,  2.9788e-01,  8.2298e-02],\n",
      "        [-3.4832e-01,  6.1649e-02,  2.1773e-01],\n",
      "        [ 1.4705e-01,  7.2960e-02, -2.4666e-01],\n",
      "        [ 1.6225e-01,  6.1558e-01,  1.0485e+00],\n",
      "        [ 6.7307e-02,  1.5308e-01, -5.9424e-01],\n",
      "        [ 1.6908e-02, -1.3231e-01,  1.7059e-01],\n",
      "        [ 8.2105e-02, -5.5394e-02,  4.8508e-02],\n",
      "        [-6.3881e-01,  1.4948e-01,  1.8585e-02],\n",
      "        [ 1.8701e-01,  5.9859e-03, -4.1342e-01],\n",
      "        [-4.4928e-01,  1.0835e-01, -1.9505e-01],\n",
      "        [ 2.4899e-01,  6.6745e-02,  6.1046e-01],\n",
      "        [ 3.8894e-01,  2.5165e-01,  2.1810e-01],\n",
      "        [-1.5140e-01, -6.3843e-01, -4.7785e-01],\n",
      "        [ 6.2819e-03, -6.6937e-02,  4.8872e-02],\n",
      "        [ 1.6842e-01, -8.2102e-02,  1.8523e-01],\n",
      "        [ 5.9337e-01, -4.8214e-02, -4.6181e-01],\n",
      "        [ 6.0566e-02, -6.5123e-01,  1.6233e-01],\n",
      "        [ 3.3625e-01,  1.5574e-01, -1.6433e-01],\n",
      "        [-4.1796e-01,  3.2799e-03,  1.2415e-02],\n",
      "        [-2.8327e-01, -1.0098e-01, -2.3697e-01],\n",
      "        [ 2.8312e-01, -3.5136e-01, -2.1011e-01],\n",
      "        [ 3.6027e-02,  7.7791e-01,  1.2175e+00],\n",
      "        [ 3.7479e-01,  4.5991e-01,  3.8856e-01],\n",
      "        [ 2.5428e-01,  2.3850e-01, -2.8208e-01],\n",
      "        [-7.6191e-01,  5.9854e-01,  2.4553e-01],\n",
      "        [ 2.6362e-01,  7.6052e-02, -6.6400e-01],\n",
      "        [-9.9246e-02, -3.9246e-02,  8.5284e-02],\n",
      "        [ 7.0122e-02,  9.4657e-02,  1.1841e-01],\n",
      "        [ 5.2918e-01, -6.4441e-01, -7.1581e-01],\n",
      "        [-2.3225e-01,  5.4990e-01,  1.4212e-01],\n",
      "        [-8.9180e-02,  3.0286e-01, -1.9385e-01],\n",
      "        [-4.4147e-02, -6.2150e-01, -3.5500e-01],\n",
      "        [ 2.8777e-01, -2.0077e-01, -3.2877e-03],\n",
      "        [-4.8072e-01, -1.9828e-02,  2.1851e-01],\n",
      "        [ 2.9717e-01,  4.8939e-01,  6.1463e-01],\n",
      "        [-3.0167e-01,  3.5380e-01, -2.3194e-01],\n",
      "        [ 3.5105e-01,  2.3240e-01, -1.3622e-01],\n",
      "        [ 4.8915e-01,  3.1334e-01,  2.7781e-01],\n",
      "        [-4.8432e-01,  4.3832e-02, -1.2151e-01],\n",
      "        [ 1.2028e-01,  4.2780e-01,  3.6812e-02],\n",
      "        [-1.4860e-02, -3.8614e-01, -9.7464e-02],\n",
      "        [ 1.8778e-01,  4.6904e-01,  1.6810e-01],\n",
      "        [-5.2713e-01, -1.6061e-01, -4.9540e-02],\n",
      "        [ 3.8932e-01,  1.0625e-01, -2.1603e-01],\n",
      "        [-2.8894e-01,  9.2964e-02,  6.5960e-02],\n",
      "        [ 2.7566e-02, -1.2373e-01, -3.1397e-01],\n",
      "        [ 3.6802e-01, -5.1896e-02,  4.9868e-01],\n",
      "        [ 3.8934e-02, -7.7202e-02, -1.7300e-01],\n",
      "        [ 5.0481e-01, -5.1766e-02, -1.1622e-03],\n",
      "        [-4.4320e-01,  9.6993e-02, -1.8729e-01],\n",
      "        [-2.5392e-01,  1.4184e-01, -3.0427e-01],\n",
      "        [-7.0777e-02,  2.2990e-01,  2.0945e-01],\n",
      "        [-3.5807e-01, -2.9017e-01, -5.1752e-02],\n",
      "        [ 2.9059e-01,  2.2902e-01, -1.6839e-01],\n",
      "        [-3.4050e-01, -1.8934e-01,  2.1111e-02],\n",
      "        [-1.5236e-01,  3.2547e-01, -1.3212e-01],\n",
      "        [-1.3790e-01, -2.2693e-01, -6.7986e-02],\n",
      "        [-7.9245e-01,  2.6862e-02,  5.5069e-01],\n",
      "        [ 5.7069e-02,  2.2193e-01, -3.5232e-01],\n",
      "        [ 4.1063e-01, -2.9897e-01,  8.5775e-02],\n",
      "        [ 2.4099e-01,  3.6710e-01,  9.1990e-02],\n",
      "        [ 2.8409e-01, -1.9834e-01, -2.7664e-01],\n",
      "        [-1.1570e-03, -2.0211e-01,  3.0657e-01],\n",
      "        [ 1.4472e-01,  2.1207e-01, -9.0496e-02],\n",
      "        [-4.1340e-01,  3.5347e-01, -2.2620e-01],\n",
      "        [-1.0424e-01,  2.3066e-01, -3.7249e-01],\n",
      "        [ 4.0820e-01,  4.2939e-03, -4.2921e-01],\n",
      "        [ 4.1325e-01, -8.0862e-02, -1.6961e-01]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([332, 3])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(lin(Xt_train))\n",
    "lin(Xt_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e8b25001",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dab00481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0986, grad_fn=<DivBackward1>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Qui usiamo tutto il dataset di training, quindi niente minibatch\n",
    "\n",
    "loss = criterion(lin(Xt_train), zt_train)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6ec52174",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0e89bc3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2478, -0.2807, -0.3126, -0.2563, -0.2791,  0.1835,  0.0339],\n",
       "        [-0.0806,  0.1151,  0.1287,  0.0995,  0.1035, -0.0766,  0.0549],\n",
       "        [-0.1672,  0.1657,  0.1840,  0.1568,  0.1756, -0.1069, -0.0887]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9b057fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2883,  0.1771,  0.1112])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin.bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "76a0ee8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, train loss: 1.0986, test loss: 1.13792, train acc: 0.412651, test acc: 0.38983\n",
      "1, train loss: 1.09092, test loss: 1.13005, train acc: 0.415663, test acc: 0.40678\n",
      "2, train loss: 1.08344, test loss: 1.12237, train acc: 0.418675, test acc: 0.40678\n",
      "3, train loss: 1.07616, test loss: 1.11489, train acc: 0.439759, test acc: 0.38983\n",
      "4, train loss: 1.06906, test loss: 1.10759, train acc: 0.457831, test acc: 0.423729\n",
      "5, train loss: 1.06216, test loss: 1.10047, train acc: 0.48494, test acc: 0.423729\n",
      "6, train loss: 1.05544, test loss: 1.09353, train acc: 0.490964, test acc: 0.440678\n",
      "7, train loss: 1.04889, test loss: 1.08676, train acc: 0.493976, test acc: 0.457627\n",
      "8, train loss: 1.04252, test loss: 1.08017, train acc: 0.493976, test acc: 0.457627\n",
      "9, train loss: 1.03633, test loss: 1.07374, train acc: 0.509036, test acc: 0.457627\n",
      "10, train loss: 1.03029, test loss: 1.06748, train acc: 0.524096, test acc: 0.474576\n",
      "11, train loss: 1.02442, test loss: 1.06137, train acc: 0.53012, test acc: 0.491525\n",
      "12, train loss: 1.0187, test loss: 1.05542, train acc: 0.539157, test acc: 0.525424\n",
      "13, train loss: 1.01314, test loss: 1.04962, train acc: 0.548193, test acc: 0.525424\n",
      "14, train loss: 1.00773, test loss: 1.04397, train acc: 0.551205, test acc: 0.525424\n",
      "15, train loss: 1.00246, test loss: 1.03846, train acc: 0.545181, test acc: 0.542373\n",
      "16, train loss: 0.997326, test loss: 1.03309, train acc: 0.554217, test acc: 0.559322\n",
      "17, train loss: 0.992333, test loss: 1.02786, train acc: 0.569277, test acc: 0.559322\n",
      "18, train loss: 0.987472, test loss: 1.02276, train acc: 0.572289, test acc: 0.559322\n",
      "19, train loss: 0.98274, test loss: 1.01779, train acc: 0.578313, test acc: 0.576271\n",
      "20, train loss: 0.978133, test loss: 1.01294, train acc: 0.584337, test acc: 0.576271\n",
      "21, train loss: 0.973646, test loss: 1.00822, train acc: 0.590361, test acc: 0.576271\n",
      "22, train loss: 0.969278, test loss: 1.00361, train acc: 0.593373, test acc: 0.576271\n",
      "23, train loss: 0.965023, test loss: 0.999122, train acc: 0.599398, test acc: 0.576271\n",
      "24, train loss: 0.960878, test loss: 0.994742, train acc: 0.599398, test acc: 0.576271\n",
      "25, train loss: 0.956841, test loss: 0.99047, train acc: 0.60241, test acc: 0.59322\n",
      "26, train loss: 0.952907, test loss: 0.986304, train acc: 0.611446, test acc: 0.610169\n",
      "27, train loss: 0.949073, test loss: 0.982239, train acc: 0.614458, test acc: 0.610169\n",
      "28, train loss: 0.945337, test loss: 0.978273, train acc: 0.614458, test acc: 0.610169\n",
      "29, train loss: 0.941695, test loss: 0.974403, train acc: 0.61747, test acc: 0.610169\n",
      "30, train loss: 0.938144, test loss: 0.970627, train acc: 0.620482, test acc: 0.610169\n",
      "31, train loss: 0.934681, test loss: 0.96694, train acc: 0.623494, test acc: 0.610169\n",
      "32, train loss: 0.931304, test loss: 0.963341, train acc: 0.626506, test acc: 0.610169\n",
      "33, train loss: 0.92801, test loss: 0.959827, train acc: 0.629518, test acc: 0.59322\n",
      "34, train loss: 0.924795, test loss: 0.956396, train acc: 0.63253, test acc: 0.59322\n",
      "35, train loss: 0.921658, test loss: 0.953044, train acc: 0.63253, test acc: 0.59322\n",
      "36, train loss: 0.918596, test loss: 0.949769, train acc: 0.635542, test acc: 0.59322\n",
      "37, train loss: 0.915607, test loss: 0.946569, train acc: 0.63253, test acc: 0.59322\n",
      "38, train loss: 0.912687, test loss: 0.943442, train acc: 0.63253, test acc: 0.59322\n",
      "39, train loss: 0.909836, test loss: 0.940386, train acc: 0.63253, test acc: 0.59322\n",
      "40, train loss: 0.90705, test loss: 0.937397, train acc: 0.63253, test acc: 0.59322\n",
      "41, train loss: 0.904328, test loss: 0.934475, train acc: 0.626506, test acc: 0.59322\n",
      "42, train loss: 0.901668, test loss: 0.931616, train acc: 0.626506, test acc: 0.59322\n",
      "43, train loss: 0.899067, test loss: 0.92882, train acc: 0.626506, test acc: 0.59322\n",
      "44, train loss: 0.896523, test loss: 0.926084, train acc: 0.626506, test acc: 0.59322\n",
      "45, train loss: 0.894036, test loss: 0.923406, train acc: 0.626506, test acc: 0.59322\n",
      "46, train loss: 0.891602, test loss: 0.920785, train acc: 0.629518, test acc: 0.59322\n",
      "47, train loss: 0.889221, test loss: 0.918219, train acc: 0.629518, test acc: 0.59322\n",
      "48, train loss: 0.88689, test loss: 0.915706, train acc: 0.629518, test acc: 0.59322\n",
      "49, train loss: 0.884608, test loss: 0.913244, train acc: 0.629518, test acc: 0.59322\n",
      "50, train loss: 0.882374, test loss: 0.910833, train acc: 0.626506, test acc: 0.59322\n",
      "51, train loss: 0.880185, test loss: 0.908469, train acc: 0.626506, test acc: 0.59322\n",
      "52, train loss: 0.878041, test loss: 0.906153, train acc: 0.626506, test acc: 0.610169\n",
      "53, train loss: 0.87594, test loss: 0.903883, train acc: 0.626506, test acc: 0.610169\n",
      "54, train loss: 0.873881, test loss: 0.901656, train acc: 0.629518, test acc: 0.610169\n",
      "55, train loss: 0.871862, test loss: 0.899473, train acc: 0.629518, test acc: 0.610169\n",
      "56, train loss: 0.869882, test loss: 0.89733, train acc: 0.63253, test acc: 0.610169\n",
      "57, train loss: 0.86794, test loss: 0.895229, train acc: 0.63253, test acc: 0.610169\n",
      "58, train loss: 0.866035, test loss: 0.893166, train acc: 0.629518, test acc: 0.610169\n",
      "59, train loss: 0.864166, test loss: 0.891142, train acc: 0.63253, test acc: 0.610169\n",
      "60, train loss: 0.862331, test loss: 0.889154, train acc: 0.63253, test acc: 0.610169\n",
      "61, train loss: 0.860529, test loss: 0.887203, train acc: 0.63253, test acc: 0.610169\n",
      "62, train loss: 0.85876, test loss: 0.885286, train acc: 0.635542, test acc: 0.610169\n",
      "63, train loss: 0.857023, test loss: 0.883403, train acc: 0.635542, test acc: 0.610169\n",
      "64, train loss: 0.855316, test loss: 0.881552, train acc: 0.635542, test acc: 0.610169\n",
      "65, train loss: 0.853638, test loss: 0.879734, train acc: 0.635542, test acc: 0.610169\n",
      "66, train loss: 0.85199, test loss: 0.877947, train acc: 0.635542, test acc: 0.610169\n",
      "67, train loss: 0.850369, test loss: 0.876189, train acc: 0.635542, test acc: 0.610169\n",
      "68, train loss: 0.848776, test loss: 0.874461, train acc: 0.635542, test acc: 0.610169\n",
      "69, train loss: 0.847208, test loss: 0.872762, train acc: 0.635542, test acc: 0.610169\n",
      "70, train loss: 0.845667, test loss: 0.87109, train acc: 0.635542, test acc: 0.610169\n",
      "71, train loss: 0.84415, test loss: 0.869445, train acc: 0.635542, test acc: 0.59322\n",
      "72, train loss: 0.842657, test loss: 0.867826, train acc: 0.641566, test acc: 0.59322\n",
      "73, train loss: 0.841188, test loss: 0.866233, train acc: 0.641566, test acc: 0.59322\n",
      "74, train loss: 0.839742, test loss: 0.864664, train acc: 0.644578, test acc: 0.59322\n",
      "75, train loss: 0.838317, test loss: 0.86312, train acc: 0.644578, test acc: 0.59322\n",
      "76, train loss: 0.836914, test loss: 0.861599, train acc: 0.644578, test acc: 0.59322\n",
      "77, train loss: 0.835533, test loss: 0.860101, train acc: 0.644578, test acc: 0.59322\n",
      "78, train loss: 0.834171, test loss: 0.858625, train acc: 0.644578, test acc: 0.59322\n",
      "79, train loss: 0.832829, test loss: 0.85717, train acc: 0.644578, test acc: 0.59322\n",
      "80, train loss: 0.831507, test loss: 0.855737, train acc: 0.644578, test acc: 0.59322\n",
      "81, train loss: 0.830203, test loss: 0.854324, train acc: 0.644578, test acc: 0.59322\n",
      "82, train loss: 0.828918, test loss: 0.852932, train acc: 0.644578, test acc: 0.59322\n",
      "83, train loss: 0.82765, test loss: 0.851559, train acc: 0.644578, test acc: 0.59322\n",
      "84, train loss: 0.8264, test loss: 0.850205, train acc: 0.641566, test acc: 0.59322\n",
      "85, train loss: 0.825167, test loss: 0.848869, train acc: 0.641566, test acc: 0.59322\n",
      "86, train loss: 0.82395, test loss: 0.847552, train acc: 0.641566, test acc: 0.59322\n",
      "87, train loss: 0.822749, test loss: 0.846252, train acc: 0.641566, test acc: 0.59322\n",
      "88, train loss: 0.821564, test loss: 0.84497, train acc: 0.641566, test acc: 0.59322\n",
      "89, train loss: 0.820394, test loss: 0.843704, train acc: 0.641566, test acc: 0.610169\n",
      "90, train loss: 0.819238, test loss: 0.842455, train acc: 0.638554, test acc: 0.610169\n",
      "91, train loss: 0.818097, test loss: 0.841221, train acc: 0.638554, test acc: 0.610169\n",
      "92, train loss: 0.816971, test loss: 0.840003, train acc: 0.638554, test acc: 0.59322\n",
      "93, train loss: 0.815858, test loss: 0.838801, train acc: 0.638554, test acc: 0.59322\n",
      "94, train loss: 0.814758, test loss: 0.837614, train acc: 0.641566, test acc: 0.59322\n",
      "95, train loss: 0.813672, test loss: 0.83644, train acc: 0.641566, test acc: 0.59322\n",
      "96, train loss: 0.812599, test loss: 0.835282, train acc: 0.641566, test acc: 0.59322\n",
      "97, train loss: 0.811537, test loss: 0.834137, train acc: 0.641566, test acc: 0.59322\n",
      "98, train loss: 0.810489, test loss: 0.833005, train acc: 0.641566, test acc: 0.59322\n",
      "99, train loss: 0.809452, test loss: 0.831887, train acc: 0.641566, test acc: 0.59322\n",
      "100, train loss: 0.808426, test loss: 0.830782, train acc: 0.641566, test acc: 0.59322\n",
      "101, train loss: 0.807413, test loss: 0.82969, train acc: 0.641566, test acc: 0.59322\n",
      "102, train loss: 0.80641, test loss: 0.82861, train acc: 0.641566, test acc: 0.59322\n",
      "103, train loss: 0.805418, test loss: 0.827542, train acc: 0.641566, test acc: 0.59322\n",
      "104, train loss: 0.804437, test loss: 0.826486, train acc: 0.641566, test acc: 0.59322\n",
      "105, train loss: 0.803466, test loss: 0.825442, train acc: 0.641566, test acc: 0.59322\n",
      "106, train loss: 0.802505, test loss: 0.824409, train acc: 0.641566, test acc: 0.59322\n",
      "107, train loss: 0.801555, test loss: 0.823387, train acc: 0.641566, test acc: 0.59322\n",
      "108, train loss: 0.800614, test loss: 0.822376, train acc: 0.641566, test acc: 0.59322\n",
      "109, train loss: 0.799682, test loss: 0.821376, train acc: 0.641566, test acc: 0.59322\n",
      "110, train loss: 0.79876, test loss: 0.820386, train acc: 0.641566, test acc: 0.59322\n",
      "111, train loss: 0.797847, test loss: 0.819407, train acc: 0.641566, test acc: 0.59322\n",
      "112, train loss: 0.796943, test loss: 0.818437, train acc: 0.641566, test acc: 0.59322\n",
      "113, train loss: 0.796048, test loss: 0.817478, train acc: 0.641566, test acc: 0.59322\n",
      "114, train loss: 0.795162, test loss: 0.816528, train acc: 0.641566, test acc: 0.59322\n",
      "115, train loss: 0.794284, test loss: 0.815587, train acc: 0.641566, test acc: 0.59322\n",
      "116, train loss: 0.793414, test loss: 0.814656, train acc: 0.641566, test acc: 0.59322\n",
      "117, train loss: 0.792552, test loss: 0.813734, train acc: 0.641566, test acc: 0.59322\n",
      "118, train loss: 0.791698, test loss: 0.81282, train acc: 0.641566, test acc: 0.59322\n",
      "119, train loss: 0.790852, test loss: 0.811916, train acc: 0.641566, test acc: 0.59322\n",
      "120, train loss: 0.790013, test loss: 0.81102, train acc: 0.641566, test acc: 0.59322\n",
      "121, train loss: 0.789182, test loss: 0.810133, train acc: 0.638554, test acc: 0.59322\n",
      "122, train loss: 0.788359, test loss: 0.809253, train acc: 0.638554, test acc: 0.59322\n",
      "123, train loss: 0.787542, test loss: 0.808382, train acc: 0.641566, test acc: 0.59322\n",
      "124, train loss: 0.786733, test loss: 0.807519, train acc: 0.641566, test acc: 0.59322\n",
      "125, train loss: 0.785931, test loss: 0.806664, train acc: 0.641566, test acc: 0.59322\n",
      "126, train loss: 0.785135, test loss: 0.805816, train acc: 0.641566, test acc: 0.59322\n",
      "127, train loss: 0.784346, test loss: 0.804976, train acc: 0.638554, test acc: 0.610169\n",
      "128, train loss: 0.783564, test loss: 0.804143, train acc: 0.638554, test acc: 0.610169\n",
      "129, train loss: 0.782788, test loss: 0.803318, train acc: 0.638554, test acc: 0.610169\n",
      "130, train loss: 0.782018, test loss: 0.8025, train acc: 0.638554, test acc: 0.610169\n",
      "131, train loss: 0.781255, test loss: 0.801688, train acc: 0.638554, test acc: 0.610169\n",
      "132, train loss: 0.780498, test loss: 0.800884, train acc: 0.638554, test acc: 0.610169\n",
      "133, train loss: 0.779747, test loss: 0.800087, train acc: 0.638554, test acc: 0.610169\n",
      "134, train loss: 0.779002, test loss: 0.799296, train acc: 0.638554, test acc: 0.610169\n",
      "135, train loss: 0.778262, test loss: 0.798512, train acc: 0.638554, test acc: 0.610169\n",
      "136, train loss: 0.777529, test loss: 0.797734, train acc: 0.638554, test acc: 0.610169\n",
      "137, train loss: 0.776801, test loss: 0.796962, train acc: 0.638554, test acc: 0.610169\n",
      "138, train loss: 0.776078, test loss: 0.796197, train acc: 0.638554, test acc: 0.610169\n",
      "139, train loss: 0.775361, test loss: 0.795438, train acc: 0.638554, test acc: 0.610169\n",
      "140, train loss: 0.77465, test loss: 0.794685, train acc: 0.638554, test acc: 0.610169\n",
      "141, train loss: 0.773943, test loss: 0.793939, train acc: 0.638554, test acc: 0.610169\n",
      "142, train loss: 0.773242, test loss: 0.793197, train acc: 0.638554, test acc: 0.610169\n",
      "143, train loss: 0.772546, test loss: 0.792462, train acc: 0.644578, test acc: 0.610169\n",
      "144, train loss: 0.771855, test loss: 0.791733, train acc: 0.644578, test acc: 0.610169\n",
      "145, train loss: 0.771169, test loss: 0.791009, train acc: 0.644578, test acc: 0.610169\n",
      "146, train loss: 0.770488, test loss: 0.79029, train acc: 0.644578, test acc: 0.610169\n",
      "147, train loss: 0.769812, test loss: 0.789577, train acc: 0.644578, test acc: 0.610169\n",
      "148, train loss: 0.76914, test loss: 0.788869, train acc: 0.644578, test acc: 0.610169\n",
      "149, train loss: 0.768473, test loss: 0.788167, train acc: 0.644578, test acc: 0.610169\n",
      "150, train loss: 0.767811, test loss: 0.78747, train acc: 0.644578, test acc: 0.610169\n",
      "151, train loss: 0.767153, test loss: 0.786778, train acc: 0.644578, test acc: 0.610169\n",
      "152, train loss: 0.7665, test loss: 0.786091, train acc: 0.64759, test acc: 0.610169\n",
      "153, train loss: 0.765851, test loss: 0.785409, train acc: 0.64759, test acc: 0.610169\n",
      "154, train loss: 0.765207, test loss: 0.784732, train acc: 0.64759, test acc: 0.610169\n",
      "155, train loss: 0.764567, test loss: 0.784059, train acc: 0.64759, test acc: 0.610169\n",
      "156, train loss: 0.763931, test loss: 0.783392, train acc: 0.64759, test acc: 0.610169\n",
      "157, train loss: 0.763299, test loss: 0.782729, train acc: 0.64759, test acc: 0.610169\n",
      "158, train loss: 0.762671, test loss: 0.782071, train acc: 0.64759, test acc: 0.610169\n",
      "159, train loss: 0.762048, test loss: 0.781418, train acc: 0.64759, test acc: 0.610169\n",
      "160, train loss: 0.761428, test loss: 0.780769, train acc: 0.64759, test acc: 0.610169\n",
      "161, train loss: 0.760813, test loss: 0.780124, train acc: 0.64759, test acc: 0.610169\n",
      "162, train loss: 0.760201, test loss: 0.779484, train acc: 0.64759, test acc: 0.610169\n",
      "163, train loss: 0.759593, test loss: 0.778848, train acc: 0.64759, test acc: 0.610169\n",
      "164, train loss: 0.75899, test loss: 0.778217, train acc: 0.64759, test acc: 0.610169\n",
      "165, train loss: 0.758389, test loss: 0.777589, train acc: 0.64759, test acc: 0.610169\n",
      "166, train loss: 0.757793, test loss: 0.776966, train acc: 0.64759, test acc: 0.610169\n",
      "167, train loss: 0.7572, test loss: 0.776348, train acc: 0.64759, test acc: 0.610169\n",
      "168, train loss: 0.756611, test loss: 0.775733, train acc: 0.64759, test acc: 0.610169\n",
      "169, train loss: 0.756025, test loss: 0.775122, train acc: 0.64759, test acc: 0.610169\n",
      "170, train loss: 0.755443, test loss: 0.774515, train acc: 0.64759, test acc: 0.610169\n",
      "171, train loss: 0.754865, test loss: 0.773912, train acc: 0.64759, test acc: 0.610169\n",
      "172, train loss: 0.75429, test loss: 0.773313, train acc: 0.64759, test acc: 0.610169\n",
      "173, train loss: 0.753718, test loss: 0.772718, train acc: 0.64759, test acc: 0.610169\n",
      "174, train loss: 0.75315, test loss: 0.772127, train acc: 0.64759, test acc: 0.610169\n",
      "175, train loss: 0.752585, test loss: 0.771539, train acc: 0.64759, test acc: 0.610169\n",
      "176, train loss: 0.752024, test loss: 0.770955, train acc: 0.650602, test acc: 0.610169\n",
      "177, train loss: 0.751465, test loss: 0.770375, train acc: 0.650602, test acc: 0.610169\n",
      "178, train loss: 0.75091, test loss: 0.769798, train acc: 0.650602, test acc: 0.610169\n",
      "179, train loss: 0.750358, test loss: 0.769225, train acc: 0.650602, test acc: 0.610169\n",
      "180, train loss: 0.74981, test loss: 0.768656, train acc: 0.650602, test acc: 0.610169\n",
      "181, train loss: 0.749264, test loss: 0.76809, train acc: 0.650602, test acc: 0.610169\n",
      "182, train loss: 0.748722, test loss: 0.767527, train acc: 0.653614, test acc: 0.610169\n",
      "183, train loss: 0.748182, test loss: 0.766968, train acc: 0.653614, test acc: 0.610169\n",
      "184, train loss: 0.747646, test loss: 0.766412, train acc: 0.656627, test acc: 0.610169\n",
      "185, train loss: 0.747112, test loss: 0.76586, train acc: 0.656627, test acc: 0.610169\n",
      "186, train loss: 0.746582, test loss: 0.765311, train acc: 0.656627, test acc: 0.610169\n",
      "187, train loss: 0.746054, test loss: 0.764765, train acc: 0.656627, test acc: 0.610169\n",
      "188, train loss: 0.74553, test loss: 0.764223, train acc: 0.656627, test acc: 0.610169\n",
      "189, train loss: 0.745008, test loss: 0.763683, train acc: 0.656627, test acc: 0.610169\n",
      "190, train loss: 0.744489, test loss: 0.763147, train acc: 0.656627, test acc: 0.610169\n",
      "191, train loss: 0.743973, test loss: 0.762614, train acc: 0.656627, test acc: 0.610169\n",
      "192, train loss: 0.74346, test loss: 0.762084, train acc: 0.656627, test acc: 0.610169\n",
      "193, train loss: 0.74295, test loss: 0.761558, train acc: 0.653614, test acc: 0.610169\n",
      "194, train loss: 0.742442, test loss: 0.761034, train acc: 0.653614, test acc: 0.610169\n",
      "195, train loss: 0.741937, test loss: 0.760513, train acc: 0.653614, test acc: 0.610169\n",
      "196, train loss: 0.741435, test loss: 0.759995, train acc: 0.650602, test acc: 0.610169\n",
      "197, train loss: 0.740935, test loss: 0.75948, train acc: 0.650602, test acc: 0.610169\n",
      "198, train loss: 0.740438, test loss: 0.758968, train acc: 0.650602, test acc: 0.610169\n",
      "199, train loss: 0.739943, test loss: 0.75846, train acc: 0.650602, test acc: 0.610169\n",
      "200, train loss: 0.739452, test loss: 0.757953, train acc: 0.650602, test acc: 0.610169\n",
      "201, train loss: 0.738962, test loss: 0.75745, train acc: 0.650602, test acc: 0.610169\n",
      "202, train loss: 0.738476, test loss: 0.75695, train acc: 0.650602, test acc: 0.610169\n",
      "203, train loss: 0.737991, test loss: 0.756452, train acc: 0.650602, test acc: 0.610169\n",
      "204, train loss: 0.73751, test loss: 0.755957, train acc: 0.650602, test acc: 0.610169\n",
      "205, train loss: 0.73703, test loss: 0.755465, train acc: 0.650602, test acc: 0.610169\n",
      "206, train loss: 0.736554, test loss: 0.754975, train acc: 0.650602, test acc: 0.610169\n",
      "207, train loss: 0.736079, test loss: 0.754489, train acc: 0.650602, test acc: 0.610169\n",
      "208, train loss: 0.735607, test loss: 0.754005, train acc: 0.650602, test acc: 0.610169\n",
      "209, train loss: 0.735138, test loss: 0.753523, train acc: 0.650602, test acc: 0.610169\n",
      "210, train loss: 0.734671, test loss: 0.753044, train acc: 0.650602, test acc: 0.610169\n",
      "211, train loss: 0.734206, test loss: 0.752568, train acc: 0.650602, test acc: 0.610169\n",
      "212, train loss: 0.733743, test loss: 0.752094, train acc: 0.650602, test acc: 0.610169\n",
      "213, train loss: 0.733283, test loss: 0.751623, train acc: 0.650602, test acc: 0.610169\n",
      "214, train loss: 0.732825, test loss: 0.751154, train acc: 0.650602, test acc: 0.610169\n",
      "215, train loss: 0.73237, test loss: 0.750688, train acc: 0.650602, test acc: 0.610169\n",
      "216, train loss: 0.731916, test loss: 0.750225, train acc: 0.650602, test acc: 0.627119\n",
      "217, train loss: 0.731465, test loss: 0.749763, train acc: 0.650602, test acc: 0.627119\n",
      "218, train loss: 0.731016, test loss: 0.749305, train acc: 0.650602, test acc: 0.627119\n",
      "219, train loss: 0.730569, test loss: 0.748848, train acc: 0.650602, test acc: 0.644068\n",
      "220, train loss: 0.730125, test loss: 0.748394, train acc: 0.650602, test acc: 0.644068\n",
      "221, train loss: 0.729683, test loss: 0.747943, train acc: 0.650602, test acc: 0.644068\n",
      "222, train loss: 0.729242, test loss: 0.747494, train acc: 0.650602, test acc: 0.644068\n",
      "223, train loss: 0.728804, test loss: 0.747047, train acc: 0.650602, test acc: 0.644068\n",
      "224, train loss: 0.728368, test loss: 0.746602, train acc: 0.650602, test acc: 0.644068\n",
      "225, train loss: 0.727934, test loss: 0.74616, train acc: 0.650602, test acc: 0.644068\n",
      "226, train loss: 0.727502, test loss: 0.74572, train acc: 0.650602, test acc: 0.644068\n",
      "227, train loss: 0.727073, test loss: 0.745282, train acc: 0.650602, test acc: 0.644068\n",
      "228, train loss: 0.726645, test loss: 0.744847, train acc: 0.650602, test acc: 0.627119\n",
      "229, train loss: 0.726219, test loss: 0.744413, train acc: 0.650602, test acc: 0.627119\n",
      "230, train loss: 0.725796, test loss: 0.743982, train acc: 0.64759, test acc: 0.627119\n",
      "231, train loss: 0.725374, test loss: 0.743554, train acc: 0.64759, test acc: 0.627119\n",
      "232, train loss: 0.724955, test loss: 0.743127, train acc: 0.64759, test acc: 0.627119\n",
      "233, train loss: 0.724537, test loss: 0.742702, train acc: 0.64759, test acc: 0.627119\n",
      "234, train loss: 0.724121, test loss: 0.74228, train acc: 0.64759, test acc: 0.627119\n",
      "235, train loss: 0.723707, test loss: 0.74186, train acc: 0.64759, test acc: 0.644068\n",
      "236, train loss: 0.723296, test loss: 0.741441, train acc: 0.650602, test acc: 0.644068\n",
      "237, train loss: 0.722886, test loss: 0.741025, train acc: 0.650602, test acc: 0.644068\n",
      "238, train loss: 0.722478, test loss: 0.740611, train acc: 0.650602, test acc: 0.644068\n",
      "239, train loss: 0.722072, test loss: 0.740199, train acc: 0.653614, test acc: 0.644068\n",
      "240, train loss: 0.721667, test loss: 0.739789, train acc: 0.653614, test acc: 0.644068\n",
      "241, train loss: 0.721265, test loss: 0.739382, train acc: 0.653614, test acc: 0.644068\n",
      "242, train loss: 0.720865, test loss: 0.738976, train acc: 0.653614, test acc: 0.644068\n",
      "243, train loss: 0.720466, test loss: 0.738572, train acc: 0.653614, test acc: 0.644068\n",
      "244, train loss: 0.720069, test loss: 0.73817, train acc: 0.653614, test acc: 0.644068\n",
      "245, train loss: 0.719674, test loss: 0.73777, train acc: 0.653614, test acc: 0.644068\n",
      "246, train loss: 0.719281, test loss: 0.737372, train acc: 0.653614, test acc: 0.644068\n",
      "247, train loss: 0.718889, test loss: 0.736976, train acc: 0.653614, test acc: 0.644068\n",
      "248, train loss: 0.7185, test loss: 0.736582, train acc: 0.653614, test acc: 0.644068\n",
      "249, train loss: 0.718112, test loss: 0.73619, train acc: 0.653614, test acc: 0.644068\n",
      "250, train loss: 0.717726, test loss: 0.735799, train acc: 0.653614, test acc: 0.644068\n",
      "251, train loss: 0.717341, test loss: 0.735411, train acc: 0.653614, test acc: 0.644068\n",
      "252, train loss: 0.716959, test loss: 0.735024, train acc: 0.656627, test acc: 0.644068\n",
      "253, train loss: 0.716578, test loss: 0.73464, train acc: 0.656627, test acc: 0.644068\n",
      "254, train loss: 0.716199, test loss: 0.734257, train acc: 0.656627, test acc: 0.644068\n",
      "255, train loss: 0.715821, test loss: 0.733876, train acc: 0.656627, test acc: 0.644068\n",
      "256, train loss: 0.715445, test loss: 0.733497, train acc: 0.656627, test acc: 0.644068\n",
      "257, train loss: 0.715071, test loss: 0.733119, train acc: 0.656627, test acc: 0.644068\n",
      "258, train loss: 0.714699, test loss: 0.732744, train acc: 0.656627, test acc: 0.644068\n",
      "259, train loss: 0.714328, test loss: 0.73237, train acc: 0.656627, test acc: 0.644068\n",
      "260, train loss: 0.713959, test loss: 0.731998, train acc: 0.656627, test acc: 0.644068\n",
      "261, train loss: 0.713591, test loss: 0.731628, train acc: 0.656627, test acc: 0.644068\n",
      "262, train loss: 0.713225, test loss: 0.731259, train acc: 0.656627, test acc: 0.644068\n",
      "263, train loss: 0.712861, test loss: 0.730892, train acc: 0.656627, test acc: 0.644068\n",
      "264, train loss: 0.712498, test loss: 0.730527, train acc: 0.656627, test acc: 0.644068\n",
      "265, train loss: 0.712137, test loss: 0.730164, train acc: 0.656627, test acc: 0.644068\n",
      "266, train loss: 0.711778, test loss: 0.729802, train acc: 0.656627, test acc: 0.644068\n",
      "267, train loss: 0.71142, test loss: 0.729442, train acc: 0.656627, test acc: 0.644068\n",
      "268, train loss: 0.711063, test loss: 0.729084, train acc: 0.656627, test acc: 0.644068\n",
      "269, train loss: 0.710708, test loss: 0.728727, train acc: 0.659639, test acc: 0.644068\n",
      "270, train loss: 0.710355, test loss: 0.728372, train acc: 0.659639, test acc: 0.644068\n",
      "271, train loss: 0.710003, test loss: 0.728019, train acc: 0.659639, test acc: 0.644068\n",
      "272, train loss: 0.709653, test loss: 0.727667, train acc: 0.659639, test acc: 0.644068\n",
      "273, train loss: 0.709304, test loss: 0.727317, train acc: 0.659639, test acc: 0.644068\n",
      "274, train loss: 0.708957, test loss: 0.726968, train acc: 0.659639, test acc: 0.644068\n",
      "275, train loss: 0.708611, test loss: 0.726621, train acc: 0.659639, test acc: 0.644068\n",
      "276, train loss: 0.708267, test loss: 0.726276, train acc: 0.659639, test acc: 0.644068\n",
      "277, train loss: 0.707925, test loss: 0.725932, train acc: 0.659639, test acc: 0.644068\n",
      "278, train loss: 0.707583, test loss: 0.72559, train acc: 0.659639, test acc: 0.644068\n",
      "279, train loss: 0.707243, test loss: 0.72525, train acc: 0.659639, test acc: 0.644068\n",
      "280, train loss: 0.706905, test loss: 0.724911, train acc: 0.659639, test acc: 0.644068\n",
      "281, train loss: 0.706568, test loss: 0.724573, train acc: 0.659639, test acc: 0.644068\n",
      "282, train loss: 0.706233, test loss: 0.724237, train acc: 0.659639, test acc: 0.644068\n",
      "283, train loss: 0.705899, test loss: 0.723903, train acc: 0.659639, test acc: 0.644068\n",
      "284, train loss: 0.705566, test loss: 0.72357, train acc: 0.659639, test acc: 0.644068\n",
      "285, train loss: 0.705235, test loss: 0.723238, train acc: 0.662651, test acc: 0.644068\n",
      "286, train loss: 0.704905, test loss: 0.722908, train acc: 0.662651, test acc: 0.644068\n",
      "287, train loss: 0.704577, test loss: 0.72258, train acc: 0.662651, test acc: 0.644068\n",
      "288, train loss: 0.70425, test loss: 0.722253, train acc: 0.665663, test acc: 0.644068\n",
      "289, train loss: 0.703924, test loss: 0.721927, train acc: 0.665663, test acc: 0.644068\n",
      "290, train loss: 0.7036, test loss: 0.721603, train acc: 0.665663, test acc: 0.644068\n",
      "291, train loss: 0.703277, test loss: 0.721281, train acc: 0.665663, test acc: 0.644068\n",
      "292, train loss: 0.702955, test loss: 0.720959, train acc: 0.665663, test acc: 0.644068\n",
      "293, train loss: 0.702635, test loss: 0.72064, train acc: 0.665663, test acc: 0.644068\n",
      "294, train loss: 0.702316, test loss: 0.720321, train acc: 0.665663, test acc: 0.644068\n",
      "295, train loss: 0.701999, test loss: 0.720005, train acc: 0.668675, test acc: 0.644068\n",
      "296, train loss: 0.701683, test loss: 0.719689, train acc: 0.668675, test acc: 0.644068\n",
      "297, train loss: 0.701368, test loss: 0.719375, train acc: 0.668675, test acc: 0.644068\n",
      "298, train loss: 0.701054, test loss: 0.719062, train acc: 0.668675, test acc: 0.644068\n",
      "299, train loss: 0.700742, test loss: 0.718751, train acc: 0.668675, test acc: 0.644068\n",
      "300, train loss: 0.700431, test loss: 0.718441, train acc: 0.668675, test acc: 0.644068\n",
      "301, train loss: 0.700121, test loss: 0.718132, train acc: 0.668675, test acc: 0.644068\n",
      "302, train loss: 0.699813, test loss: 0.717825, train acc: 0.668675, test acc: 0.644068\n",
      "303, train loss: 0.699506, test loss: 0.717519, train acc: 0.671687, test acc: 0.644068\n",
      "304, train loss: 0.6992, test loss: 0.717215, train acc: 0.671687, test acc: 0.644068\n",
      "305, train loss: 0.698895, test loss: 0.716911, train acc: 0.671687, test acc: 0.644068\n",
      "306, train loss: 0.698592, test loss: 0.716609, train acc: 0.671687, test acc: 0.644068\n",
      "307, train loss: 0.69829, test loss: 0.716309, train acc: 0.674699, test acc: 0.644068\n",
      "308, train loss: 0.697989, test loss: 0.71601, train acc: 0.674699, test acc: 0.644068\n",
      "309, train loss: 0.69769, test loss: 0.715712, train acc: 0.674699, test acc: 0.644068\n",
      "310, train loss: 0.697391, test loss: 0.715415, train acc: 0.680723, test acc: 0.644068\n",
      "311, train loss: 0.697094, test loss: 0.715119, train acc: 0.680723, test acc: 0.644068\n",
      "312, train loss: 0.696798, test loss: 0.714825, train acc: 0.680723, test acc: 0.644068\n",
      "313, train loss: 0.696504, test loss: 0.714533, train acc: 0.680723, test acc: 0.644068\n",
      "314, train loss: 0.69621, test loss: 0.714241, train acc: 0.680723, test acc: 0.644068\n",
      "315, train loss: 0.695918, test loss: 0.713951, train acc: 0.680723, test acc: 0.644068\n",
      "316, train loss: 0.695626, test loss: 0.713662, train acc: 0.680723, test acc: 0.644068\n",
      "317, train loss: 0.695337, test loss: 0.713374, train acc: 0.680723, test acc: 0.644068\n",
      "318, train loss: 0.695048, test loss: 0.713087, train acc: 0.680723, test acc: 0.644068\n",
      "319, train loss: 0.69476, test loss: 0.712802, train acc: 0.677711, test acc: 0.644068\n",
      "320, train loss: 0.694474, test loss: 0.712517, train acc: 0.677711, test acc: 0.644068\n",
      "321, train loss: 0.694188, test loss: 0.712235, train acc: 0.677711, test acc: 0.644068\n",
      "322, train loss: 0.693904, test loss: 0.711953, train acc: 0.677711, test acc: 0.644068\n",
      "323, train loss: 0.693621, test loss: 0.711672, train acc: 0.680723, test acc: 0.644068\n",
      "324, train loss: 0.693339, test loss: 0.711393, train acc: 0.680723, test acc: 0.644068\n",
      "325, train loss: 0.693059, test loss: 0.711115, train acc: 0.680723, test acc: 0.644068\n",
      "326, train loss: 0.692779, test loss: 0.710838, train acc: 0.680723, test acc: 0.644068\n",
      "327, train loss: 0.692501, test loss: 0.710562, train acc: 0.680723, test acc: 0.644068\n",
      "328, train loss: 0.692223, test loss: 0.710287, train acc: 0.680723, test acc: 0.644068\n",
      "329, train loss: 0.691947, test loss: 0.710014, train acc: 0.680723, test acc: 0.644068\n",
      "330, train loss: 0.691672, test loss: 0.709742, train acc: 0.680723, test acc: 0.644068\n",
      "331, train loss: 0.691398, test loss: 0.70947, train acc: 0.680723, test acc: 0.644068\n",
      "332, train loss: 0.691125, test loss: 0.7092, train acc: 0.680723, test acc: 0.644068\n",
      "333, train loss: 0.690853, test loss: 0.708932, train acc: 0.680723, test acc: 0.644068\n",
      "334, train loss: 0.690582, test loss: 0.708664, train acc: 0.680723, test acc: 0.644068\n",
      "335, train loss: 0.690313, test loss: 0.708397, train acc: 0.680723, test acc: 0.644068\n",
      "336, train loss: 0.690044, test loss: 0.708132, train acc: 0.680723, test acc: 0.644068\n",
      "337, train loss: 0.689776, test loss: 0.707867, train acc: 0.680723, test acc: 0.644068\n",
      "338, train loss: 0.68951, test loss: 0.707604, train acc: 0.680723, test acc: 0.644068\n",
      "339, train loss: 0.689245, test loss: 0.707342, train acc: 0.680723, test acc: 0.644068\n",
      "340, train loss: 0.68898, test loss: 0.707081, train acc: 0.680723, test acc: 0.644068\n",
      "341, train loss: 0.688717, test loss: 0.706821, train acc: 0.680723, test acc: 0.644068\n",
      "342, train loss: 0.688455, test loss: 0.706562, train acc: 0.680723, test acc: 0.644068\n",
      "343, train loss: 0.688193, test loss: 0.706304, train acc: 0.677711, test acc: 0.644068\n",
      "344, train loss: 0.687933, test loss: 0.706047, train acc: 0.677711, test acc: 0.644068\n",
      "345, train loss: 0.687674, test loss: 0.705791, train acc: 0.677711, test acc: 0.644068\n",
      "346, train loss: 0.687415, test loss: 0.705537, train acc: 0.677711, test acc: 0.644068\n",
      "347, train loss: 0.687158, test loss: 0.705283, train acc: 0.680723, test acc: 0.644068\n",
      "348, train loss: 0.686902, test loss: 0.70503, train acc: 0.680723, test acc: 0.644068\n",
      "349, train loss: 0.686647, test loss: 0.704779, train acc: 0.683735, test acc: 0.644068\n",
      "350, train loss: 0.686393, test loss: 0.704528, train acc: 0.683735, test acc: 0.644068\n",
      "351, train loss: 0.686139, test loss: 0.704279, train acc: 0.686747, test acc: 0.677966\n",
      "352, train loss: 0.685887, test loss: 0.70403, train acc: 0.686747, test acc: 0.677966\n",
      "353, train loss: 0.685636, test loss: 0.703783, train acc: 0.686747, test acc: 0.677966\n",
      "354, train loss: 0.685386, test loss: 0.703537, train acc: 0.686747, test acc: 0.677966\n",
      "355, train loss: 0.685136, test loss: 0.703291, train acc: 0.686747, test acc: 0.677966\n",
      "356, train loss: 0.684888, test loss: 0.703047, train acc: 0.689759, test acc: 0.677966\n",
      "357, train loss: 0.684641, test loss: 0.702803, train acc: 0.689759, test acc: 0.677966\n",
      "358, train loss: 0.684394, test loss: 0.702561, train acc: 0.689759, test acc: 0.677966\n",
      "359, train loss: 0.684149, test loss: 0.702319, train acc: 0.689759, test acc: 0.677966\n",
      "360, train loss: 0.683904, test loss: 0.702079, train acc: 0.689759, test acc: 0.677966\n",
      "361, train loss: 0.683661, test loss: 0.70184, train acc: 0.689759, test acc: 0.677966\n",
      "362, train loss: 0.683418, test loss: 0.701601, train acc: 0.689759, test acc: 0.677966\n",
      "363, train loss: 0.683176, test loss: 0.701364, train acc: 0.689759, test acc: 0.677966\n",
      "364, train loss: 0.682936, test loss: 0.701127, train acc: 0.689759, test acc: 0.677966\n",
      "365, train loss: 0.682696, test loss: 0.700891, train acc: 0.689759, test acc: 0.677966\n",
      "366, train loss: 0.682457, test loss: 0.700657, train acc: 0.689759, test acc: 0.677966\n",
      "367, train loss: 0.682219, test loss: 0.700423, train acc: 0.689759, test acc: 0.677966\n",
      "368, train loss: 0.681982, test loss: 0.70019, train acc: 0.689759, test acc: 0.677966\n",
      "369, train loss: 0.681745, test loss: 0.699958, train acc: 0.692771, test acc: 0.677966\n",
      "370, train loss: 0.68151, test loss: 0.699727, train acc: 0.689759, test acc: 0.677966\n",
      "371, train loss: 0.681276, test loss: 0.699497, train acc: 0.689759, test acc: 0.677966\n",
      "372, train loss: 0.681042, test loss: 0.699268, train acc: 0.689759, test acc: 0.677966\n",
      "373, train loss: 0.680809, test loss: 0.69904, train acc: 0.689759, test acc: 0.677966\n",
      "374, train loss: 0.680578, test loss: 0.698813, train acc: 0.689759, test acc: 0.677966\n",
      "375, train loss: 0.680347, test loss: 0.698587, train acc: 0.689759, test acc: 0.677966\n",
      "376, train loss: 0.680117, test loss: 0.698361, train acc: 0.689759, test acc: 0.677966\n",
      "377, train loss: 0.679888, test loss: 0.698137, train acc: 0.689759, test acc: 0.677966\n",
      "378, train loss: 0.679659, test loss: 0.697913, train acc: 0.692771, test acc: 0.677966\n",
      "379, train loss: 0.679432, test loss: 0.69769, train acc: 0.698795, test acc: 0.677966\n",
      "380, train loss: 0.679206, test loss: 0.697468, train acc: 0.695783, test acc: 0.677966\n",
      "381, train loss: 0.67898, test loss: 0.697247, train acc: 0.698795, test acc: 0.677966\n",
      "382, train loss: 0.678755, test loss: 0.697027, train acc: 0.698795, test acc: 0.677966\n",
      "383, train loss: 0.678531, test loss: 0.696808, train acc: 0.701807, test acc: 0.677966\n",
      "384, train loss: 0.678308, test loss: 0.69659, train acc: 0.701807, test acc: 0.677966\n",
      "385, train loss: 0.678086, test loss: 0.696372, train acc: 0.701807, test acc: 0.677966\n",
      "386, train loss: 0.677864, test loss: 0.696156, train acc: 0.701807, test acc: 0.677966\n",
      "387, train loss: 0.677644, test loss: 0.69594, train acc: 0.701807, test acc: 0.677966\n",
      "388, train loss: 0.677424, test loss: 0.695725, train acc: 0.701807, test acc: 0.677966\n",
      "389, train loss: 0.677205, test loss: 0.695511, train acc: 0.701807, test acc: 0.677966\n",
      "390, train loss: 0.676987, test loss: 0.695297, train acc: 0.701807, test acc: 0.677966\n",
      "391, train loss: 0.676769, test loss: 0.695085, train acc: 0.701807, test acc: 0.677966\n",
      "392, train loss: 0.676553, test loss: 0.694874, train acc: 0.701807, test acc: 0.677966\n",
      "393, train loss: 0.676337, test loss: 0.694663, train acc: 0.701807, test acc: 0.677966\n",
      "394, train loss: 0.676122, test loss: 0.694453, train acc: 0.701807, test acc: 0.677966\n",
      "395, train loss: 0.675908, test loss: 0.694244, train acc: 0.701807, test acc: 0.677966\n",
      "396, train loss: 0.675694, test loss: 0.694035, train acc: 0.701807, test acc: 0.677966\n",
      "397, train loss: 0.675482, test loss: 0.693828, train acc: 0.701807, test acc: 0.677966\n",
      "398, train loss: 0.67527, test loss: 0.693621, train acc: 0.701807, test acc: 0.677966\n",
      "399, train loss: 0.675059, test loss: 0.693415, train acc: 0.701807, test acc: 0.677966\n",
      "400, train loss: 0.674849, test loss: 0.69321, train acc: 0.701807, test acc: 0.677966\n",
      "401, train loss: 0.674639, test loss: 0.693006, train acc: 0.701807, test acc: 0.677966\n",
      "402, train loss: 0.674431, test loss: 0.692802, train acc: 0.701807, test acc: 0.677966\n",
      "403, train loss: 0.674223, test loss: 0.6926, train acc: 0.701807, test acc: 0.677966\n",
      "404, train loss: 0.674016, test loss: 0.692398, train acc: 0.701807, test acc: 0.677966\n",
      "405, train loss: 0.673809, test loss: 0.692197, train acc: 0.701807, test acc: 0.677966\n",
      "406, train loss: 0.673604, test loss: 0.691996, train acc: 0.701807, test acc: 0.677966\n",
      "407, train loss: 0.673399, test loss: 0.691797, train acc: 0.701807, test acc: 0.677966\n",
      "408, train loss: 0.673195, test loss: 0.691598, train acc: 0.701807, test acc: 0.677966\n",
      "409, train loss: 0.672991, test loss: 0.6914, train acc: 0.701807, test acc: 0.677966\n",
      "410, train loss: 0.672789, test loss: 0.691203, train acc: 0.704819, test acc: 0.694915\n",
      "411, train loss: 0.672587, test loss: 0.691006, train acc: 0.704819, test acc: 0.694915\n",
      "412, train loss: 0.672386, test loss: 0.69081, train acc: 0.704819, test acc: 0.694915\n",
      "413, train loss: 0.672185, test loss: 0.690615, train acc: 0.704819, test acc: 0.694915\n",
      "414, train loss: 0.671986, test loss: 0.690421, train acc: 0.704819, test acc: 0.694915\n",
      "415, train loss: 0.671787, test loss: 0.690227, train acc: 0.704819, test acc: 0.694915\n",
      "416, train loss: 0.671588, test loss: 0.690034, train acc: 0.704819, test acc: 0.694915\n",
      "417, train loss: 0.671391, test loss: 0.689842, train acc: 0.704819, test acc: 0.694915\n",
      "418, train loss: 0.671194, test loss: 0.689651, train acc: 0.704819, test acc: 0.694915\n",
      "419, train loss: 0.670998, test loss: 0.68946, train acc: 0.704819, test acc: 0.694915\n",
      "420, train loss: 0.670802, test loss: 0.68927, train acc: 0.710843, test acc: 0.711864\n",
      "421, train loss: 0.670608, test loss: 0.689081, train acc: 0.710843, test acc: 0.711864\n",
      "422, train loss: 0.670414, test loss: 0.688893, train acc: 0.710843, test acc: 0.711864\n",
      "423, train loss: 0.67022, test loss: 0.688705, train acc: 0.713855, test acc: 0.711864\n",
      "424, train loss: 0.670028, test loss: 0.688518, train acc: 0.713855, test acc: 0.711864\n",
      "425, train loss: 0.669836, test loss: 0.688332, train acc: 0.713855, test acc: 0.711864\n",
      "426, train loss: 0.669645, test loss: 0.688146, train acc: 0.713855, test acc: 0.711864\n",
      "427, train loss: 0.669454, test loss: 0.687961, train acc: 0.713855, test acc: 0.711864\n",
      "428, train loss: 0.669264, test loss: 0.687777, train acc: 0.716867, test acc: 0.711864\n",
      "429, train loss: 0.669075, test loss: 0.687593, train acc: 0.716867, test acc: 0.711864\n",
      "430, train loss: 0.668887, test loss: 0.68741, train acc: 0.716867, test acc: 0.711864\n",
      "431, train loss: 0.668699, test loss: 0.687228, train acc: 0.71988, test acc: 0.711864\n",
      "432, train loss: 0.668512, test loss: 0.687047, train acc: 0.71988, test acc: 0.711864\n",
      "433, train loss: 0.668325, test loss: 0.686866, train acc: 0.722892, test acc: 0.711864\n",
      "434, train loss: 0.668139, test loss: 0.686686, train acc: 0.722892, test acc: 0.711864\n",
      "435, train loss: 0.667954, test loss: 0.686506, train acc: 0.722892, test acc: 0.711864\n",
      "436, train loss: 0.667769, test loss: 0.686327, train acc: 0.722892, test acc: 0.711864\n",
      "437, train loss: 0.667586, test loss: 0.686149, train acc: 0.722892, test acc: 0.711864\n",
      "438, train loss: 0.667402, test loss: 0.685972, train acc: 0.722892, test acc: 0.711864\n",
      "439, train loss: 0.66722, test loss: 0.685795, train acc: 0.722892, test acc: 0.711864\n",
      "440, train loss: 0.667038, test loss: 0.685619, train acc: 0.722892, test acc: 0.711864\n",
      "441, train loss: 0.666857, test loss: 0.685443, train acc: 0.722892, test acc: 0.711864\n",
      "442, train loss: 0.666676, test loss: 0.685268, train acc: 0.722892, test acc: 0.711864\n",
      "443, train loss: 0.666496, test loss: 0.685094, train acc: 0.722892, test acc: 0.711864\n",
      "444, train loss: 0.666317, test loss: 0.684921, train acc: 0.722892, test acc: 0.711864\n",
      "445, train loss: 0.666138, test loss: 0.684748, train acc: 0.722892, test acc: 0.711864\n",
      "446, train loss: 0.66596, test loss: 0.684575, train acc: 0.722892, test acc: 0.711864\n",
      "447, train loss: 0.665782, test loss: 0.684404, train acc: 0.722892, test acc: 0.711864\n",
      "448, train loss: 0.665605, test loss: 0.684233, train acc: 0.722892, test acc: 0.711864\n",
      "449, train loss: 0.665429, test loss: 0.684062, train acc: 0.722892, test acc: 0.711864\n",
      "450, train loss: 0.665253, test loss: 0.683893, train acc: 0.722892, test acc: 0.711864\n",
      "451, train loss: 0.665078, test loss: 0.683723, train acc: 0.722892, test acc: 0.711864\n",
      "452, train loss: 0.664904, test loss: 0.683555, train acc: 0.722892, test acc: 0.711864\n",
      "453, train loss: 0.66473, test loss: 0.683387, train acc: 0.722892, test acc: 0.711864\n",
      "454, train loss: 0.664557, test loss: 0.683219, train acc: 0.722892, test acc: 0.711864\n",
      "455, train loss: 0.664384, test loss: 0.683053, train acc: 0.722892, test acc: 0.711864\n",
      "456, train loss: 0.664212, test loss: 0.682887, train acc: 0.722892, test acc: 0.711864\n",
      "457, train loss: 0.664041, test loss: 0.682721, train acc: 0.722892, test acc: 0.711864\n",
      "458, train loss: 0.66387, test loss: 0.682556, train acc: 0.722892, test acc: 0.711864\n",
      "459, train loss: 0.6637, test loss: 0.682392, train acc: 0.722892, test acc: 0.711864\n",
      "460, train loss: 0.66353, test loss: 0.682229, train acc: 0.722892, test acc: 0.711864\n",
      "461, train loss: 0.663361, test loss: 0.682065, train acc: 0.722892, test acc: 0.711864\n",
      "462, train loss: 0.663192, test loss: 0.681903, train acc: 0.722892, test acc: 0.711864\n",
      "463, train loss: 0.663025, test loss: 0.681741, train acc: 0.725904, test acc: 0.711864\n",
      "464, train loss: 0.662857, test loss: 0.68158, train acc: 0.725904, test acc: 0.711864\n",
      "465, train loss: 0.66269, test loss: 0.681419, train acc: 0.725904, test acc: 0.711864\n",
      "466, train loss: 0.662524, test loss: 0.681259, train acc: 0.725904, test acc: 0.711864\n",
      "467, train loss: 0.662359, test loss: 0.681099, train acc: 0.725904, test acc: 0.711864\n",
      "468, train loss: 0.662193, test loss: 0.68094, train acc: 0.725904, test acc: 0.711864\n",
      "469, train loss: 0.662029, test loss: 0.680782, train acc: 0.725904, test acc: 0.711864\n",
      "470, train loss: 0.661865, test loss: 0.680624, train acc: 0.725904, test acc: 0.711864\n",
      "471, train loss: 0.661702, test loss: 0.680466, train acc: 0.725904, test acc: 0.711864\n",
      "472, train loss: 0.661539, test loss: 0.68031, train acc: 0.725904, test acc: 0.711864\n",
      "473, train loss: 0.661377, test loss: 0.680153, train acc: 0.725904, test acc: 0.711864\n",
      "474, train loss: 0.661215, test loss: 0.679998, train acc: 0.725904, test acc: 0.711864\n",
      "475, train loss: 0.661054, test loss: 0.679843, train acc: 0.725904, test acc: 0.711864\n",
      "476, train loss: 0.660893, test loss: 0.679688, train acc: 0.725904, test acc: 0.711864\n",
      "477, train loss: 0.660733, test loss: 0.679534, train acc: 0.722892, test acc: 0.711864\n",
      "478, train loss: 0.660574, test loss: 0.679381, train acc: 0.722892, test acc: 0.711864\n",
      "479, train loss: 0.660415, test loss: 0.679228, train acc: 0.722892, test acc: 0.711864\n",
      "480, train loss: 0.660256, test loss: 0.679076, train acc: 0.722892, test acc: 0.711864\n",
      "481, train loss: 0.660098, test loss: 0.678924, train acc: 0.722892, test acc: 0.711864\n",
      "482, train loss: 0.659941, test loss: 0.678773, train acc: 0.722892, test acc: 0.711864\n",
      "483, train loss: 0.659784, test loss: 0.678622, train acc: 0.722892, test acc: 0.711864\n",
      "484, train loss: 0.659628, test loss: 0.678472, train acc: 0.722892, test acc: 0.711864\n",
      "485, train loss: 0.659472, test loss: 0.678322, train acc: 0.722892, test acc: 0.711864\n",
      "486, train loss: 0.659317, test loss: 0.678173, train acc: 0.722892, test acc: 0.711864\n",
      "487, train loss: 0.659162, test loss: 0.678024, train acc: 0.725904, test acc: 0.711864\n",
      "488, train loss: 0.659007, test loss: 0.677876, train acc: 0.725904, test acc: 0.711864\n",
      "489, train loss: 0.658854, test loss: 0.677728, train acc: 0.725904, test acc: 0.711864\n",
      "490, train loss: 0.658701, test loss: 0.677581, train acc: 0.728916, test acc: 0.711864\n",
      "491, train loss: 0.658548, test loss: 0.677435, train acc: 0.728916, test acc: 0.711864\n",
      "492, train loss: 0.658396, test loss: 0.677289, train acc: 0.728916, test acc: 0.711864\n",
      "493, train loss: 0.658244, test loss: 0.677143, train acc: 0.728916, test acc: 0.711864\n",
      "494, train loss: 0.658093, test loss: 0.676998, train acc: 0.728916, test acc: 0.711864\n",
      "495, train loss: 0.657942, test loss: 0.676854, train acc: 0.728916, test acc: 0.711864\n",
      "496, train loss: 0.657792, test loss: 0.67671, train acc: 0.728916, test acc: 0.711864\n",
      "497, train loss: 0.657642, test loss: 0.676566, train acc: 0.728916, test acc: 0.711864\n",
      "498, train loss: 0.657493, test loss: 0.676423, train acc: 0.728916, test acc: 0.711864\n",
      "499, train loss: 0.657344, test loss: 0.676281, train acc: 0.728916, test acc: 0.711864\n",
      "500, train loss: 0.657196, test loss: 0.676139, train acc: 0.728916, test acc: 0.711864\n",
      "501, train loss: 0.657048, test loss: 0.675997, train acc: 0.728916, test acc: 0.711864\n",
      "502, train loss: 0.656901, test loss: 0.675856, train acc: 0.728916, test acc: 0.711864\n",
      "503, train loss: 0.656754, test loss: 0.675716, train acc: 0.728916, test acc: 0.711864\n",
      "504, train loss: 0.656608, test loss: 0.675575, train acc: 0.728916, test acc: 0.711864\n",
      "505, train loss: 0.656462, test loss: 0.675436, train acc: 0.728916, test acc: 0.711864\n",
      "506, train loss: 0.656317, test loss: 0.675297, train acc: 0.728916, test acc: 0.711864\n",
      "507, train loss: 0.656172, test loss: 0.675158, train acc: 0.728916, test acc: 0.711864\n",
      "508, train loss: 0.656027, test loss: 0.67502, train acc: 0.728916, test acc: 0.711864\n",
      "509, train loss: 0.655883, test loss: 0.674882, train acc: 0.728916, test acc: 0.711864\n",
      "510, train loss: 0.65574, test loss: 0.674745, train acc: 0.728916, test acc: 0.711864\n",
      "511, train loss: 0.655597, test loss: 0.674608, train acc: 0.728916, test acc: 0.711864\n",
      "512, train loss: 0.655455, test loss: 0.674472, train acc: 0.728916, test acc: 0.694915\n",
      "513, train loss: 0.655312, test loss: 0.674336, train acc: 0.728916, test acc: 0.694915\n",
      "514, train loss: 0.655171, test loss: 0.674201, train acc: 0.728916, test acc: 0.694915\n",
      "515, train loss: 0.65503, test loss: 0.674066, train acc: 0.728916, test acc: 0.694915\n",
      "516, train loss: 0.654889, test loss: 0.673931, train acc: 0.728916, test acc: 0.694915\n",
      "517, train loss: 0.654749, test loss: 0.673797, train acc: 0.728916, test acc: 0.694915\n",
      "518, train loss: 0.654609, test loss: 0.673664, train acc: 0.728916, test acc: 0.694915\n",
      "519, train loss: 0.65447, test loss: 0.673531, train acc: 0.728916, test acc: 0.694915\n",
      "520, train loss: 0.654331, test loss: 0.673398, train acc: 0.728916, test acc: 0.694915\n",
      "521, train loss: 0.654192, test loss: 0.673266, train acc: 0.728916, test acc: 0.694915\n",
      "522, train loss: 0.654054, test loss: 0.673134, train acc: 0.728916, test acc: 0.694915\n",
      "523, train loss: 0.653917, test loss: 0.673003, train acc: 0.728916, test acc: 0.694915\n",
      "524, train loss: 0.65378, test loss: 0.672872, train acc: 0.728916, test acc: 0.694915\n",
      "525, train loss: 0.653643, test loss: 0.672742, train acc: 0.728916, test acc: 0.694915\n",
      "526, train loss: 0.653507, test loss: 0.672612, train acc: 0.728916, test acc: 0.694915\n",
      "527, train loss: 0.653371, test loss: 0.672482, train acc: 0.728916, test acc: 0.694915\n",
      "528, train loss: 0.653236, test loss: 0.672353, train acc: 0.728916, test acc: 0.694915\n",
      "529, train loss: 0.653101, test loss: 0.672225, train acc: 0.728916, test acc: 0.694915\n",
      "530, train loss: 0.652966, test loss: 0.672096, train acc: 0.728916, test acc: 0.694915\n",
      "531, train loss: 0.652832, test loss: 0.671969, train acc: 0.725904, test acc: 0.694915\n",
      "532, train loss: 0.652698, test loss: 0.671841, train acc: 0.725904, test acc: 0.694915\n",
      "533, train loss: 0.652565, test loss: 0.671714, train acc: 0.725904, test acc: 0.694915\n",
      "534, train loss: 0.652432, test loss: 0.671588, train acc: 0.725904, test acc: 0.694915\n",
      "535, train loss: 0.6523, test loss: 0.671462, train acc: 0.725904, test acc: 0.694915\n",
      "536, train loss: 0.652168, test loss: 0.671336, train acc: 0.725904, test acc: 0.694915\n",
      "537, train loss: 0.652036, test loss: 0.671211, train acc: 0.725904, test acc: 0.694915\n",
      "538, train loss: 0.651905, test loss: 0.671086, train acc: 0.725904, test acc: 0.694915\n",
      "539, train loss: 0.651775, test loss: 0.670962, train acc: 0.725904, test acc: 0.694915\n",
      "540, train loss: 0.651644, test loss: 0.670838, train acc: 0.725904, test acc: 0.694915\n",
      "541, train loss: 0.651514, test loss: 0.670714, train acc: 0.725904, test acc: 0.694915\n",
      "542, train loss: 0.651385, test loss: 0.670591, train acc: 0.725904, test acc: 0.694915\n",
      "543, train loss: 0.651256, test loss: 0.670468, train acc: 0.725904, test acc: 0.694915\n",
      "544, train loss: 0.651127, test loss: 0.670345, train acc: 0.725904, test acc: 0.694915\n",
      "545, train loss: 0.650999, test loss: 0.670224, train acc: 0.725904, test acc: 0.694915\n",
      "546, train loss: 0.650871, test loss: 0.670102, train acc: 0.725904, test acc: 0.694915\n",
      "547, train loss: 0.650743, test loss: 0.669981, train acc: 0.725904, test acc: 0.694915\n",
      "548, train loss: 0.650616, test loss: 0.66986, train acc: 0.725904, test acc: 0.694915\n",
      "549, train loss: 0.65049, test loss: 0.66974, train acc: 0.725904, test acc: 0.694915\n",
      "550, train loss: 0.650363, test loss: 0.66962, train acc: 0.725904, test acc: 0.694915\n",
      "551, train loss: 0.650237, test loss: 0.6695, train acc: 0.725904, test acc: 0.694915\n",
      "552, train loss: 0.650112, test loss: 0.669381, train acc: 0.725904, test acc: 0.694915\n",
      "553, train loss: 0.649987, test loss: 0.669262, train acc: 0.725904, test acc: 0.694915\n",
      "554, train loss: 0.649862, test loss: 0.669143, train acc: 0.725904, test acc: 0.694915\n",
      "555, train loss: 0.649737, test loss: 0.669025, train acc: 0.725904, test acc: 0.694915\n",
      "556, train loss: 0.649613, test loss: 0.668908, train acc: 0.725904, test acc: 0.694915\n",
      "557, train loss: 0.64949, test loss: 0.66879, train acc: 0.725904, test acc: 0.694915\n",
      "558, train loss: 0.649367, test loss: 0.668673, train acc: 0.725904, test acc: 0.694915\n",
      "559, train loss: 0.649244, test loss: 0.668557, train acc: 0.725904, test acc: 0.694915\n",
      "560, train loss: 0.649121, test loss: 0.668441, train acc: 0.725904, test acc: 0.694915\n",
      "561, train loss: 0.648999, test loss: 0.668325, train acc: 0.725904, test acc: 0.694915\n",
      "562, train loss: 0.648878, test loss: 0.66821, train acc: 0.725904, test acc: 0.694915\n",
      "563, train loss: 0.648756, test loss: 0.668095, train acc: 0.725904, test acc: 0.694915\n",
      "564, train loss: 0.648635, test loss: 0.66798, train acc: 0.725904, test acc: 0.694915\n",
      "565, train loss: 0.648515, test loss: 0.667865, train acc: 0.725904, test acc: 0.694915\n",
      "566, train loss: 0.648394, test loss: 0.667751, train acc: 0.725904, test acc: 0.694915\n",
      "567, train loss: 0.648274, test loss: 0.667638, train acc: 0.725904, test acc: 0.694915\n",
      "568, train loss: 0.648155, test loss: 0.667525, train acc: 0.728916, test acc: 0.694915\n",
      "569, train loss: 0.648036, test loss: 0.667412, train acc: 0.728916, test acc: 0.694915\n",
      "570, train loss: 0.647917, test loss: 0.667299, train acc: 0.728916, test acc: 0.694915\n",
      "571, train loss: 0.647799, test loss: 0.667187, train acc: 0.728916, test acc: 0.694915\n",
      "572, train loss: 0.64768, test loss: 0.667075, train acc: 0.725904, test acc: 0.694915\n",
      "573, train loss: 0.647563, test loss: 0.666964, train acc: 0.725904, test acc: 0.694915\n",
      "574, train loss: 0.647445, test loss: 0.666853, train acc: 0.725904, test acc: 0.694915\n",
      "575, train loss: 0.647328, test loss: 0.666742, train acc: 0.725904, test acc: 0.694915\n",
      "576, train loss: 0.647212, test loss: 0.666632, train acc: 0.725904, test acc: 0.694915\n",
      "577, train loss: 0.647095, test loss: 0.666522, train acc: 0.725904, test acc: 0.694915\n",
      "578, train loss: 0.646979, test loss: 0.666412, train acc: 0.725904, test acc: 0.694915\n",
      "579, train loss: 0.646864, test loss: 0.666303, train acc: 0.725904, test acc: 0.694915\n",
      "580, train loss: 0.646748, test loss: 0.666194, train acc: 0.725904, test acc: 0.694915\n",
      "581, train loss: 0.646634, test loss: 0.666085, train acc: 0.725904, test acc: 0.694915\n",
      "582, train loss: 0.646519, test loss: 0.665977, train acc: 0.725904, test acc: 0.694915\n",
      "583, train loss: 0.646405, test loss: 0.665869, train acc: 0.725904, test acc: 0.694915\n",
      "584, train loss: 0.646291, test loss: 0.665761, train acc: 0.725904, test acc: 0.677966\n",
      "585, train loss: 0.646177, test loss: 0.665654, train acc: 0.725904, test acc: 0.677966\n",
      "586, train loss: 0.646064, test loss: 0.665547, train acc: 0.725904, test acc: 0.677966\n",
      "587, train loss: 0.645951, test loss: 0.66544, train acc: 0.725904, test acc: 0.677966\n",
      "588, train loss: 0.645839, test loss: 0.665334, train acc: 0.725904, test acc: 0.677966\n",
      "589, train loss: 0.645726, test loss: 0.665228, train acc: 0.725904, test acc: 0.677966\n",
      "590, train loss: 0.645615, test loss: 0.665123, train acc: 0.725904, test acc: 0.677966\n",
      "591, train loss: 0.645503, test loss: 0.665017, train acc: 0.725904, test acc: 0.677966\n",
      "592, train loss: 0.645392, test loss: 0.664912, train acc: 0.725904, test acc: 0.677966\n",
      "593, train loss: 0.645281, test loss: 0.664808, train acc: 0.725904, test acc: 0.677966\n",
      "594, train loss: 0.64517, test loss: 0.664703, train acc: 0.725904, test acc: 0.677966\n",
      "595, train loss: 0.64506, test loss: 0.664599, train acc: 0.725904, test acc: 0.677966\n",
      "596, train loss: 0.64495, test loss: 0.664496, train acc: 0.725904, test acc: 0.677966\n",
      "597, train loss: 0.64484, test loss: 0.664392, train acc: 0.725904, test acc: 0.677966\n",
      "598, train loss: 0.644731, test loss: 0.664289, train acc: 0.725904, test acc: 0.677966\n",
      "599, train loss: 0.644622, test loss: 0.664187, train acc: 0.725904, test acc: 0.677966\n",
      "600, train loss: 0.644513, test loss: 0.664084, train acc: 0.725904, test acc: 0.677966\n",
      "601, train loss: 0.644405, test loss: 0.663982, train acc: 0.725904, test acc: 0.677966\n",
      "602, train loss: 0.644297, test loss: 0.66388, train acc: 0.725904, test acc: 0.677966\n",
      "603, train loss: 0.644189, test loss: 0.663779, train acc: 0.725904, test acc: 0.677966\n",
      "604, train loss: 0.644082, test loss: 0.663678, train acc: 0.725904, test acc: 0.677966\n",
      "605, train loss: 0.643975, test loss: 0.663577, train acc: 0.725904, test acc: 0.677966\n",
      "606, train loss: 0.643868, test loss: 0.663476, train acc: 0.725904, test acc: 0.694915\n",
      "607, train loss: 0.643762, test loss: 0.663376, train acc: 0.725904, test acc: 0.694915\n",
      "608, train loss: 0.643656, test loss: 0.663276, train acc: 0.725904, test acc: 0.694915\n",
      "609, train loss: 0.64355, test loss: 0.663177, train acc: 0.725904, test acc: 0.694915\n",
      "610, train loss: 0.643444, test loss: 0.663077, train acc: 0.725904, test acc: 0.694915\n",
      "611, train loss: 0.643339, test loss: 0.662978, train acc: 0.725904, test acc: 0.694915\n",
      "612, train loss: 0.643234, test loss: 0.66288, train acc: 0.725904, test acc: 0.694915\n",
      "613, train loss: 0.643129, test loss: 0.662781, train acc: 0.725904, test acc: 0.694915\n",
      "614, train loss: 0.643025, test loss: 0.662683, train acc: 0.725904, test acc: 0.694915\n",
      "615, train loss: 0.642921, test loss: 0.662585, train acc: 0.722892, test acc: 0.694915\n",
      "616, train loss: 0.642817, test loss: 0.662488, train acc: 0.722892, test acc: 0.694915\n",
      "617, train loss: 0.642714, test loss: 0.662391, train acc: 0.722892, test acc: 0.694915\n",
      "618, train loss: 0.642611, test loss: 0.662294, train acc: 0.722892, test acc: 0.694915\n",
      "619, train loss: 0.642508, test loss: 0.662197, train acc: 0.722892, test acc: 0.694915\n",
      "620, train loss: 0.642405, test loss: 0.662101, train acc: 0.722892, test acc: 0.694915\n",
      "621, train loss: 0.642303, test loss: 0.662005, train acc: 0.722892, test acc: 0.694915\n",
      "622, train loss: 0.642201, test loss: 0.661909, train acc: 0.725904, test acc: 0.694915\n",
      "623, train loss: 0.642099, test loss: 0.661813, train acc: 0.725904, test acc: 0.694915\n",
      "624, train loss: 0.641998, test loss: 0.661718, train acc: 0.725904, test acc: 0.694915\n",
      "625, train loss: 0.641897, test loss: 0.661623, train acc: 0.725904, test acc: 0.694915\n",
      "626, train loss: 0.641796, test loss: 0.661529, train acc: 0.725904, test acc: 0.694915\n",
      "627, train loss: 0.641695, test loss: 0.661434, train acc: 0.725904, test acc: 0.694915\n",
      "628, train loss: 0.641595, test loss: 0.66134, train acc: 0.725904, test acc: 0.694915\n",
      "629, train loss: 0.641495, test loss: 0.661246, train acc: 0.725904, test acc: 0.694915\n",
      "630, train loss: 0.641395, test loss: 0.661153, train acc: 0.725904, test acc: 0.694915\n",
      "631, train loss: 0.641296, test loss: 0.66106, train acc: 0.725904, test acc: 0.694915\n",
      "632, train loss: 0.641197, test loss: 0.660967, train acc: 0.725904, test acc: 0.694915\n",
      "633, train loss: 0.641098, test loss: 0.660874, train acc: 0.725904, test acc: 0.694915\n",
      "634, train loss: 0.641, test loss: 0.660781, train acc: 0.725904, test acc: 0.694915\n",
      "635, train loss: 0.640901, test loss: 0.660689, train acc: 0.725904, test acc: 0.694915\n",
      "636, train loss: 0.640803, test loss: 0.660597, train acc: 0.725904, test acc: 0.694915\n",
      "637, train loss: 0.640705, test loss: 0.660506, train acc: 0.725904, test acc: 0.694915\n",
      "638, train loss: 0.640608, test loss: 0.660414, train acc: 0.725904, test acc: 0.694915\n",
      "639, train loss: 0.640511, test loss: 0.660323, train acc: 0.725904, test acc: 0.694915\n",
      "640, train loss: 0.640414, test loss: 0.660233, train acc: 0.725904, test acc: 0.694915\n",
      "641, train loss: 0.640317, test loss: 0.660142, train acc: 0.725904, test acc: 0.694915\n",
      "642, train loss: 0.640221, test loss: 0.660052, train acc: 0.725904, test acc: 0.694915\n",
      "643, train loss: 0.640125, test loss: 0.659962, train acc: 0.725904, test acc: 0.694915\n",
      "644, train loss: 0.640029, test loss: 0.659872, train acc: 0.725904, test acc: 0.694915\n",
      "645, train loss: 0.639933, test loss: 0.659783, train acc: 0.725904, test acc: 0.694915\n",
      "646, train loss: 0.639838, test loss: 0.659693, train acc: 0.725904, test acc: 0.694915\n",
      "647, train loss: 0.639743, test loss: 0.659604, train acc: 0.725904, test acc: 0.694915\n",
      "648, train loss: 0.639648, test loss: 0.659516, train acc: 0.725904, test acc: 0.694915\n",
      "649, train loss: 0.639553, test loss: 0.659427, train acc: 0.725904, test acc: 0.694915\n",
      "650, train loss: 0.639459, test loss: 0.659339, train acc: 0.725904, test acc: 0.694915\n",
      "651, train loss: 0.639365, test loss: 0.659251, train acc: 0.725904, test acc: 0.694915\n",
      "652, train loss: 0.639271, test loss: 0.659163, train acc: 0.725904, test acc: 0.694915\n",
      "653, train loss: 0.639178, test loss: 0.659076, train acc: 0.725904, test acc: 0.677966\n",
      "654, train loss: 0.639084, test loss: 0.658989, train acc: 0.725904, test acc: 0.694915\n",
      "655, train loss: 0.638991, test loss: 0.658902, train acc: 0.725904, test acc: 0.694915\n",
      "656, train loss: 0.638898, test loss: 0.658815, train acc: 0.725904, test acc: 0.694915\n",
      "657, train loss: 0.638806, test loss: 0.658729, train acc: 0.725904, test acc: 0.694915\n",
      "658, train loss: 0.638714, test loss: 0.658643, train acc: 0.725904, test acc: 0.694915\n",
      "659, train loss: 0.638622, test loss: 0.658557, train acc: 0.725904, test acc: 0.694915\n",
      "660, train loss: 0.63853, test loss: 0.658471, train acc: 0.725904, test acc: 0.694915\n",
      "661, train loss: 0.638438, test loss: 0.658385, train acc: 0.725904, test acc: 0.694915\n",
      "662, train loss: 0.638347, test loss: 0.6583, train acc: 0.725904, test acc: 0.694915\n",
      "663, train loss: 0.638256, test loss: 0.658215, train acc: 0.725904, test acc: 0.694915\n",
      "664, train loss: 0.638165, test loss: 0.65813, train acc: 0.725904, test acc: 0.694915\n",
      "665, train loss: 0.638075, test loss: 0.658046, train acc: 0.725904, test acc: 0.694915\n",
      "666, train loss: 0.637985, test loss: 0.657962, train acc: 0.725904, test acc: 0.694915\n",
      "667, train loss: 0.637894, test loss: 0.657878, train acc: 0.725904, test acc: 0.694915\n",
      "668, train loss: 0.637805, test loss: 0.657794, train acc: 0.725904, test acc: 0.694915\n",
      "669, train loss: 0.637715, test loss: 0.65771, train acc: 0.725904, test acc: 0.694915\n",
      "670, train loss: 0.637626, test loss: 0.657627, train acc: 0.725904, test acc: 0.694915\n",
      "671, train loss: 0.637537, test loss: 0.657544, train acc: 0.725904, test acc: 0.694915\n",
      "672, train loss: 0.637448, test loss: 0.657461, train acc: 0.725904, test acc: 0.694915\n",
      "673, train loss: 0.637359, test loss: 0.657379, train acc: 0.725904, test acc: 0.694915\n",
      "674, train loss: 0.637271, test loss: 0.657296, train acc: 0.725904, test acc: 0.694915\n",
      "675, train loss: 0.637183, test loss: 0.657214, train acc: 0.725904, test acc: 0.694915\n",
      "676, train loss: 0.637095, test loss: 0.657132, train acc: 0.725904, test acc: 0.677966\n",
      "677, train loss: 0.637007, test loss: 0.657051, train acc: 0.725904, test acc: 0.677966\n",
      "678, train loss: 0.63692, test loss: 0.656969, train acc: 0.725904, test acc: 0.677966\n",
      "679, train loss: 0.636832, test loss: 0.656888, train acc: 0.725904, test acc: 0.677966\n",
      "680, train loss: 0.636745, test loss: 0.656807, train acc: 0.725904, test acc: 0.677966\n",
      "681, train loss: 0.636659, test loss: 0.656726, train acc: 0.728916, test acc: 0.677966\n",
      "682, train loss: 0.636572, test loss: 0.656645, train acc: 0.728916, test acc: 0.677966\n",
      "683, train loss: 0.636486, test loss: 0.656565, train acc: 0.728916, test acc: 0.677966\n",
      "684, train loss: 0.6364, test loss: 0.656485, train acc: 0.728916, test acc: 0.677966\n",
      "685, train loss: 0.636314, test loss: 0.656405, train acc: 0.728916, test acc: 0.677966\n",
      "686, train loss: 0.636228, test loss: 0.656326, train acc: 0.728916, test acc: 0.677966\n",
      "687, train loss: 0.636143, test loss: 0.656246, train acc: 0.728916, test acc: 0.677966\n",
      "688, train loss: 0.636058, test loss: 0.656167, train acc: 0.728916, test acc: 0.677966\n",
      "689, train loss: 0.635973, test loss: 0.656088, train acc: 0.728916, test acc: 0.677966\n",
      "690, train loss: 0.635888, test loss: 0.656009, train acc: 0.728916, test acc: 0.677966\n",
      "691, train loss: 0.635803, test loss: 0.655931, train acc: 0.728916, test acc: 0.677966\n",
      "692, train loss: 0.635719, test loss: 0.655852, train acc: 0.728916, test acc: 0.677966\n",
      "693, train loss: 0.635635, test loss: 0.655774, train acc: 0.728916, test acc: 0.677966\n",
      "694, train loss: 0.635551, test loss: 0.655696, train acc: 0.728916, test acc: 0.677966\n",
      "695, train loss: 0.635468, test loss: 0.655618, train acc: 0.728916, test acc: 0.677966\n",
      "696, train loss: 0.635384, test loss: 0.655541, train acc: 0.728916, test acc: 0.677966\n",
      "697, train loss: 0.635301, test loss: 0.655464, train acc: 0.728916, test acc: 0.677966\n",
      "698, train loss: 0.635218, test loss: 0.655386, train acc: 0.728916, test acc: 0.694915\n",
      "699, train loss: 0.635135, test loss: 0.65531, train acc: 0.728916, test acc: 0.694915\n",
      "700, train loss: 0.635053, test loss: 0.655233, train acc: 0.728916, test acc: 0.694915\n",
      "701, train loss: 0.63497, test loss: 0.655157, train acc: 0.725904, test acc: 0.694915\n",
      "702, train loss: 0.634888, test loss: 0.65508, train acc: 0.725904, test acc: 0.694915\n",
      "703, train loss: 0.634806, test loss: 0.655004, train acc: 0.725904, test acc: 0.694915\n",
      "704, train loss: 0.634725, test loss: 0.654928, train acc: 0.725904, test acc: 0.694915\n",
      "705, train loss: 0.634643, test loss: 0.654853, train acc: 0.725904, test acc: 0.694915\n",
      "706, train loss: 0.634562, test loss: 0.654777, train acc: 0.725904, test acc: 0.694915\n",
      "707, train loss: 0.634481, test loss: 0.654702, train acc: 0.725904, test acc: 0.694915\n",
      "708, train loss: 0.6344, test loss: 0.654627, train acc: 0.725904, test acc: 0.694915\n",
      "709, train loss: 0.634319, test loss: 0.654552, train acc: 0.725904, test acc: 0.694915\n",
      "710, train loss: 0.634239, test loss: 0.654478, train acc: 0.725904, test acc: 0.694915\n",
      "711, train loss: 0.634158, test loss: 0.654403, train acc: 0.725904, test acc: 0.694915\n",
      "712, train loss: 0.634078, test loss: 0.654329, train acc: 0.725904, test acc: 0.694915\n",
      "713, train loss: 0.633999, test loss: 0.654255, train acc: 0.725904, test acc: 0.694915\n",
      "714, train loss: 0.633919, test loss: 0.654181, train acc: 0.725904, test acc: 0.694915\n",
      "715, train loss: 0.633839, test loss: 0.654108, train acc: 0.722892, test acc: 0.694915\n",
      "716, train loss: 0.63376, test loss: 0.654034, train acc: 0.722892, test acc: 0.694915\n",
      "717, train loss: 0.633681, test loss: 0.653961, train acc: 0.722892, test acc: 0.694915\n",
      "718, train loss: 0.633602, test loss: 0.653888, train acc: 0.722892, test acc: 0.694915\n",
      "719, train loss: 0.633524, test loss: 0.653815, train acc: 0.722892, test acc: 0.694915\n",
      "720, train loss: 0.633445, test loss: 0.653742, train acc: 0.722892, test acc: 0.694915\n",
      "721, train loss: 0.633367, test loss: 0.65367, train acc: 0.722892, test acc: 0.694915\n",
      "722, train loss: 0.633289, test loss: 0.653597, train acc: 0.722892, test acc: 0.694915\n",
      "723, train loss: 0.633211, test loss: 0.653525, train acc: 0.722892, test acc: 0.694915\n",
      "724, train loss: 0.633133, test loss: 0.653454, train acc: 0.722892, test acc: 0.694915\n",
      "725, train loss: 0.633056, test loss: 0.653382, train acc: 0.722892, test acc: 0.694915\n",
      "726, train loss: 0.632978, test loss: 0.65331, train acc: 0.725904, test acc: 0.694915\n",
      "727, train loss: 0.632901, test loss: 0.653239, train acc: 0.725904, test acc: 0.694915\n",
      "728, train loss: 0.632824, test loss: 0.653168, train acc: 0.725904, test acc: 0.694915\n",
      "729, train loss: 0.632748, test loss: 0.653097, train acc: 0.725904, test acc: 0.694915\n",
      "730, train loss: 0.632671, test loss: 0.653026, train acc: 0.725904, test acc: 0.694915\n",
      "731, train loss: 0.632595, test loss: 0.652955, train acc: 0.725904, test acc: 0.694915\n",
      "732, train loss: 0.632519, test loss: 0.652885, train acc: 0.725904, test acc: 0.694915\n",
      "733, train loss: 0.632443, test loss: 0.652815, train acc: 0.725904, test acc: 0.694915\n",
      "734, train loss: 0.632367, test loss: 0.652745, train acc: 0.725904, test acc: 0.694915\n",
      "735, train loss: 0.632291, test loss: 0.652675, train acc: 0.725904, test acc: 0.694915\n",
      "736, train loss: 0.632216, test loss: 0.652605, train acc: 0.725904, test acc: 0.694915\n",
      "737, train loss: 0.632141, test loss: 0.652536, train acc: 0.725904, test acc: 0.694915\n",
      "738, train loss: 0.632066, test loss: 0.652466, train acc: 0.725904, test acc: 0.694915\n",
      "739, train loss: 0.631991, test loss: 0.652397, train acc: 0.725904, test acc: 0.694915\n",
      "740, train loss: 0.631916, test loss: 0.652328, train acc: 0.725904, test acc: 0.694915\n",
      "741, train loss: 0.631842, test loss: 0.65226, train acc: 0.725904, test acc: 0.694915\n",
      "742, train loss: 0.631767, test loss: 0.652191, train acc: 0.725904, test acc: 0.694915\n",
      "743, train loss: 0.631693, test loss: 0.652122, train acc: 0.725904, test acc: 0.694915\n",
      "744, train loss: 0.631619, test loss: 0.652054, train acc: 0.725904, test acc: 0.694915\n",
      "745, train loss: 0.631546, test loss: 0.651986, train acc: 0.725904, test acc: 0.694915\n",
      "746, train loss: 0.631472, test loss: 0.651918, train acc: 0.725904, test acc: 0.694915\n",
      "747, train loss: 0.631399, test loss: 0.651851, train acc: 0.725904, test acc: 0.694915\n",
      "748, train loss: 0.631325, test loss: 0.651783, train acc: 0.725904, test acc: 0.694915\n",
      "749, train loss: 0.631252, test loss: 0.651716, train acc: 0.725904, test acc: 0.694915\n",
      "750, train loss: 0.631179, test loss: 0.651648, train acc: 0.725904, test acc: 0.694915\n",
      "751, train loss: 0.631107, test loss: 0.651581, train acc: 0.725904, test acc: 0.694915\n",
      "752, train loss: 0.631034, test loss: 0.651514, train acc: 0.725904, test acc: 0.694915\n",
      "753, train loss: 0.630962, test loss: 0.651448, train acc: 0.725904, test acc: 0.694915\n",
      "754, train loss: 0.63089, test loss: 0.651381, train acc: 0.725904, test acc: 0.694915\n",
      "755, train loss: 0.630818, test loss: 0.651315, train acc: 0.725904, test acc: 0.694915\n",
      "756, train loss: 0.630746, test loss: 0.651248, train acc: 0.725904, test acc: 0.694915\n",
      "757, train loss: 0.630674, test loss: 0.651183, train acc: 0.725904, test acc: 0.694915\n",
      "758, train loss: 0.630603, test loss: 0.651117, train acc: 0.725904, test acc: 0.694915\n",
      "759, train loss: 0.630531, test loss: 0.651051, train acc: 0.725904, test acc: 0.694915\n",
      "760, train loss: 0.63046, test loss: 0.650985, train acc: 0.725904, test acc: 0.694915\n",
      "761, train loss: 0.630389, test loss: 0.65092, train acc: 0.725904, test acc: 0.694915\n",
      "762, train loss: 0.630319, test loss: 0.650855, train acc: 0.725904, test acc: 0.694915\n",
      "763, train loss: 0.630248, test loss: 0.65079, train acc: 0.725904, test acc: 0.694915\n",
      "764, train loss: 0.630178, test loss: 0.650725, train acc: 0.725904, test acc: 0.694915\n",
      "765, train loss: 0.630107, test loss: 0.65066, train acc: 0.725904, test acc: 0.694915\n",
      "766, train loss: 0.630037, test loss: 0.650595, train acc: 0.725904, test acc: 0.694915\n",
      "767, train loss: 0.629967, test loss: 0.650531, train acc: 0.725904, test acc: 0.694915\n",
      "768, train loss: 0.629897, test loss: 0.650467, train acc: 0.725904, test acc: 0.694915\n",
      "769, train loss: 0.629828, test loss: 0.650403, train acc: 0.725904, test acc: 0.694915\n",
      "770, train loss: 0.629758, test loss: 0.650339, train acc: 0.725904, test acc: 0.694915\n",
      "771, train loss: 0.629689, test loss: 0.650275, train acc: 0.725904, test acc: 0.694915\n",
      "772, train loss: 0.62962, test loss: 0.650211, train acc: 0.725904, test acc: 0.694915\n",
      "773, train loss: 0.629551, test loss: 0.650148, train acc: 0.725904, test acc: 0.694915\n",
      "774, train loss: 0.629482, test loss: 0.650085, train acc: 0.725904, test acc: 0.694915\n",
      "775, train loss: 0.629413, test loss: 0.650021, train acc: 0.725904, test acc: 0.694915\n",
      "776, train loss: 0.629345, test loss: 0.649958, train acc: 0.722892, test acc: 0.694915\n",
      "777, train loss: 0.629276, test loss: 0.649896, train acc: 0.722892, test acc: 0.694915\n",
      "778, train loss: 0.629208, test loss: 0.649833, train acc: 0.722892, test acc: 0.694915\n",
      "779, train loss: 0.62914, test loss: 0.64977, train acc: 0.722892, test acc: 0.694915\n",
      "780, train loss: 0.629072, test loss: 0.649708, train acc: 0.725904, test acc: 0.694915\n",
      "781, train loss: 0.629005, test loss: 0.649646, train acc: 0.725904, test acc: 0.694915\n",
      "782, train loss: 0.628937, test loss: 0.649584, train acc: 0.725904, test acc: 0.694915\n",
      "783, train loss: 0.62887, test loss: 0.649522, train acc: 0.725904, test acc: 0.694915\n",
      "784, train loss: 0.628803, test loss: 0.64946, train acc: 0.725904, test acc: 0.694915\n",
      "785, train loss: 0.628735, test loss: 0.649398, train acc: 0.725904, test acc: 0.694915\n",
      "786, train loss: 0.628669, test loss: 0.649337, train acc: 0.725904, test acc: 0.694915\n",
      "787, train loss: 0.628602, test loss: 0.649276, train acc: 0.725904, test acc: 0.694915\n",
      "788, train loss: 0.628535, test loss: 0.649215, train acc: 0.725904, test acc: 0.694915\n",
      "789, train loss: 0.628469, test loss: 0.649153, train acc: 0.725904, test acc: 0.694915\n",
      "790, train loss: 0.628402, test loss: 0.649093, train acc: 0.725904, test acc: 0.694915\n",
      "791, train loss: 0.628336, test loss: 0.649032, train acc: 0.725904, test acc: 0.694915\n",
      "792, train loss: 0.62827, test loss: 0.648971, train acc: 0.725904, test acc: 0.694915\n",
      "793, train loss: 0.628204, test loss: 0.648911, train acc: 0.725904, test acc: 0.694915\n",
      "794, train loss: 0.628139, test loss: 0.64885, train acc: 0.725904, test acc: 0.694915\n",
      "795, train loss: 0.628073, test loss: 0.64879, train acc: 0.725904, test acc: 0.694915\n",
      "796, train loss: 0.628008, test loss: 0.64873, train acc: 0.725904, test acc: 0.694915\n",
      "797, train loss: 0.627942, test loss: 0.64867, train acc: 0.725904, test acc: 0.694915\n",
      "798, train loss: 0.627877, test loss: 0.648611, train acc: 0.725904, test acc: 0.694915\n",
      "799, train loss: 0.627812, test loss: 0.648551, train acc: 0.725904, test acc: 0.694915\n",
      "800, train loss: 0.627748, test loss: 0.648492, train acc: 0.725904, test acc: 0.694915\n",
      "801, train loss: 0.627683, test loss: 0.648432, train acc: 0.725904, test acc: 0.694915\n",
      "802, train loss: 0.627618, test loss: 0.648373, train acc: 0.725904, test acc: 0.694915\n",
      "803, train loss: 0.627554, test loss: 0.648314, train acc: 0.725904, test acc: 0.694915\n",
      "804, train loss: 0.62749, test loss: 0.648255, train acc: 0.725904, test acc: 0.694915\n",
      "805, train loss: 0.627426, test loss: 0.648197, train acc: 0.725904, test acc: 0.694915\n",
      "806, train loss: 0.627362, test loss: 0.648138, train acc: 0.725904, test acc: 0.694915\n",
      "807, train loss: 0.627298, test loss: 0.64808, train acc: 0.725904, test acc: 0.694915\n",
      "808, train loss: 0.627234, test loss: 0.648021, train acc: 0.725904, test acc: 0.694915\n",
      "809, train loss: 0.627171, test loss: 0.647963, train acc: 0.725904, test acc: 0.694915\n",
      "810, train loss: 0.627108, test loss: 0.647905, train acc: 0.725904, test acc: 0.694915\n",
      "811, train loss: 0.627044, test loss: 0.647847, train acc: 0.725904, test acc: 0.694915\n",
      "812, train loss: 0.626981, test loss: 0.647789, train acc: 0.725904, test acc: 0.694915\n",
      "813, train loss: 0.626918, test loss: 0.647732, train acc: 0.725904, test acc: 0.694915\n",
      "814, train loss: 0.626855, test loss: 0.647674, train acc: 0.725904, test acc: 0.694915\n",
      "815, train loss: 0.626793, test loss: 0.647617, train acc: 0.725904, test acc: 0.694915\n",
      "816, train loss: 0.62673, test loss: 0.647559, train acc: 0.725904, test acc: 0.694915\n",
      "817, train loss: 0.626668, test loss: 0.647502, train acc: 0.725904, test acc: 0.694915\n",
      "818, train loss: 0.626606, test loss: 0.647445, train acc: 0.725904, test acc: 0.694915\n",
      "819, train loss: 0.626544, test loss: 0.647388, train acc: 0.725904, test acc: 0.694915\n",
      "820, train loss: 0.626482, test loss: 0.647332, train acc: 0.725904, test acc: 0.694915\n",
      "821, train loss: 0.62642, test loss: 0.647275, train acc: 0.725904, test acc: 0.694915\n",
      "822, train loss: 0.626358, test loss: 0.647219, train acc: 0.725904, test acc: 0.694915\n",
      "823, train loss: 0.626297, test loss: 0.647162, train acc: 0.725904, test acc: 0.694915\n",
      "824, train loss: 0.626235, test loss: 0.647106, train acc: 0.725904, test acc: 0.694915\n",
      "825, train loss: 0.626174, test loss: 0.64705, train acc: 0.725904, test acc: 0.694915\n",
      "826, train loss: 0.626113, test loss: 0.646994, train acc: 0.722892, test acc: 0.694915\n",
      "827, train loss: 0.626052, test loss: 0.646938, train acc: 0.722892, test acc: 0.694915\n",
      "828, train loss: 0.625991, test loss: 0.646883, train acc: 0.722892, test acc: 0.694915\n",
      "829, train loss: 0.62593, test loss: 0.646827, train acc: 0.722892, test acc: 0.694915\n",
      "830, train loss: 0.625869, test loss: 0.646772, train acc: 0.722892, test acc: 0.694915\n",
      "831, train loss: 0.625809, test loss: 0.646716, train acc: 0.722892, test acc: 0.694915\n",
      "832, train loss: 0.625749, test loss: 0.646661, train acc: 0.722892, test acc: 0.694915\n",
      "833, train loss: 0.625688, test loss: 0.646606, train acc: 0.722892, test acc: 0.694915\n",
      "834, train loss: 0.625628, test loss: 0.646551, train acc: 0.722892, test acc: 0.694915\n",
      "835, train loss: 0.625568, test loss: 0.646496, train acc: 0.722892, test acc: 0.694915\n",
      "836, train loss: 0.625508, test loss: 0.646442, train acc: 0.722892, test acc: 0.694915\n",
      "837, train loss: 0.625449, test loss: 0.646387, train acc: 0.722892, test acc: 0.694915\n",
      "838, train loss: 0.625389, test loss: 0.646333, train acc: 0.722892, test acc: 0.694915\n",
      "839, train loss: 0.62533, test loss: 0.646279, train acc: 0.722892, test acc: 0.694915\n",
      "840, train loss: 0.62527, test loss: 0.646224, train acc: 0.722892, test acc: 0.694915\n",
      "841, train loss: 0.625211, test loss: 0.64617, train acc: 0.722892, test acc: 0.694915\n",
      "842, train loss: 0.625152, test loss: 0.646116, train acc: 0.722892, test acc: 0.694915\n",
      "843, train loss: 0.625093, test loss: 0.646062, train acc: 0.722892, test acc: 0.694915\n",
      "844, train loss: 0.625034, test loss: 0.646009, train acc: 0.722892, test acc: 0.694915\n",
      "845, train loss: 0.624976, test loss: 0.645955, train acc: 0.722892, test acc: 0.694915\n",
      "846, train loss: 0.624917, test loss: 0.645902, train acc: 0.722892, test acc: 0.694915\n",
      "847, train loss: 0.624859, test loss: 0.645848, train acc: 0.722892, test acc: 0.694915\n",
      "848, train loss: 0.6248, test loss: 0.645795, train acc: 0.722892, test acc: 0.694915\n",
      "849, train loss: 0.624742, test loss: 0.645742, train acc: 0.722892, test acc: 0.694915\n",
      "850, train loss: 0.624684, test loss: 0.645689, train acc: 0.722892, test acc: 0.694915\n",
      "851, train loss: 0.624626, test loss: 0.645636, train acc: 0.722892, test acc: 0.694915\n",
      "852, train loss: 0.624568, test loss: 0.645583, train acc: 0.722892, test acc: 0.694915\n",
      "853, train loss: 0.624511, test loss: 0.645531, train acc: 0.722892, test acc: 0.694915\n",
      "854, train loss: 0.624453, test loss: 0.645478, train acc: 0.722892, test acc: 0.694915\n",
      "855, train loss: 0.624396, test loss: 0.645426, train acc: 0.722892, test acc: 0.694915\n",
      "856, train loss: 0.624338, test loss: 0.645374, train acc: 0.722892, test acc: 0.694915\n",
      "857, train loss: 0.624281, test loss: 0.645321, train acc: 0.722892, test acc: 0.694915\n",
      "858, train loss: 0.624224, test loss: 0.645269, train acc: 0.722892, test acc: 0.694915\n",
      "859, train loss: 0.624167, test loss: 0.645217, train acc: 0.722892, test acc: 0.694915\n",
      "860, train loss: 0.62411, test loss: 0.645166, train acc: 0.722892, test acc: 0.694915\n",
      "861, train loss: 0.624054, test loss: 0.645114, train acc: 0.722892, test acc: 0.694915\n",
      "862, train loss: 0.623997, test loss: 0.645062, train acc: 0.722892, test acc: 0.694915\n",
      "863, train loss: 0.623941, test loss: 0.645011, train acc: 0.722892, test acc: 0.694915\n",
      "864, train loss: 0.623884, test loss: 0.644959, train acc: 0.722892, test acc: 0.694915\n",
      "865, train loss: 0.623828, test loss: 0.644908, train acc: 0.722892, test acc: 0.694915\n",
      "866, train loss: 0.623772, test loss: 0.644857, train acc: 0.722892, test acc: 0.694915\n",
      "867, train loss: 0.623716, test loss: 0.644806, train acc: 0.722892, test acc: 0.694915\n",
      "868, train loss: 0.62366, test loss: 0.644755, train acc: 0.722892, test acc: 0.694915\n",
      "869, train loss: 0.623604, test loss: 0.644704, train acc: 0.722892, test acc: 0.694915\n",
      "870, train loss: 0.623548, test loss: 0.644653, train acc: 0.722892, test acc: 0.694915\n",
      "871, train loss: 0.623493, test loss: 0.644603, train acc: 0.722892, test acc: 0.694915\n",
      "872, train loss: 0.623437, test loss: 0.644552, train acc: 0.722892, test acc: 0.694915\n",
      "873, train loss: 0.623382, test loss: 0.644502, train acc: 0.722892, test acc: 0.694915\n",
      "874, train loss: 0.623327, test loss: 0.644452, train acc: 0.722892, test acc: 0.694915\n",
      "875, train loss: 0.623272, test loss: 0.644401, train acc: 0.722892, test acc: 0.694915\n",
      "876, train loss: 0.623217, test loss: 0.644351, train acc: 0.722892, test acc: 0.694915\n",
      "877, train loss: 0.623162, test loss: 0.644301, train acc: 0.722892, test acc: 0.694915\n",
      "878, train loss: 0.623107, test loss: 0.644252, train acc: 0.722892, test acc: 0.694915\n",
      "879, train loss: 0.623053, test loss: 0.644202, train acc: 0.722892, test acc: 0.694915\n",
      "880, train loss: 0.622998, test loss: 0.644152, train acc: 0.722892, test acc: 0.694915\n",
      "881, train loss: 0.622944, test loss: 0.644103, train acc: 0.722892, test acc: 0.694915\n",
      "882, train loss: 0.622889, test loss: 0.644053, train acc: 0.722892, test acc: 0.694915\n",
      "883, train loss: 0.622835, test loss: 0.644004, train acc: 0.722892, test acc: 0.694915\n",
      "884, train loss: 0.622781, test loss: 0.643955, train acc: 0.722892, test acc: 0.694915\n",
      "885, train loss: 0.622727, test loss: 0.643906, train acc: 0.722892, test acc: 0.694915\n",
      "886, train loss: 0.622673, test loss: 0.643857, train acc: 0.722892, test acc: 0.694915\n",
      "887, train loss: 0.622619, test loss: 0.643808, train acc: 0.722892, test acc: 0.694915\n",
      "888, train loss: 0.622566, test loss: 0.643759, train acc: 0.722892, test acc: 0.694915\n",
      "889, train loss: 0.622512, test loss: 0.64371, train acc: 0.722892, test acc: 0.694915\n",
      "890, train loss: 0.622459, test loss: 0.643662, train acc: 0.722892, test acc: 0.694915\n",
      "891, train loss: 0.622405, test loss: 0.643613, train acc: 0.722892, test acc: 0.694915\n",
      "892, train loss: 0.622352, test loss: 0.643565, train acc: 0.722892, test acc: 0.694915\n",
      "893, train loss: 0.622299, test loss: 0.643516, train acc: 0.722892, test acc: 0.694915\n",
      "894, train loss: 0.622246, test loss: 0.643468, train acc: 0.722892, test acc: 0.694915\n",
      "895, train loss: 0.622193, test loss: 0.64342, train acc: 0.722892, test acc: 0.694915\n",
      "896, train loss: 0.62214, test loss: 0.643372, train acc: 0.722892, test acc: 0.694915\n",
      "897, train loss: 0.622088, test loss: 0.643324, train acc: 0.722892, test acc: 0.694915\n",
      "898, train loss: 0.622035, test loss: 0.643276, train acc: 0.722892, test acc: 0.694915\n",
      "899, train loss: 0.621983, test loss: 0.643229, train acc: 0.722892, test acc: 0.694915\n",
      "900, train loss: 0.62193, test loss: 0.643181, train acc: 0.722892, test acc: 0.694915\n",
      "901, train loss: 0.621878, test loss: 0.643134, train acc: 0.722892, test acc: 0.694915\n",
      "902, train loss: 0.621826, test loss: 0.643086, train acc: 0.722892, test acc: 0.694915\n",
      "903, train loss: 0.621774, test loss: 0.643039, train acc: 0.722892, test acc: 0.694915\n",
      "904, train loss: 0.621722, test loss: 0.642992, train acc: 0.722892, test acc: 0.694915\n",
      "905, train loss: 0.62167, test loss: 0.642945, train acc: 0.722892, test acc: 0.694915\n",
      "906, train loss: 0.621618, test loss: 0.642898, train acc: 0.722892, test acc: 0.694915\n",
      "907, train loss: 0.621567, test loss: 0.642851, train acc: 0.722892, test acc: 0.694915\n",
      "908, train loss: 0.621515, test loss: 0.642804, train acc: 0.722892, test acc: 0.694915\n",
      "909, train loss: 0.621464, test loss: 0.642757, train acc: 0.722892, test acc: 0.694915\n",
      "910, train loss: 0.621412, test loss: 0.64271, train acc: 0.722892, test acc: 0.694915\n",
      "911, train loss: 0.621361, test loss: 0.642664, train acc: 0.722892, test acc: 0.694915\n",
      "912, train loss: 0.62131, test loss: 0.642618, train acc: 0.722892, test acc: 0.694915\n",
      "913, train loss: 0.621259, test loss: 0.642571, train acc: 0.722892, test acc: 0.694915\n",
      "914, train loss: 0.621208, test loss: 0.642525, train acc: 0.722892, test acc: 0.694915\n",
      "915, train loss: 0.621157, test loss: 0.642479, train acc: 0.722892, test acc: 0.694915\n",
      "916, train loss: 0.621106, test loss: 0.642433, train acc: 0.722892, test acc: 0.694915\n",
      "917, train loss: 0.621056, test loss: 0.642387, train acc: 0.722892, test acc: 0.694915\n",
      "918, train loss: 0.621005, test loss: 0.642341, train acc: 0.722892, test acc: 0.694915\n",
      "919, train loss: 0.620955, test loss: 0.642295, train acc: 0.722892, test acc: 0.694915\n",
      "920, train loss: 0.620905, test loss: 0.642249, train acc: 0.722892, test acc: 0.694915\n",
      "921, train loss: 0.620854, test loss: 0.642204, train acc: 0.722892, test acc: 0.694915\n",
      "922, train loss: 0.620804, test loss: 0.642158, train acc: 0.722892, test acc: 0.694915\n",
      "923, train loss: 0.620754, test loss: 0.642113, train acc: 0.722892, test acc: 0.694915\n",
      "924, train loss: 0.620704, test loss: 0.642067, train acc: 0.722892, test acc: 0.694915\n",
      "925, train loss: 0.620654, test loss: 0.642022, train acc: 0.722892, test acc: 0.694915\n",
      "926, train loss: 0.620605, test loss: 0.641977, train acc: 0.722892, test acc: 0.694915\n",
      "927, train loss: 0.620555, test loss: 0.641932, train acc: 0.722892, test acc: 0.694915\n",
      "928, train loss: 0.620505, test loss: 0.641887, train acc: 0.722892, test acc: 0.694915\n",
      "929, train loss: 0.620456, test loss: 0.641842, train acc: 0.722892, test acc: 0.694915\n",
      "930, train loss: 0.620406, test loss: 0.641797, train acc: 0.722892, test acc: 0.694915\n",
      "931, train loss: 0.620357, test loss: 0.641753, train acc: 0.722892, test acc: 0.694915\n",
      "932, train loss: 0.620308, test loss: 0.641708, train acc: 0.722892, test acc: 0.694915\n",
      "933, train loss: 0.620259, test loss: 0.641663, train acc: 0.722892, test acc: 0.694915\n",
      "934, train loss: 0.62021, test loss: 0.641619, train acc: 0.722892, test acc: 0.694915\n",
      "935, train loss: 0.620161, test loss: 0.641575, train acc: 0.722892, test acc: 0.694915\n",
      "936, train loss: 0.620112, test loss: 0.641531, train acc: 0.722892, test acc: 0.694915\n",
      "937, train loss: 0.620064, test loss: 0.641486, train acc: 0.722892, test acc: 0.694915\n",
      "938, train loss: 0.620015, test loss: 0.641442, train acc: 0.722892, test acc: 0.694915\n",
      "939, train loss: 0.619966, test loss: 0.641398, train acc: 0.722892, test acc: 0.694915\n",
      "940, train loss: 0.619918, test loss: 0.641354, train acc: 0.722892, test acc: 0.694915\n",
      "941, train loss: 0.61987, test loss: 0.64131, train acc: 0.722892, test acc: 0.694915\n",
      "942, train loss: 0.619821, test loss: 0.641267, train acc: 0.722892, test acc: 0.694915\n",
      "943, train loss: 0.619773, test loss: 0.641223, train acc: 0.722892, test acc: 0.694915\n",
      "944, train loss: 0.619725, test loss: 0.64118, train acc: 0.722892, test acc: 0.694915\n",
      "945, train loss: 0.619677, test loss: 0.641136, train acc: 0.722892, test acc: 0.694915\n",
      "946, train loss: 0.619629, test loss: 0.641093, train acc: 0.722892, test acc: 0.694915\n",
      "947, train loss: 0.619581, test loss: 0.641049, train acc: 0.722892, test acc: 0.694915\n",
      "948, train loss: 0.619534, test loss: 0.641006, train acc: 0.722892, test acc: 0.694915\n",
      "949, train loss: 0.619486, test loss: 0.640963, train acc: 0.722892, test acc: 0.694915\n",
      "950, train loss: 0.619439, test loss: 0.64092, train acc: 0.722892, test acc: 0.694915\n",
      "951, train loss: 0.619391, test loss: 0.640877, train acc: 0.722892, test acc: 0.694915\n",
      "952, train loss: 0.619344, test loss: 0.640834, train acc: 0.722892, test acc: 0.694915\n",
      "953, train loss: 0.619296, test loss: 0.640791, train acc: 0.722892, test acc: 0.694915\n",
      "954, train loss: 0.619249, test loss: 0.640748, train acc: 0.722892, test acc: 0.694915\n",
      "955, train loss: 0.619202, test loss: 0.640706, train acc: 0.722892, test acc: 0.694915\n",
      "956, train loss: 0.619155, test loss: 0.640663, train acc: 0.722892, test acc: 0.694915\n",
      "957, train loss: 0.619108, test loss: 0.640621, train acc: 0.722892, test acc: 0.694915\n",
      "958, train loss: 0.619062, test loss: 0.640578, train acc: 0.722892, test acc: 0.694915\n",
      "959, train loss: 0.619015, test loss: 0.640536, train acc: 0.722892, test acc: 0.694915\n",
      "960, train loss: 0.618968, test loss: 0.640494, train acc: 0.722892, test acc: 0.694915\n",
      "961, train loss: 0.618922, test loss: 0.640451, train acc: 0.722892, test acc: 0.694915\n",
      "962, train loss: 0.618875, test loss: 0.640409, train acc: 0.722892, test acc: 0.694915\n",
      "963, train loss: 0.618829, test loss: 0.640367, train acc: 0.722892, test acc: 0.694915\n",
      "964, train loss: 0.618782, test loss: 0.640325, train acc: 0.722892, test acc: 0.694915\n",
      "965, train loss: 0.618736, test loss: 0.640283, train acc: 0.722892, test acc: 0.694915\n",
      "966, train loss: 0.61869, test loss: 0.640242, train acc: 0.722892, test acc: 0.694915\n",
      "967, train loss: 0.618644, test loss: 0.6402, train acc: 0.725904, test acc: 0.694915\n",
      "968, train loss: 0.618598, test loss: 0.640158, train acc: 0.725904, test acc: 0.694915\n",
      "969, train loss: 0.618552, test loss: 0.640117, train acc: 0.725904, test acc: 0.694915\n",
      "970, train loss: 0.618506, test loss: 0.640075, train acc: 0.725904, test acc: 0.694915\n",
      "971, train loss: 0.61846, test loss: 0.640034, train acc: 0.725904, test acc: 0.694915\n",
      "972, train loss: 0.618415, test loss: 0.639993, train acc: 0.725904, test acc: 0.694915\n",
      "973, train loss: 0.618369, test loss: 0.639951, train acc: 0.725904, test acc: 0.694915\n",
      "974, train loss: 0.618324, test loss: 0.63991, train acc: 0.725904, test acc: 0.694915\n",
      "975, train loss: 0.618279, test loss: 0.639869, train acc: 0.725904, test acc: 0.694915\n",
      "976, train loss: 0.618233, test loss: 0.639828, train acc: 0.725904, test acc: 0.694915\n",
      "977, train loss: 0.618188, test loss: 0.639787, train acc: 0.725904, test acc: 0.694915\n",
      "978, train loss: 0.618143, test loss: 0.639746, train acc: 0.725904, test acc: 0.694915\n",
      "979, train loss: 0.618098, test loss: 0.639706, train acc: 0.725904, test acc: 0.694915\n",
      "980, train loss: 0.618053, test loss: 0.639665, train acc: 0.725904, test acc: 0.694915\n",
      "981, train loss: 0.618008, test loss: 0.639624, train acc: 0.725904, test acc: 0.694915\n",
      "982, train loss: 0.617963, test loss: 0.639584, train acc: 0.725904, test acc: 0.694915\n",
      "983, train loss: 0.617918, test loss: 0.639543, train acc: 0.725904, test acc: 0.694915\n",
      "984, train loss: 0.617874, test loss: 0.639503, train acc: 0.725904, test acc: 0.694915\n",
      "985, train loss: 0.617829, test loss: 0.639463, train acc: 0.725904, test acc: 0.694915\n",
      "986, train loss: 0.617785, test loss: 0.639422, train acc: 0.725904, test acc: 0.694915\n",
      "987, train loss: 0.61774, test loss: 0.639382, train acc: 0.725904, test acc: 0.694915\n",
      "988, train loss: 0.617696, test loss: 0.639342, train acc: 0.725904, test acc: 0.694915\n",
      "989, train loss: 0.617652, test loss: 0.639302, train acc: 0.725904, test acc: 0.694915\n",
      "990, train loss: 0.617607, test loss: 0.639262, train acc: 0.725904, test acc: 0.694915\n",
      "991, train loss: 0.617563, test loss: 0.639222, train acc: 0.725904, test acc: 0.694915\n",
      "992, train loss: 0.617519, test loss: 0.639182, train acc: 0.725904, test acc: 0.694915\n",
      "993, train loss: 0.617475, test loss: 0.639143, train acc: 0.725904, test acc: 0.694915\n",
      "994, train loss: 0.617431, test loss: 0.639103, train acc: 0.725904, test acc: 0.694915\n",
      "995, train loss: 0.617388, test loss: 0.639063, train acc: 0.725904, test acc: 0.694915\n",
      "996, train loss: 0.617344, test loss: 0.639024, train acc: 0.725904, test acc: 0.694915\n",
      "997, train loss: 0.6173, test loss: 0.638984, train acc: 0.725904, test acc: 0.694915\n",
      "998, train loss: 0.617257, test loss: 0.638945, train acc: 0.725904, test acc: 0.694915\n",
      "999, train loss: 0.617213, test loss: 0.638906, train acc: 0.725904, test acc: 0.694915\n",
      "1000, train loss: 0.61717, test loss: 0.638866, train acc: 0.725904, test acc: 0.694915\n",
      "1001, train loss: 0.617127, test loss: 0.638827, train acc: 0.725904, test acc: 0.694915\n",
      "1002, train loss: 0.617083, test loss: 0.638788, train acc: 0.725904, test acc: 0.694915\n",
      "1003, train loss: 0.61704, test loss: 0.638749, train acc: 0.725904, test acc: 0.694915\n",
      "1004, train loss: 0.616997, test loss: 0.63871, train acc: 0.725904, test acc: 0.694915\n",
      "1005, train loss: 0.616954, test loss: 0.638671, train acc: 0.725904, test acc: 0.694915\n",
      "1006, train loss: 0.616911, test loss: 0.638632, train acc: 0.725904, test acc: 0.694915\n",
      "1007, train loss: 0.616868, test loss: 0.638593, train acc: 0.725904, test acc: 0.694915\n",
      "1008, train loss: 0.616825, test loss: 0.638555, train acc: 0.725904, test acc: 0.694915\n",
      "1009, train loss: 0.616783, test loss: 0.638516, train acc: 0.725904, test acc: 0.694915\n",
      "1010, train loss: 0.61674, test loss: 0.638478, train acc: 0.725904, test acc: 0.694915\n",
      "1011, train loss: 0.616697, test loss: 0.638439, train acc: 0.725904, test acc: 0.694915\n",
      "1012, train loss: 0.616655, test loss: 0.638401, train acc: 0.725904, test acc: 0.694915\n",
      "1013, train loss: 0.616612, test loss: 0.638362, train acc: 0.725904, test acc: 0.694915\n",
      "1014, train loss: 0.61657, test loss: 0.638324, train acc: 0.725904, test acc: 0.694915\n",
      "1015, train loss: 0.616528, test loss: 0.638286, train acc: 0.725904, test acc: 0.694915\n",
      "1016, train loss: 0.616486, test loss: 0.638248, train acc: 0.725904, test acc: 0.694915\n",
      "1017, train loss: 0.616444, test loss: 0.63821, train acc: 0.725904, test acc: 0.694915\n",
      "1018, train loss: 0.616401, test loss: 0.638172, train acc: 0.725904, test acc: 0.694915\n",
      "1019, train loss: 0.616359, test loss: 0.638134, train acc: 0.725904, test acc: 0.694915\n",
      "1020, train loss: 0.616318, test loss: 0.638096, train acc: 0.725904, test acc: 0.694915\n",
      "1021, train loss: 0.616276, test loss: 0.638058, train acc: 0.728916, test acc: 0.694915\n",
      "1022, train loss: 0.616234, test loss: 0.63802, train acc: 0.728916, test acc: 0.694915\n",
      "1023, train loss: 0.616192, test loss: 0.637982, train acc: 0.728916, test acc: 0.694915\n",
      "1024, train loss: 0.616151, test loss: 0.637945, train acc: 0.728916, test acc: 0.694915\n",
      "1025, train loss: 0.616109, test loss: 0.637907, train acc: 0.728916, test acc: 0.694915\n",
      "1026, train loss: 0.616067, test loss: 0.63787, train acc: 0.728916, test acc: 0.694915\n",
      "1027, train loss: 0.616026, test loss: 0.637832, train acc: 0.728916, test acc: 0.694915\n",
      "1028, train loss: 0.615985, test loss: 0.637795, train acc: 0.728916, test acc: 0.694915\n",
      "1029, train loss: 0.615943, test loss: 0.637758, train acc: 0.728916, test acc: 0.694915\n",
      "1030, train loss: 0.615902, test loss: 0.63772, train acc: 0.728916, test acc: 0.694915\n",
      "1031, train loss: 0.615861, test loss: 0.637683, train acc: 0.728916, test acc: 0.694915\n",
      "1032, train loss: 0.61582, test loss: 0.637646, train acc: 0.728916, test acc: 0.694915\n",
      "1033, train loss: 0.615779, test loss: 0.637609, train acc: 0.728916, test acc: 0.694915\n",
      "1034, train loss: 0.615738, test loss: 0.637572, train acc: 0.728916, test acc: 0.694915\n",
      "1035, train loss: 0.615697, test loss: 0.637535, train acc: 0.728916, test acc: 0.694915\n",
      "1036, train loss: 0.615656, test loss: 0.637498, train acc: 0.728916, test acc: 0.694915\n",
      "1037, train loss: 0.615616, test loss: 0.637462, train acc: 0.728916, test acc: 0.694915\n",
      "1038, train loss: 0.615575, test loss: 0.637425, train acc: 0.728916, test acc: 0.694915\n",
      "1039, train loss: 0.615534, test loss: 0.637388, train acc: 0.728916, test acc: 0.694915\n",
      "1040, train loss: 0.615494, test loss: 0.637351, train acc: 0.728916, test acc: 0.694915\n",
      "1041, train loss: 0.615454, test loss: 0.637315, train acc: 0.728916, test acc: 0.694915\n",
      "1042, train loss: 0.615413, test loss: 0.637278, train acc: 0.728916, test acc: 0.694915\n",
      "1043, train loss: 0.615373, test loss: 0.637242, train acc: 0.728916, test acc: 0.694915\n",
      "1044, train loss: 0.615333, test loss: 0.637206, train acc: 0.728916, test acc: 0.711864\n",
      "1045, train loss: 0.615292, test loss: 0.637169, train acc: 0.728916, test acc: 0.711864\n",
      "1046, train loss: 0.615252, test loss: 0.637133, train acc: 0.728916, test acc: 0.711864\n",
      "1047, train loss: 0.615212, test loss: 0.637097, train acc: 0.728916, test acc: 0.711864\n",
      "1048, train loss: 0.615172, test loss: 0.637061, train acc: 0.728916, test acc: 0.711864\n",
      "1049, train loss: 0.615132, test loss: 0.637025, train acc: 0.728916, test acc: 0.711864\n",
      "1050, train loss: 0.615092, test loss: 0.636989, train acc: 0.728916, test acc: 0.711864\n",
      "1051, train loss: 0.615053, test loss: 0.636953, train acc: 0.728916, test acc: 0.711864\n",
      "1052, train loss: 0.615013, test loss: 0.636917, train acc: 0.728916, test acc: 0.711864\n",
      "1053, train loss: 0.614973, test loss: 0.636881, train acc: 0.728916, test acc: 0.711864\n",
      "1054, train loss: 0.614934, test loss: 0.636845, train acc: 0.728916, test acc: 0.711864\n",
      "1055, train loss: 0.614894, test loss: 0.63681, train acc: 0.728916, test acc: 0.711864\n",
      "1056, train loss: 0.614855, test loss: 0.636774, train acc: 0.728916, test acc: 0.711864\n",
      "1057, train loss: 0.614815, test loss: 0.636738, train acc: 0.728916, test acc: 0.711864\n",
      "1058, train loss: 0.614776, test loss: 0.636703, train acc: 0.728916, test acc: 0.711864\n",
      "1059, train loss: 0.614737, test loss: 0.636667, train acc: 0.728916, test acc: 0.711864\n",
      "1060, train loss: 0.614697, test loss: 0.636632, train acc: 0.728916, test acc: 0.711864\n",
      "1061, train loss: 0.614658, test loss: 0.636597, train acc: 0.728916, test acc: 0.711864\n",
      "1062, train loss: 0.614619, test loss: 0.636561, train acc: 0.728916, test acc: 0.711864\n",
      "1063, train loss: 0.61458, test loss: 0.636526, train acc: 0.728916, test acc: 0.711864\n",
      "1064, train loss: 0.614541, test loss: 0.636491, train acc: 0.728916, test acc: 0.711864\n",
      "1065, train loss: 0.614502, test loss: 0.636456, train acc: 0.725904, test acc: 0.711864\n",
      "1066, train loss: 0.614464, test loss: 0.636421, train acc: 0.725904, test acc: 0.711864\n",
      "1067, train loss: 0.614425, test loss: 0.636386, train acc: 0.725904, test acc: 0.711864\n",
      "1068, train loss: 0.614386, test loss: 0.636351, train acc: 0.725904, test acc: 0.711864\n",
      "1069, train loss: 0.614347, test loss: 0.636316, train acc: 0.725904, test acc: 0.711864\n",
      "1070, train loss: 0.614309, test loss: 0.636281, train acc: 0.725904, test acc: 0.711864\n",
      "1071, train loss: 0.61427, test loss: 0.636246, train acc: 0.725904, test acc: 0.711864\n",
      "1072, train loss: 0.614232, test loss: 0.636211, train acc: 0.725904, test acc: 0.711864\n",
      "1073, train loss: 0.614194, test loss: 0.636177, train acc: 0.725904, test acc: 0.711864\n",
      "1074, train loss: 0.614155, test loss: 0.636142, train acc: 0.725904, test acc: 0.711864\n",
      "1075, train loss: 0.614117, test loss: 0.636108, train acc: 0.725904, test acc: 0.711864\n",
      "1076, train loss: 0.614079, test loss: 0.636073, train acc: 0.725904, test acc: 0.711864\n",
      "1077, train loss: 0.61404, test loss: 0.636038, train acc: 0.725904, test acc: 0.711864\n",
      "1078, train loss: 0.614002, test loss: 0.636004, train acc: 0.725904, test acc: 0.711864\n",
      "1079, train loss: 0.613964, test loss: 0.63597, train acc: 0.725904, test acc: 0.711864\n",
      "1080, train loss: 0.613926, test loss: 0.635935, train acc: 0.725904, test acc: 0.711864\n",
      "1081, train loss: 0.613889, test loss: 0.635901, train acc: 0.725904, test acc: 0.711864\n",
      "1082, train loss: 0.613851, test loss: 0.635867, train acc: 0.725904, test acc: 0.728814\n",
      "1083, train loss: 0.613813, test loss: 0.635833, train acc: 0.725904, test acc: 0.728814\n",
      "1084, train loss: 0.613775, test loss: 0.635799, train acc: 0.725904, test acc: 0.728814\n",
      "1085, train loss: 0.613738, test loss: 0.635765, train acc: 0.725904, test acc: 0.728814\n",
      "1086, train loss: 0.6137, test loss: 0.635731, train acc: 0.725904, test acc: 0.728814\n",
      "1087, train loss: 0.613662, test loss: 0.635697, train acc: 0.725904, test acc: 0.728814\n",
      "1088, train loss: 0.613625, test loss: 0.635663, train acc: 0.725904, test acc: 0.728814\n",
      "1089, train loss: 0.613587, test loss: 0.635629, train acc: 0.725904, test acc: 0.728814\n",
      "1090, train loss: 0.61355, test loss: 0.635596, train acc: 0.725904, test acc: 0.728814\n",
      "1091, train loss: 0.613513, test loss: 0.635562, train acc: 0.725904, test acc: 0.728814\n",
      "1092, train loss: 0.613476, test loss: 0.635528, train acc: 0.725904, test acc: 0.728814\n",
      "1093, train loss: 0.613438, test loss: 0.635495, train acc: 0.725904, test acc: 0.728814\n",
      "1094, train loss: 0.613401, test loss: 0.635461, train acc: 0.725904, test acc: 0.728814\n",
      "1095, train loss: 0.613364, test loss: 0.635428, train acc: 0.725904, test acc: 0.728814\n",
      "1096, train loss: 0.613327, test loss: 0.635394, train acc: 0.725904, test acc: 0.728814\n",
      "1097, train loss: 0.61329, test loss: 0.635361, train acc: 0.725904, test acc: 0.728814\n",
      "1098, train loss: 0.613253, test loss: 0.635327, train acc: 0.725904, test acc: 0.728814\n",
      "1099, train loss: 0.613216, test loss: 0.635294, train acc: 0.725904, test acc: 0.728814\n",
      "1100, train loss: 0.61318, test loss: 0.635261, train acc: 0.725904, test acc: 0.728814\n",
      "1101, train loss: 0.613143, test loss: 0.635228, train acc: 0.725904, test acc: 0.728814\n",
      "1102, train loss: 0.613106, test loss: 0.635195, train acc: 0.725904, test acc: 0.728814\n",
      "1103, train loss: 0.61307, test loss: 0.635161, train acc: 0.725904, test acc: 0.728814\n",
      "1104, train loss: 0.613033, test loss: 0.635128, train acc: 0.725904, test acc: 0.728814\n",
      "1105, train loss: 0.612996, test loss: 0.635095, train acc: 0.725904, test acc: 0.728814\n",
      "1106, train loss: 0.61296, test loss: 0.635063, train acc: 0.725904, test acc: 0.728814\n",
      "1107, train loss: 0.612924, test loss: 0.63503, train acc: 0.725904, test acc: 0.728814\n",
      "1108, train loss: 0.612887, test loss: 0.634997, train acc: 0.725904, test acc: 0.728814\n",
      "1109, train loss: 0.612851, test loss: 0.634964, train acc: 0.725904, test acc: 0.728814\n",
      "1110, train loss: 0.612815, test loss: 0.634931, train acc: 0.725904, test acc: 0.728814\n",
      "1111, train loss: 0.612779, test loss: 0.634899, train acc: 0.725904, test acc: 0.728814\n",
      "1112, train loss: 0.612742, test loss: 0.634866, train acc: 0.725904, test acc: 0.728814\n",
      "1113, train loss: 0.612706, test loss: 0.634833, train acc: 0.725904, test acc: 0.728814\n",
      "1114, train loss: 0.61267, test loss: 0.634801, train acc: 0.725904, test acc: 0.728814\n",
      "1115, train loss: 0.612634, test loss: 0.634768, train acc: 0.725904, test acc: 0.728814\n",
      "1116, train loss: 0.612599, test loss: 0.634736, train acc: 0.725904, test acc: 0.728814\n",
      "1117, train loss: 0.612563, test loss: 0.634704, train acc: 0.725904, test acc: 0.728814\n",
      "1118, train loss: 0.612527, test loss: 0.634671, train acc: 0.725904, test acc: 0.728814\n",
      "1119, train loss: 0.612491, test loss: 0.634639, train acc: 0.725904, test acc: 0.728814\n",
      "1120, train loss: 0.612455, test loss: 0.634607, train acc: 0.725904, test acc: 0.728814\n",
      "1121, train loss: 0.61242, test loss: 0.634574, train acc: 0.725904, test acc: 0.728814\n",
      "1122, train loss: 0.612384, test loss: 0.634542, train acc: 0.725904, test acc: 0.728814\n",
      "1123, train loss: 0.612349, test loss: 0.63451, train acc: 0.725904, test acc: 0.728814\n",
      "1124, train loss: 0.612313, test loss: 0.634478, train acc: 0.725904, test acc: 0.728814\n",
      "1125, train loss: 0.612278, test loss: 0.634446, train acc: 0.725904, test acc: 0.728814\n",
      "1126, train loss: 0.612243, test loss: 0.634414, train acc: 0.725904, test acc: 0.728814\n",
      "1127, train loss: 0.612207, test loss: 0.634382, train acc: 0.725904, test acc: 0.728814\n",
      "1128, train loss: 0.612172, test loss: 0.63435, train acc: 0.725904, test acc: 0.728814\n",
      "1129, train loss: 0.612137, test loss: 0.634319, train acc: 0.725904, test acc: 0.728814\n",
      "1130, train loss: 0.612102, test loss: 0.634287, train acc: 0.725904, test acc: 0.728814\n",
      "1131, train loss: 0.612067, test loss: 0.634255, train acc: 0.725904, test acc: 0.728814\n",
      "1132, train loss: 0.612032, test loss: 0.634223, train acc: 0.725904, test acc: 0.728814\n",
      "1133, train loss: 0.611997, test loss: 0.634192, train acc: 0.725904, test acc: 0.728814\n",
      "1134, train loss: 0.611962, test loss: 0.63416, train acc: 0.725904, test acc: 0.728814\n",
      "1135, train loss: 0.611927, test loss: 0.634129, train acc: 0.725904, test acc: 0.728814\n",
      "1136, train loss: 0.611892, test loss: 0.634097, train acc: 0.725904, test acc: 0.728814\n",
      "1137, train loss: 0.611857, test loss: 0.634066, train acc: 0.725904, test acc: 0.728814\n",
      "1138, train loss: 0.611822, test loss: 0.634034, train acc: 0.725904, test acc: 0.728814\n",
      "1139, train loss: 0.611788, test loss: 0.634003, train acc: 0.725904, test acc: 0.728814\n",
      "1140, train loss: 0.611753, test loss: 0.633972, train acc: 0.725904, test acc: 0.728814\n",
      "1141, train loss: 0.611718, test loss: 0.63394, train acc: 0.725904, test acc: 0.728814\n",
      "1142, train loss: 0.611684, test loss: 0.633909, train acc: 0.725904, test acc: 0.728814\n",
      "1143, train loss: 0.611649, test loss: 0.633878, train acc: 0.725904, test acc: 0.728814\n",
      "1144, train loss: 0.611615, test loss: 0.633847, train acc: 0.725904, test acc: 0.728814\n",
      "1145, train loss: 0.611581, test loss: 0.633816, train acc: 0.725904, test acc: 0.728814\n",
      "1146, train loss: 0.611546, test loss: 0.633785, train acc: 0.725904, test acc: 0.728814\n",
      "1147, train loss: 0.611512, test loss: 0.633754, train acc: 0.725904, test acc: 0.728814\n",
      "1148, train loss: 0.611478, test loss: 0.633723, train acc: 0.725904, test acc: 0.728814\n",
      "1149, train loss: 0.611444, test loss: 0.633692, train acc: 0.725904, test acc: 0.728814\n",
      "1150, train loss: 0.611409, test loss: 0.633661, train acc: 0.725904, test acc: 0.728814\n",
      "1151, train loss: 0.611375, test loss: 0.63363, train acc: 0.725904, test acc: 0.728814\n",
      "1152, train loss: 0.611341, test loss: 0.6336, train acc: 0.725904, test acc: 0.728814\n",
      "1153, train loss: 0.611307, test loss: 0.633569, train acc: 0.725904, test acc: 0.728814\n",
      "1154, train loss: 0.611273, test loss: 0.633538, train acc: 0.725904, test acc: 0.728814\n",
      "1155, train loss: 0.61124, test loss: 0.633507, train acc: 0.725904, test acc: 0.728814\n",
      "1156, train loss: 0.611206, test loss: 0.633477, train acc: 0.725904, test acc: 0.728814\n",
      "1157, train loss: 0.611172, test loss: 0.633446, train acc: 0.725904, test acc: 0.728814\n",
      "1158, train loss: 0.611138, test loss: 0.633416, train acc: 0.725904, test acc: 0.728814\n",
      "1159, train loss: 0.611105, test loss: 0.633385, train acc: 0.725904, test acc: 0.728814\n",
      "1160, train loss: 0.611071, test loss: 0.633355, train acc: 0.725904, test acc: 0.728814\n",
      "1161, train loss: 0.611037, test loss: 0.633324, train acc: 0.725904, test acc: 0.728814\n",
      "1162, train loss: 0.611004, test loss: 0.633294, train acc: 0.725904, test acc: 0.728814\n",
      "1163, train loss: 0.61097, test loss: 0.633264, train acc: 0.725904, test acc: 0.728814\n",
      "1164, train loss: 0.610937, test loss: 0.633234, train acc: 0.725904, test acc: 0.728814\n",
      "1165, train loss: 0.610903, test loss: 0.633203, train acc: 0.725904, test acc: 0.728814\n",
      "1166, train loss: 0.61087, test loss: 0.633173, train acc: 0.725904, test acc: 0.728814\n",
      "1167, train loss: 0.610837, test loss: 0.633143, train acc: 0.725904, test acc: 0.728814\n",
      "1168, train loss: 0.610803, test loss: 0.633113, train acc: 0.725904, test acc: 0.728814\n",
      "1169, train loss: 0.61077, test loss: 0.633083, train acc: 0.728916, test acc: 0.728814\n",
      "1170, train loss: 0.610737, test loss: 0.633053, train acc: 0.728916, test acc: 0.728814\n",
      "1171, train loss: 0.610704, test loss: 0.633023, train acc: 0.728916, test acc: 0.728814\n",
      "1172, train loss: 0.610671, test loss: 0.632993, train acc: 0.728916, test acc: 0.728814\n",
      "1173, train loss: 0.610638, test loss: 0.632963, train acc: 0.728916, test acc: 0.728814\n",
      "1174, train loss: 0.610605, test loss: 0.632933, train acc: 0.728916, test acc: 0.728814\n",
      "1175, train loss: 0.610572, test loss: 0.632903, train acc: 0.728916, test acc: 0.728814\n",
      "1176, train loss: 0.610539, test loss: 0.632874, train acc: 0.728916, test acc: 0.728814\n",
      "1177, train loss: 0.610506, test loss: 0.632844, train acc: 0.728916, test acc: 0.728814\n",
      "1178, train loss: 0.610474, test loss: 0.632814, train acc: 0.728916, test acc: 0.728814\n",
      "1179, train loss: 0.610441, test loss: 0.632785, train acc: 0.728916, test acc: 0.728814\n",
      "1180, train loss: 0.610408, test loss: 0.632755, train acc: 0.728916, test acc: 0.728814\n",
      "1181, train loss: 0.610375, test loss: 0.632725, train acc: 0.728916, test acc: 0.728814\n",
      "1182, train loss: 0.610343, test loss: 0.632696, train acc: 0.728916, test acc: 0.728814\n",
      "1183, train loss: 0.61031, test loss: 0.632667, train acc: 0.728916, test acc: 0.728814\n",
      "1184, train loss: 0.610278, test loss: 0.632637, train acc: 0.728916, test acc: 0.728814\n",
      "1185, train loss: 0.610245, test loss: 0.632608, train acc: 0.728916, test acc: 0.728814\n",
      "1186, train loss: 0.610213, test loss: 0.632578, train acc: 0.728916, test acc: 0.728814\n",
      "1187, train loss: 0.61018, test loss: 0.632549, train acc: 0.728916, test acc: 0.728814\n",
      "1188, train loss: 0.610148, test loss: 0.63252, train acc: 0.728916, test acc: 0.728814\n",
      "1189, train loss: 0.610116, test loss: 0.63249, train acc: 0.728916, test acc: 0.728814\n",
      "1190, train loss: 0.610084, test loss: 0.632461, train acc: 0.728916, test acc: 0.728814\n",
      "1191, train loss: 0.610051, test loss: 0.632432, train acc: 0.728916, test acc: 0.728814\n",
      "1192, train loss: 0.610019, test loss: 0.632403, train acc: 0.728916, test acc: 0.728814\n",
      "1193, train loss: 0.609987, test loss: 0.632374, train acc: 0.728916, test acc: 0.728814\n",
      "1194, train loss: 0.609955, test loss: 0.632345, train acc: 0.728916, test acc: 0.728814\n",
      "1195, train loss: 0.609923, test loss: 0.632316, train acc: 0.728916, test acc: 0.728814\n",
      "1196, train loss: 0.609891, test loss: 0.632287, train acc: 0.728916, test acc: 0.728814\n",
      "1197, train loss: 0.609859, test loss: 0.632258, train acc: 0.728916, test acc: 0.728814\n",
      "1198, train loss: 0.609827, test loss: 0.632229, train acc: 0.728916, test acc: 0.728814\n",
      "1199, train loss: 0.609795, test loss: 0.6322, train acc: 0.728916, test acc: 0.728814\n",
      "1200, train loss: 0.609764, test loss: 0.632171, train acc: 0.728916, test acc: 0.728814\n",
      "1201, train loss: 0.609732, test loss: 0.632143, train acc: 0.728916, test acc: 0.728814\n",
      "1202, train loss: 0.6097, test loss: 0.632114, train acc: 0.728916, test acc: 0.728814\n",
      "1203, train loss: 0.609668, test loss: 0.632085, train acc: 0.728916, test acc: 0.728814\n",
      "1204, train loss: 0.609637, test loss: 0.632056, train acc: 0.728916, test acc: 0.728814\n",
      "1205, train loss: 0.609605, test loss: 0.632028, train acc: 0.728916, test acc: 0.728814\n",
      "1206, train loss: 0.609574, test loss: 0.631999, train acc: 0.728916, test acc: 0.728814\n",
      "1207, train loss: 0.609542, test loss: 0.631971, train acc: 0.728916, test acc: 0.728814\n",
      "1208, train loss: 0.609511, test loss: 0.631942, train acc: 0.728916, test acc: 0.728814\n",
      "1209, train loss: 0.609479, test loss: 0.631914, train acc: 0.728916, test acc: 0.728814\n",
      "1210, train loss: 0.609448, test loss: 0.631885, train acc: 0.728916, test acc: 0.728814\n",
      "1211, train loss: 0.609416, test loss: 0.631857, train acc: 0.728916, test acc: 0.728814\n",
      "1212, train loss: 0.609385, test loss: 0.631828, train acc: 0.728916, test acc: 0.728814\n",
      "1213, train loss: 0.609354, test loss: 0.6318, train acc: 0.728916, test acc: 0.728814\n",
      "1214, train loss: 0.609323, test loss: 0.631772, train acc: 0.728916, test acc: 0.728814\n",
      "1215, train loss: 0.609292, test loss: 0.631744, train acc: 0.728916, test acc: 0.728814\n",
      "1216, train loss: 0.60926, test loss: 0.631715, train acc: 0.728916, test acc: 0.728814\n",
      "1217, train loss: 0.609229, test loss: 0.631687, train acc: 0.728916, test acc: 0.728814\n",
      "1218, train loss: 0.609198, test loss: 0.631659, train acc: 0.728916, test acc: 0.728814\n",
      "1219, train loss: 0.609167, test loss: 0.631631, train acc: 0.728916, test acc: 0.728814\n",
      "1220, train loss: 0.609136, test loss: 0.631603, train acc: 0.728916, test acc: 0.728814\n",
      "1221, train loss: 0.609105, test loss: 0.631575, train acc: 0.728916, test acc: 0.728814\n",
      "1222, train loss: 0.609075, test loss: 0.631547, train acc: 0.728916, test acc: 0.728814\n",
      "1223, train loss: 0.609044, test loss: 0.631519, train acc: 0.728916, test acc: 0.728814\n",
      "1224, train loss: 0.609013, test loss: 0.631491, train acc: 0.728916, test acc: 0.728814\n",
      "1225, train loss: 0.608982, test loss: 0.631463, train acc: 0.728916, test acc: 0.728814\n",
      "1226, train loss: 0.608951, test loss: 0.631435, train acc: 0.728916, test acc: 0.728814\n",
      "1227, train loss: 0.608921, test loss: 0.631407, train acc: 0.728916, test acc: 0.728814\n",
      "1228, train loss: 0.60889, test loss: 0.631379, train acc: 0.728916, test acc: 0.728814\n",
      "1229, train loss: 0.60886, test loss: 0.631352, train acc: 0.728916, test acc: 0.728814\n",
      "1230, train loss: 0.608829, test loss: 0.631324, train acc: 0.728916, test acc: 0.728814\n",
      "1231, train loss: 0.608799, test loss: 0.631296, train acc: 0.728916, test acc: 0.728814\n",
      "1232, train loss: 0.608768, test loss: 0.631269, train acc: 0.728916, test acc: 0.728814\n",
      "1233, train loss: 0.608738, test loss: 0.631241, train acc: 0.728916, test acc: 0.728814\n",
      "1234, train loss: 0.608707, test loss: 0.631213, train acc: 0.728916, test acc: 0.728814\n",
      "1235, train loss: 0.608677, test loss: 0.631186, train acc: 0.728916, test acc: 0.728814\n",
      "1236, train loss: 0.608647, test loss: 0.631158, train acc: 0.728916, test acc: 0.728814\n",
      "1237, train loss: 0.608616, test loss: 0.631131, train acc: 0.728916, test acc: 0.728814\n",
      "1238, train loss: 0.608586, test loss: 0.631103, train acc: 0.728916, test acc: 0.728814\n",
      "1239, train loss: 0.608556, test loss: 0.631076, train acc: 0.728916, test acc: 0.728814\n",
      "1240, train loss: 0.608526, test loss: 0.631049, train acc: 0.728916, test acc: 0.728814\n",
      "1241, train loss: 0.608496, test loss: 0.631021, train acc: 0.728916, test acc: 0.728814\n",
      "1242, train loss: 0.608465, test loss: 0.630994, train acc: 0.728916, test acc: 0.728814\n",
      "1243, train loss: 0.608435, test loss: 0.630967, train acc: 0.728916, test acc: 0.728814\n",
      "1244, train loss: 0.608405, test loss: 0.630939, train acc: 0.728916, test acc: 0.728814\n",
      "1245, train loss: 0.608375, test loss: 0.630912, train acc: 0.728916, test acc: 0.728814\n",
      "1246, train loss: 0.608346, test loss: 0.630885, train acc: 0.728916, test acc: 0.728814\n",
      "1247, train loss: 0.608316, test loss: 0.630858, train acc: 0.728916, test acc: 0.728814\n",
      "1248, train loss: 0.608286, test loss: 0.630831, train acc: 0.728916, test acc: 0.728814\n",
      "1249, train loss: 0.608256, test loss: 0.630804, train acc: 0.728916, test acc: 0.728814\n",
      "1250, train loss: 0.608226, test loss: 0.630777, train acc: 0.728916, test acc: 0.728814\n",
      "1251, train loss: 0.608196, test loss: 0.63075, train acc: 0.728916, test acc: 0.728814\n",
      "1252, train loss: 0.608167, test loss: 0.630723, train acc: 0.728916, test acc: 0.728814\n",
      "1253, train loss: 0.608137, test loss: 0.630696, train acc: 0.728916, test acc: 0.728814\n",
      "1254, train loss: 0.608108, test loss: 0.630669, train acc: 0.728916, test acc: 0.728814\n",
      "1255, train loss: 0.608078, test loss: 0.630642, train acc: 0.728916, test acc: 0.728814\n",
      "1256, train loss: 0.608048, test loss: 0.630615, train acc: 0.728916, test acc: 0.728814\n",
      "1257, train loss: 0.608019, test loss: 0.630588, train acc: 0.728916, test acc: 0.728814\n",
      "1258, train loss: 0.607989, test loss: 0.630561, train acc: 0.728916, test acc: 0.728814\n",
      "1259, train loss: 0.60796, test loss: 0.630535, train acc: 0.728916, test acc: 0.728814\n",
      "1260, train loss: 0.607931, test loss: 0.630508, train acc: 0.728916, test acc: 0.728814\n",
      "1261, train loss: 0.607901, test loss: 0.630481, train acc: 0.725904, test acc: 0.728814\n",
      "1262, train loss: 0.607872, test loss: 0.630454, train acc: 0.725904, test acc: 0.728814\n",
      "1263, train loss: 0.607843, test loss: 0.630428, train acc: 0.725904, test acc: 0.728814\n",
      "1264, train loss: 0.607813, test loss: 0.630401, train acc: 0.725904, test acc: 0.728814\n",
      "1265, train loss: 0.607784, test loss: 0.630375, train acc: 0.725904, test acc: 0.728814\n",
      "1266, train loss: 0.607755, test loss: 0.630348, train acc: 0.725904, test acc: 0.728814\n",
      "1267, train loss: 0.607726, test loss: 0.630322, train acc: 0.725904, test acc: 0.728814\n",
      "1268, train loss: 0.607697, test loss: 0.630295, train acc: 0.725904, test acc: 0.728814\n",
      "1269, train loss: 0.607668, test loss: 0.630269, train acc: 0.725904, test acc: 0.728814\n",
      "1270, train loss: 0.607639, test loss: 0.630242, train acc: 0.725904, test acc: 0.728814\n",
      "1271, train loss: 0.60761, test loss: 0.630216, train acc: 0.725904, test acc: 0.728814\n",
      "1272, train loss: 0.607581, test loss: 0.63019, train acc: 0.725904, test acc: 0.728814\n",
      "1273, train loss: 0.607552, test loss: 0.630163, train acc: 0.725904, test acc: 0.728814\n",
      "1274, train loss: 0.607523, test loss: 0.630137, train acc: 0.725904, test acc: 0.728814\n",
      "1275, train loss: 0.607494, test loss: 0.630111, train acc: 0.725904, test acc: 0.728814\n",
      "1276, train loss: 0.607465, test loss: 0.630085, train acc: 0.725904, test acc: 0.728814\n",
      "1277, train loss: 0.607436, test loss: 0.630058, train acc: 0.725904, test acc: 0.728814\n",
      "1278, train loss: 0.607408, test loss: 0.630032, train acc: 0.725904, test acc: 0.728814\n",
      "1279, train loss: 0.607379, test loss: 0.630006, train acc: 0.725904, test acc: 0.728814\n",
      "1280, train loss: 0.60735, test loss: 0.62998, train acc: 0.725904, test acc: 0.728814\n",
      "1281, train loss: 0.607322, test loss: 0.629954, train acc: 0.725904, test acc: 0.728814\n",
      "1282, train loss: 0.607293, test loss: 0.629928, train acc: 0.725904, test acc: 0.728814\n",
      "1283, train loss: 0.607265, test loss: 0.629902, train acc: 0.725904, test acc: 0.728814\n",
      "1284, train loss: 0.607236, test loss: 0.629876, train acc: 0.725904, test acc: 0.728814\n",
      "1285, train loss: 0.607208, test loss: 0.62985, train acc: 0.725904, test acc: 0.728814\n",
      "1286, train loss: 0.607179, test loss: 0.629824, train acc: 0.725904, test acc: 0.728814\n",
      "1287, train loss: 0.607151, test loss: 0.629798, train acc: 0.725904, test acc: 0.728814\n",
      "1288, train loss: 0.607122, test loss: 0.629772, train acc: 0.725904, test acc: 0.728814\n",
      "1289, train loss: 0.607094, test loss: 0.629746, train acc: 0.725904, test acc: 0.728814\n",
      "1290, train loss: 0.607066, test loss: 0.629721, train acc: 0.725904, test acc: 0.728814\n",
      "1291, train loss: 0.607037, test loss: 0.629695, train acc: 0.725904, test acc: 0.728814\n",
      "1292, train loss: 0.607009, test loss: 0.629669, train acc: 0.725904, test acc: 0.728814\n",
      "1293, train loss: 0.606981, test loss: 0.629643, train acc: 0.722892, test acc: 0.728814\n",
      "1294, train loss: 0.606953, test loss: 0.629618, train acc: 0.722892, test acc: 0.728814\n",
      "1295, train loss: 0.606924, test loss: 0.629592, train acc: 0.722892, test acc: 0.728814\n",
      "1296, train loss: 0.606896, test loss: 0.629566, train acc: 0.722892, test acc: 0.728814\n",
      "1297, train loss: 0.606868, test loss: 0.629541, train acc: 0.722892, test acc: 0.728814\n",
      "1298, train loss: 0.60684, test loss: 0.629515, train acc: 0.722892, test acc: 0.728814\n",
      "1299, train loss: 0.606812, test loss: 0.62949, train acc: 0.722892, test acc: 0.728814\n",
      "1300, train loss: 0.606784, test loss: 0.629464, train acc: 0.722892, test acc: 0.728814\n",
      "1301, train loss: 0.606756, test loss: 0.629439, train acc: 0.722892, test acc: 0.728814\n",
      "1302, train loss: 0.606728, test loss: 0.629413, train acc: 0.722892, test acc: 0.728814\n",
      "1303, train loss: 0.606701, test loss: 0.629388, train acc: 0.722892, test acc: 0.728814\n",
      "1304, train loss: 0.606673, test loss: 0.629362, train acc: 0.722892, test acc: 0.728814\n",
      "1305, train loss: 0.606645, test loss: 0.629337, train acc: 0.722892, test acc: 0.728814\n",
      "1306, train loss: 0.606617, test loss: 0.629312, train acc: 0.722892, test acc: 0.728814\n",
      "1307, train loss: 0.606589, test loss: 0.629286, train acc: 0.722892, test acc: 0.728814\n",
      "1308, train loss: 0.606562, test loss: 0.629261, train acc: 0.722892, test acc: 0.728814\n",
      "1309, train loss: 0.606534, test loss: 0.629236, train acc: 0.722892, test acc: 0.728814\n",
      "1310, train loss: 0.606506, test loss: 0.62921, train acc: 0.722892, test acc: 0.728814\n",
      "1311, train loss: 0.606479, test loss: 0.629185, train acc: 0.722892, test acc: 0.728814\n",
      "1312, train loss: 0.606451, test loss: 0.62916, train acc: 0.722892, test acc: 0.728814\n",
      "1313, train loss: 0.606424, test loss: 0.629135, train acc: 0.722892, test acc: 0.728814\n",
      "1314, train loss: 0.606396, test loss: 0.62911, train acc: 0.722892, test acc: 0.728814\n",
      "1315, train loss: 0.606369, test loss: 0.629085, train acc: 0.722892, test acc: 0.728814\n",
      "1316, train loss: 0.606341, test loss: 0.62906, train acc: 0.722892, test acc: 0.728814\n",
      "1317, train loss: 0.606314, test loss: 0.629035, train acc: 0.722892, test acc: 0.728814\n",
      "1318, train loss: 0.606286, test loss: 0.62901, train acc: 0.722892, test acc: 0.728814\n",
      "1319, train loss: 0.606259, test loss: 0.628985, train acc: 0.722892, test acc: 0.728814\n",
      "1320, train loss: 0.606232, test loss: 0.62896, train acc: 0.722892, test acc: 0.728814\n",
      "1321, train loss: 0.606204, test loss: 0.628935, train acc: 0.722892, test acc: 0.728814\n",
      "1322, train loss: 0.606177, test loss: 0.62891, train acc: 0.722892, test acc: 0.728814\n",
      "1323, train loss: 0.60615, test loss: 0.628885, train acc: 0.722892, test acc: 0.728814\n",
      "1324, train loss: 0.606123, test loss: 0.62886, train acc: 0.722892, test acc: 0.728814\n",
      "1325, train loss: 0.606095, test loss: 0.628835, train acc: 0.722892, test acc: 0.728814\n",
      "1326, train loss: 0.606068, test loss: 0.62881, train acc: 0.722892, test acc: 0.728814\n",
      "1327, train loss: 0.606041, test loss: 0.628786, train acc: 0.722892, test acc: 0.728814\n",
      "1328, train loss: 0.606014, test loss: 0.628761, train acc: 0.722892, test acc: 0.728814\n",
      "1329, train loss: 0.605987, test loss: 0.628736, train acc: 0.722892, test acc: 0.728814\n",
      "1330, train loss: 0.60596, test loss: 0.628712, train acc: 0.722892, test acc: 0.728814\n",
      "1331, train loss: 0.605933, test loss: 0.628687, train acc: 0.722892, test acc: 0.728814\n",
      "1332, train loss: 0.605906, test loss: 0.628662, train acc: 0.722892, test acc: 0.728814\n",
      "1333, train loss: 0.605879, test loss: 0.628638, train acc: 0.722892, test acc: 0.728814\n",
      "1334, train loss: 0.605852, test loss: 0.628613, train acc: 0.722892, test acc: 0.728814\n",
      "1335, train loss: 0.605826, test loss: 0.628589, train acc: 0.722892, test acc: 0.728814\n",
      "1336, train loss: 0.605799, test loss: 0.628564, train acc: 0.722892, test acc: 0.728814\n",
      "1337, train loss: 0.605772, test loss: 0.62854, train acc: 0.722892, test acc: 0.728814\n",
      "1338, train loss: 0.605745, test loss: 0.628515, train acc: 0.722892, test acc: 0.728814\n",
      "1339, train loss: 0.605719, test loss: 0.628491, train acc: 0.71988, test acc: 0.728814\n",
      "1340, train loss: 0.605692, test loss: 0.628466, train acc: 0.71988, test acc: 0.728814\n",
      "1341, train loss: 0.605665, test loss: 0.628442, train acc: 0.71988, test acc: 0.728814\n",
      "1342, train loss: 0.605639, test loss: 0.628417, train acc: 0.71988, test acc: 0.728814\n",
      "1343, train loss: 0.605612, test loss: 0.628393, train acc: 0.71988, test acc: 0.711864\n",
      "1344, train loss: 0.605585, test loss: 0.628369, train acc: 0.71988, test acc: 0.711864\n",
      "1345, train loss: 0.605559, test loss: 0.628344, train acc: 0.71988, test acc: 0.711864\n",
      "1346, train loss: 0.605532, test loss: 0.62832, train acc: 0.71988, test acc: 0.711864\n",
      "1347, train loss: 0.605506, test loss: 0.628296, train acc: 0.71988, test acc: 0.711864\n",
      "1348, train loss: 0.605479, test loss: 0.628272, train acc: 0.71988, test acc: 0.711864\n",
      "1349, train loss: 0.605453, test loss: 0.628247, train acc: 0.71988, test acc: 0.711864\n",
      "1350, train loss: 0.605427, test loss: 0.628223, train acc: 0.71988, test acc: 0.711864\n",
      "1351, train loss: 0.6054, test loss: 0.628199, train acc: 0.71988, test acc: 0.711864\n",
      "1352, train loss: 0.605374, test loss: 0.628175, train acc: 0.71988, test acc: 0.711864\n",
      "1353, train loss: 0.605348, test loss: 0.628151, train acc: 0.71988, test acc: 0.711864\n",
      "1354, train loss: 0.605321, test loss: 0.628127, train acc: 0.71988, test acc: 0.711864\n",
      "1355, train loss: 0.605295, test loss: 0.628103, train acc: 0.71988, test acc: 0.711864\n",
      "1356, train loss: 0.605269, test loss: 0.628079, train acc: 0.716867, test acc: 0.711864\n",
      "1357, train loss: 0.605243, test loss: 0.628055, train acc: 0.716867, test acc: 0.711864\n",
      "1358, train loss: 0.605217, test loss: 0.628031, train acc: 0.716867, test acc: 0.711864\n",
      "1359, train loss: 0.60519, test loss: 0.628007, train acc: 0.716867, test acc: 0.711864\n",
      "1360, train loss: 0.605164, test loss: 0.627983, train acc: 0.716867, test acc: 0.711864\n",
      "1361, train loss: 0.605138, test loss: 0.627959, train acc: 0.716867, test acc: 0.711864\n",
      "1362, train loss: 0.605112, test loss: 0.627935, train acc: 0.716867, test acc: 0.711864\n",
      "1363, train loss: 0.605086, test loss: 0.627911, train acc: 0.716867, test acc: 0.711864\n",
      "1364, train loss: 0.60506, test loss: 0.627887, train acc: 0.716867, test acc: 0.711864\n",
      "1365, train loss: 0.605034, test loss: 0.627864, train acc: 0.716867, test acc: 0.711864\n",
      "1366, train loss: 0.605008, test loss: 0.62784, train acc: 0.716867, test acc: 0.711864\n",
      "1367, train loss: 0.604982, test loss: 0.627816, train acc: 0.716867, test acc: 0.711864\n",
      "1368, train loss: 0.604957, test loss: 0.627792, train acc: 0.716867, test acc: 0.711864\n",
      "1369, train loss: 0.604931, test loss: 0.627769, train acc: 0.716867, test acc: 0.711864\n",
      "1370, train loss: 0.604905, test loss: 0.627745, train acc: 0.716867, test acc: 0.711864\n",
      "1371, train loss: 0.604879, test loss: 0.627721, train acc: 0.716867, test acc: 0.711864\n",
      "1372, train loss: 0.604853, test loss: 0.627698, train acc: 0.716867, test acc: 0.711864\n",
      "1373, train loss: 0.604828, test loss: 0.627674, train acc: 0.716867, test acc: 0.711864\n",
      "1374, train loss: 0.604802, test loss: 0.62765, train acc: 0.716867, test acc: 0.711864\n",
      "1375, train loss: 0.604776, test loss: 0.627627, train acc: 0.716867, test acc: 0.711864\n",
      "1376, train loss: 0.604751, test loss: 0.627603, train acc: 0.716867, test acc: 0.711864\n",
      "1377, train loss: 0.604725, test loss: 0.62758, train acc: 0.716867, test acc: 0.711864\n",
      "1378, train loss: 0.604699, test loss: 0.627556, train acc: 0.716867, test acc: 0.711864\n",
      "1379, train loss: 0.604674, test loss: 0.627533, train acc: 0.716867, test acc: 0.711864\n",
      "1380, train loss: 0.604648, test loss: 0.627509, train acc: 0.716867, test acc: 0.711864\n",
      "1381, train loss: 0.604623, test loss: 0.627486, train acc: 0.716867, test acc: 0.711864\n",
      "1382, train loss: 0.604597, test loss: 0.627462, train acc: 0.716867, test acc: 0.711864\n",
      "1383, train loss: 0.604572, test loss: 0.627439, train acc: 0.716867, test acc: 0.711864\n",
      "1384, train loss: 0.604546, test loss: 0.627416, train acc: 0.716867, test acc: 0.711864\n",
      "1385, train loss: 0.604521, test loss: 0.627392, train acc: 0.716867, test acc: 0.711864\n",
      "1386, train loss: 0.604496, test loss: 0.627369, train acc: 0.716867, test acc: 0.711864\n",
      "1387, train loss: 0.60447, test loss: 0.627346, train acc: 0.716867, test acc: 0.711864\n",
      "1388, train loss: 0.604445, test loss: 0.627322, train acc: 0.716867, test acc: 0.711864\n",
      "1389, train loss: 0.60442, test loss: 0.627299, train acc: 0.716867, test acc: 0.711864\n",
      "1390, train loss: 0.604395, test loss: 0.627276, train acc: 0.716867, test acc: 0.711864\n",
      "1391, train loss: 0.604369, test loss: 0.627253, train acc: 0.716867, test acc: 0.711864\n",
      "1392, train loss: 0.604344, test loss: 0.62723, train acc: 0.716867, test acc: 0.711864\n",
      "1393, train loss: 0.604319, test loss: 0.627206, train acc: 0.716867, test acc: 0.711864\n",
      "1394, train loss: 0.604294, test loss: 0.627183, train acc: 0.716867, test acc: 0.711864\n",
      "1395, train loss: 0.604269, test loss: 0.62716, train acc: 0.716867, test acc: 0.711864\n",
      "1396, train loss: 0.604244, test loss: 0.627137, train acc: 0.716867, test acc: 0.711864\n",
      "1397, train loss: 0.604219, test loss: 0.627114, train acc: 0.716867, test acc: 0.711864\n",
      "1398, train loss: 0.604194, test loss: 0.627091, train acc: 0.716867, test acc: 0.711864\n",
      "1399, train loss: 0.604168, test loss: 0.627068, train acc: 0.716867, test acc: 0.711864\n",
      "1400, train loss: 0.604143, test loss: 0.627045, train acc: 0.716867, test acc: 0.711864\n",
      "1401, train loss: 0.604119, test loss: 0.627022, train acc: 0.716867, test acc: 0.711864\n",
      "1402, train loss: 0.604094, test loss: 0.626999, train acc: 0.716867, test acc: 0.711864\n",
      "1403, train loss: 0.604069, test loss: 0.626976, train acc: 0.716867, test acc: 0.711864\n",
      "1404, train loss: 0.604044, test loss: 0.626953, train acc: 0.716867, test acc: 0.711864\n",
      "1405, train loss: 0.604019, test loss: 0.62693, train acc: 0.716867, test acc: 0.711864\n",
      "1406, train loss: 0.603994, test loss: 0.626907, train acc: 0.716867, test acc: 0.711864\n",
      "1407, train loss: 0.603969, test loss: 0.626884, train acc: 0.716867, test acc: 0.711864\n",
      "1408, train loss: 0.603945, test loss: 0.626862, train acc: 0.716867, test acc: 0.711864\n",
      "1409, train loss: 0.60392, test loss: 0.626839, train acc: 0.716867, test acc: 0.711864\n",
      "1410, train loss: 0.603895, test loss: 0.626816, train acc: 0.716867, test acc: 0.711864\n",
      "1411, train loss: 0.60387, test loss: 0.626793, train acc: 0.716867, test acc: 0.711864\n",
      "1412, train loss: 0.603846, test loss: 0.626771, train acc: 0.716867, test acc: 0.711864\n",
      "1413, train loss: 0.603821, test loss: 0.626748, train acc: 0.716867, test acc: 0.711864\n",
      "1414, train loss: 0.603796, test loss: 0.626725, train acc: 0.716867, test acc: 0.711864\n",
      "1415, train loss: 0.603772, test loss: 0.626702, train acc: 0.716867, test acc: 0.711864\n",
      "1416, train loss: 0.603747, test loss: 0.62668, train acc: 0.716867, test acc: 0.711864\n",
      "1417, train loss: 0.603723, test loss: 0.626657, train acc: 0.716867, test acc: 0.711864\n",
      "1418, train loss: 0.603698, test loss: 0.626634, train acc: 0.716867, test acc: 0.711864\n",
      "1419, train loss: 0.603674, test loss: 0.626612, train acc: 0.716867, test acc: 0.711864\n",
      "1420, train loss: 0.603649, test loss: 0.626589, train acc: 0.716867, test acc: 0.711864\n",
      "1421, train loss: 0.603625, test loss: 0.626567, train acc: 0.716867, test acc: 0.711864\n",
      "1422, train loss: 0.6036, test loss: 0.626544, train acc: 0.716867, test acc: 0.711864\n",
      "1423, train loss: 0.603576, test loss: 0.626522, train acc: 0.716867, test acc: 0.711864\n",
      "1424, train loss: 0.603552, test loss: 0.626499, train acc: 0.716867, test acc: 0.711864\n",
      "1425, train loss: 0.603527, test loss: 0.626477, train acc: 0.716867, test acc: 0.711864\n",
      "1426, train loss: 0.603503, test loss: 0.626454, train acc: 0.716867, test acc: 0.711864\n",
      "1427, train loss: 0.603479, test loss: 0.626432, train acc: 0.716867, test acc: 0.711864\n",
      "1428, train loss: 0.603454, test loss: 0.626409, train acc: 0.716867, test acc: 0.711864\n",
      "1429, train loss: 0.60343, test loss: 0.626387, train acc: 0.716867, test acc: 0.711864\n",
      "1430, train loss: 0.603406, test loss: 0.626365, train acc: 0.716867, test acc: 0.711864\n",
      "1431, train loss: 0.603382, test loss: 0.626342, train acc: 0.716867, test acc: 0.711864\n",
      "1432, train loss: 0.603358, test loss: 0.62632, train acc: 0.716867, test acc: 0.711864\n",
      "1433, train loss: 0.603333, test loss: 0.626298, train acc: 0.716867, test acc: 0.711864\n",
      "1434, train loss: 0.603309, test loss: 0.626275, train acc: 0.716867, test acc: 0.711864\n",
      "1435, train loss: 0.603285, test loss: 0.626253, train acc: 0.716867, test acc: 0.711864\n",
      "1436, train loss: 0.603261, test loss: 0.626231, train acc: 0.716867, test acc: 0.711864\n",
      "1437, train loss: 0.603237, test loss: 0.626209, train acc: 0.716867, test acc: 0.711864\n",
      "1438, train loss: 0.603213, test loss: 0.626186, train acc: 0.716867, test acc: 0.711864\n",
      "1439, train loss: 0.603189, test loss: 0.626164, train acc: 0.716867, test acc: 0.711864\n",
      "1440, train loss: 0.603165, test loss: 0.626142, train acc: 0.716867, test acc: 0.711864\n",
      "1441, train loss: 0.603141, test loss: 0.62612, train acc: 0.716867, test acc: 0.711864\n",
      "1442, train loss: 0.603117, test loss: 0.626098, train acc: 0.716867, test acc: 0.711864\n",
      "1443, train loss: 0.603093, test loss: 0.626076, train acc: 0.716867, test acc: 0.711864\n",
      "1444, train loss: 0.603069, test loss: 0.626054, train acc: 0.716867, test acc: 0.711864\n",
      "1445, train loss: 0.603046, test loss: 0.626032, train acc: 0.716867, test acc: 0.711864\n",
      "1446, train loss: 0.603022, test loss: 0.626009, train acc: 0.716867, test acc: 0.711864\n",
      "1447, train loss: 0.602998, test loss: 0.625987, train acc: 0.716867, test acc: 0.711864\n",
      "1448, train loss: 0.602974, test loss: 0.625965, train acc: 0.716867, test acc: 0.711864\n",
      "1449, train loss: 0.602951, test loss: 0.625943, train acc: 0.716867, test acc: 0.711864\n",
      "1450, train loss: 0.602927, test loss: 0.625921, train acc: 0.716867, test acc: 0.711864\n",
      "1451, train loss: 0.602903, test loss: 0.625899, train acc: 0.716867, test acc: 0.711864\n",
      "1452, train loss: 0.602879, test loss: 0.625877, train acc: 0.716867, test acc: 0.711864\n",
      "1453, train loss: 0.602856, test loss: 0.625856, train acc: 0.716867, test acc: 0.711864\n",
      "1454, train loss: 0.602832, test loss: 0.625834, train acc: 0.716867, test acc: 0.711864\n",
      "1455, train loss: 0.602808, test loss: 0.625812, train acc: 0.716867, test acc: 0.711864\n",
      "1456, train loss: 0.602785, test loss: 0.62579, train acc: 0.716867, test acc: 0.711864\n",
      "1457, train loss: 0.602761, test loss: 0.625768, train acc: 0.716867, test acc: 0.711864\n",
      "1458, train loss: 0.602738, test loss: 0.625746, train acc: 0.716867, test acc: 0.711864\n",
      "1459, train loss: 0.602714, test loss: 0.625725, train acc: 0.716867, test acc: 0.711864\n",
      "1460, train loss: 0.602691, test loss: 0.625703, train acc: 0.716867, test acc: 0.711864\n",
      "1461, train loss: 0.602667, test loss: 0.625681, train acc: 0.716867, test acc: 0.711864\n",
      "1462, train loss: 0.602644, test loss: 0.625659, train acc: 0.716867, test acc: 0.711864\n",
      "1463, train loss: 0.60262, test loss: 0.625638, train acc: 0.716867, test acc: 0.711864\n",
      "1464, train loss: 0.602597, test loss: 0.625616, train acc: 0.716867, test acc: 0.711864\n",
      "1465, train loss: 0.602574, test loss: 0.625594, train acc: 0.716867, test acc: 0.711864\n",
      "1466, train loss: 0.60255, test loss: 0.625572, train acc: 0.716867, test acc: 0.711864\n",
      "1467, train loss: 0.602527, test loss: 0.625551, train acc: 0.716867, test acc: 0.711864\n",
      "1468, train loss: 0.602504, test loss: 0.625529, train acc: 0.716867, test acc: 0.711864\n",
      "1469, train loss: 0.60248, test loss: 0.625508, train acc: 0.716867, test acc: 0.711864\n",
      "1470, train loss: 0.602457, test loss: 0.625486, train acc: 0.716867, test acc: 0.711864\n",
      "1471, train loss: 0.602434, test loss: 0.625464, train acc: 0.716867, test acc: 0.711864\n",
      "1472, train loss: 0.602411, test loss: 0.625443, train acc: 0.716867, test acc: 0.711864\n",
      "1473, train loss: 0.602387, test loss: 0.625421, train acc: 0.716867, test acc: 0.711864\n",
      "1474, train loss: 0.602364, test loss: 0.6254, train acc: 0.716867, test acc: 0.711864\n",
      "1475, train loss: 0.602341, test loss: 0.625378, train acc: 0.716867, test acc: 0.711864\n",
      "1476, train loss: 0.602318, test loss: 0.625357, train acc: 0.716867, test acc: 0.711864\n",
      "1477, train loss: 0.602295, test loss: 0.625335, train acc: 0.716867, test acc: 0.711864\n",
      "1478, train loss: 0.602272, test loss: 0.625314, train acc: 0.716867, test acc: 0.711864\n",
      "1479, train loss: 0.602249, test loss: 0.625292, train acc: 0.716867, test acc: 0.711864\n",
      "1480, train loss: 0.602226, test loss: 0.625271, train acc: 0.716867, test acc: 0.711864\n",
      "1481, train loss: 0.602203, test loss: 0.625249, train acc: 0.716867, test acc: 0.711864\n",
      "1482, train loss: 0.60218, test loss: 0.625228, train acc: 0.716867, test acc: 0.728814\n",
      "1483, train loss: 0.602157, test loss: 0.625207, train acc: 0.716867, test acc: 0.728814\n",
      "1484, train loss: 0.602134, test loss: 0.625185, train acc: 0.71988, test acc: 0.728814\n",
      "1485, train loss: 0.602111, test loss: 0.625164, train acc: 0.71988, test acc: 0.728814\n",
      "1486, train loss: 0.602088, test loss: 0.625143, train acc: 0.71988, test acc: 0.728814\n",
      "1487, train loss: 0.602065, test loss: 0.625121, train acc: 0.71988, test acc: 0.728814\n",
      "1488, train loss: 0.602042, test loss: 0.6251, train acc: 0.71988, test acc: 0.728814\n",
      "1489, train loss: 0.602019, test loss: 0.625079, train acc: 0.71988, test acc: 0.728814\n",
      "1490, train loss: 0.601996, test loss: 0.625058, train acc: 0.71988, test acc: 0.728814\n",
      "1491, train loss: 0.601974, test loss: 0.625036, train acc: 0.71988, test acc: 0.728814\n",
      "1492, train loss: 0.601951, test loss: 0.625015, train acc: 0.71988, test acc: 0.728814\n",
      "1493, train loss: 0.601928, test loss: 0.624994, train acc: 0.71988, test acc: 0.728814\n",
      "1494, train loss: 0.601905, test loss: 0.624973, train acc: 0.71988, test acc: 0.728814\n",
      "1495, train loss: 0.601883, test loss: 0.624952, train acc: 0.71988, test acc: 0.728814\n",
      "1496, train loss: 0.60186, test loss: 0.624931, train acc: 0.71988, test acc: 0.728814\n",
      "1497, train loss: 0.601837, test loss: 0.624909, train acc: 0.71988, test acc: 0.728814\n",
      "1498, train loss: 0.601815, test loss: 0.624888, train acc: 0.71988, test acc: 0.728814\n",
      "1499, train loss: 0.601792, test loss: 0.624867, train acc: 0.71988, test acc: 0.728814\n",
      "1500, train loss: 0.601769, test loss: 0.624846, train acc: 0.71988, test acc: 0.728814\n",
      "1501, train loss: 0.601747, test loss: 0.624825, train acc: 0.71988, test acc: 0.728814\n",
      "1502, train loss: 0.601724, test loss: 0.624804, train acc: 0.71988, test acc: 0.728814\n",
      "1503, train loss: 0.601702, test loss: 0.624783, train acc: 0.71988, test acc: 0.728814\n",
      "1504, train loss: 0.601679, test loss: 0.624762, train acc: 0.71988, test acc: 0.728814\n",
      "1505, train loss: 0.601657, test loss: 0.624741, train acc: 0.71988, test acc: 0.728814\n",
      "1506, train loss: 0.601634, test loss: 0.62472, train acc: 0.71988, test acc: 0.728814\n",
      "1507, train loss: 0.601612, test loss: 0.624699, train acc: 0.71988, test acc: 0.728814\n",
      "1508, train loss: 0.601589, test loss: 0.624678, train acc: 0.71988, test acc: 0.728814\n",
      "1509, train loss: 0.601567, test loss: 0.624657, train acc: 0.71988, test acc: 0.728814\n",
      "1510, train loss: 0.601544, test loss: 0.624636, train acc: 0.71988, test acc: 0.728814\n",
      "1511, train loss: 0.601522, test loss: 0.624616, train acc: 0.722892, test acc: 0.728814\n",
      "1512, train loss: 0.6015, test loss: 0.624595, train acc: 0.722892, test acc: 0.728814\n",
      "1513, train loss: 0.601477, test loss: 0.624574, train acc: 0.722892, test acc: 0.728814\n",
      "1514, train loss: 0.601455, test loss: 0.624553, train acc: 0.722892, test acc: 0.728814\n",
      "1515, train loss: 0.601433, test loss: 0.624532, train acc: 0.722892, test acc: 0.728814\n",
      "1516, train loss: 0.60141, test loss: 0.624511, train acc: 0.722892, test acc: 0.728814\n",
      "1517, train loss: 0.601388, test loss: 0.624491, train acc: 0.722892, test acc: 0.728814\n",
      "1518, train loss: 0.601366, test loss: 0.62447, train acc: 0.722892, test acc: 0.728814\n",
      "1519, train loss: 0.601344, test loss: 0.624449, train acc: 0.722892, test acc: 0.728814\n",
      "1520, train loss: 0.601321, test loss: 0.624428, train acc: 0.722892, test acc: 0.728814\n",
      "1521, train loss: 0.601299, test loss: 0.624408, train acc: 0.722892, test acc: 0.728814\n",
      "1522, train loss: 0.601277, test loss: 0.624387, train acc: 0.722892, test acc: 0.728814\n",
      "1523, train loss: 0.601255, test loss: 0.624366, train acc: 0.722892, test acc: 0.728814\n",
      "1524, train loss: 0.601233, test loss: 0.624346, train acc: 0.722892, test acc: 0.728814\n",
      "1525, train loss: 0.601211, test loss: 0.624325, train acc: 0.722892, test acc: 0.728814\n",
      "1526, train loss: 0.601189, test loss: 0.624304, train acc: 0.722892, test acc: 0.728814\n",
      "1527, train loss: 0.601167, test loss: 0.624284, train acc: 0.722892, test acc: 0.728814\n",
      "1528, train loss: 0.601145, test loss: 0.624263, train acc: 0.722892, test acc: 0.728814\n",
      "1529, train loss: 0.601123, test loss: 0.624242, train acc: 0.722892, test acc: 0.728814\n",
      "1530, train loss: 0.601101, test loss: 0.624222, train acc: 0.722892, test acc: 0.728814\n",
      "1531, train loss: 0.601079, test loss: 0.624201, train acc: 0.722892, test acc: 0.728814\n",
      "1532, train loss: 0.601057, test loss: 0.624181, train acc: 0.722892, test acc: 0.728814\n",
      "1533, train loss: 0.601035, test loss: 0.62416, train acc: 0.722892, test acc: 0.728814\n",
      "1534, train loss: 0.601013, test loss: 0.62414, train acc: 0.722892, test acc: 0.728814\n",
      "1535, train loss: 0.600991, test loss: 0.624119, train acc: 0.722892, test acc: 0.728814\n",
      "1536, train loss: 0.600969, test loss: 0.624099, train acc: 0.722892, test acc: 0.728814\n",
      "1537, train loss: 0.600947, test loss: 0.624078, train acc: 0.722892, test acc: 0.728814\n",
      "1538, train loss: 0.600925, test loss: 0.624058, train acc: 0.722892, test acc: 0.728814\n",
      "1539, train loss: 0.600904, test loss: 0.624037, train acc: 0.722892, test acc: 0.728814\n",
      "1540, train loss: 0.600882, test loss: 0.624017, train acc: 0.722892, test acc: 0.728814\n",
      "1541, train loss: 0.60086, test loss: 0.623997, train acc: 0.722892, test acc: 0.728814\n",
      "1542, train loss: 0.600838, test loss: 0.623976, train acc: 0.722892, test acc: 0.728814\n",
      "1543, train loss: 0.600816, test loss: 0.623956, train acc: 0.722892, test acc: 0.728814\n",
      "1544, train loss: 0.600795, test loss: 0.623936, train acc: 0.722892, test acc: 0.728814\n",
      "1545, train loss: 0.600773, test loss: 0.623915, train acc: 0.722892, test acc: 0.728814\n",
      "1546, train loss: 0.600751, test loss: 0.623895, train acc: 0.722892, test acc: 0.728814\n",
      "1547, train loss: 0.60073, test loss: 0.623875, train acc: 0.722892, test acc: 0.728814\n",
      "1548, train loss: 0.600708, test loss: 0.623854, train acc: 0.722892, test acc: 0.728814\n",
      "1549, train loss: 0.600686, test loss: 0.623834, train acc: 0.722892, test acc: 0.728814\n",
      "1550, train loss: 0.600665, test loss: 0.623814, train acc: 0.722892, test acc: 0.728814\n",
      "1551, train loss: 0.600643, test loss: 0.623793, train acc: 0.722892, test acc: 0.728814\n",
      "1552, train loss: 0.600622, test loss: 0.623773, train acc: 0.722892, test acc: 0.728814\n",
      "1553, train loss: 0.6006, test loss: 0.623753, train acc: 0.722892, test acc: 0.728814\n",
      "1554, train loss: 0.600579, test loss: 0.623733, train acc: 0.722892, test acc: 0.728814\n",
      "1555, train loss: 0.600557, test loss: 0.623713, train acc: 0.722892, test acc: 0.728814\n",
      "1556, train loss: 0.600536, test loss: 0.623692, train acc: 0.722892, test acc: 0.728814\n",
      "1557, train loss: 0.600514, test loss: 0.623672, train acc: 0.722892, test acc: 0.728814\n",
      "1558, train loss: 0.600493, test loss: 0.623652, train acc: 0.722892, test acc: 0.728814\n",
      "1559, train loss: 0.600471, test loss: 0.623632, train acc: 0.722892, test acc: 0.728814\n",
      "1560, train loss: 0.60045, test loss: 0.623612, train acc: 0.722892, test acc: 0.728814\n",
      "1561, train loss: 0.600428, test loss: 0.623592, train acc: 0.722892, test acc: 0.728814\n",
      "1562, train loss: 0.600407, test loss: 0.623572, train acc: 0.722892, test acc: 0.728814\n",
      "1563, train loss: 0.600386, test loss: 0.623552, train acc: 0.722892, test acc: 0.728814\n",
      "1564, train loss: 0.600364, test loss: 0.623532, train acc: 0.722892, test acc: 0.728814\n",
      "1565, train loss: 0.600343, test loss: 0.623512, train acc: 0.722892, test acc: 0.728814\n",
      "1566, train loss: 0.600322, test loss: 0.623491, train acc: 0.722892, test acc: 0.728814\n",
      "1567, train loss: 0.6003, test loss: 0.623471, train acc: 0.722892, test acc: 0.728814\n",
      "1568, train loss: 0.600279, test loss: 0.623451, train acc: 0.722892, test acc: 0.728814\n",
      "1569, train loss: 0.600258, test loss: 0.623432, train acc: 0.722892, test acc: 0.728814\n",
      "1570, train loss: 0.600237, test loss: 0.623412, train acc: 0.722892, test acc: 0.728814\n",
      "1571, train loss: 0.600215, test loss: 0.623392, train acc: 0.722892, test acc: 0.728814\n",
      "1572, train loss: 0.600194, test loss: 0.623372, train acc: 0.722892, test acc: 0.728814\n",
      "1573, train loss: 0.600173, test loss: 0.623352, train acc: 0.722892, test acc: 0.728814\n",
      "1574, train loss: 0.600152, test loss: 0.623332, train acc: 0.722892, test acc: 0.728814\n",
      "1575, train loss: 0.600131, test loss: 0.623312, train acc: 0.722892, test acc: 0.728814\n",
      "1576, train loss: 0.60011, test loss: 0.623292, train acc: 0.722892, test acc: 0.728814\n",
      "1577, train loss: 0.600089, test loss: 0.623272, train acc: 0.722892, test acc: 0.728814\n",
      "1578, train loss: 0.600067, test loss: 0.623252, train acc: 0.722892, test acc: 0.728814\n",
      "1579, train loss: 0.600046, test loss: 0.623232, train acc: 0.722892, test acc: 0.728814\n",
      "1580, train loss: 0.600025, test loss: 0.623213, train acc: 0.722892, test acc: 0.728814\n",
      "1581, train loss: 0.600004, test loss: 0.623193, train acc: 0.722892, test acc: 0.728814\n",
      "1582, train loss: 0.599983, test loss: 0.623173, train acc: 0.722892, test acc: 0.728814\n",
      "1583, train loss: 0.599962, test loss: 0.623153, train acc: 0.722892, test acc: 0.728814\n",
      "1584, train loss: 0.599941, test loss: 0.623133, train acc: 0.722892, test acc: 0.728814\n",
      "1585, train loss: 0.59992, test loss: 0.623114, train acc: 0.722892, test acc: 0.728814\n",
      "1586, train loss: 0.599899, test loss: 0.623094, train acc: 0.722892, test acc: 0.728814\n",
      "1587, train loss: 0.599878, test loss: 0.623074, train acc: 0.722892, test acc: 0.728814\n",
      "1588, train loss: 0.599858, test loss: 0.623055, train acc: 0.722892, test acc: 0.728814\n",
      "1589, train loss: 0.599837, test loss: 0.623035, train acc: 0.722892, test acc: 0.728814\n",
      "1590, train loss: 0.599816, test loss: 0.623015, train acc: 0.722892, test acc: 0.728814\n",
      "1591, train loss: 0.599795, test loss: 0.622995, train acc: 0.722892, test acc: 0.728814\n",
      "1592, train loss: 0.599774, test loss: 0.622976, train acc: 0.722892, test acc: 0.728814\n",
      "1593, train loss: 0.599753, test loss: 0.622956, train acc: 0.722892, test acc: 0.728814\n",
      "1594, train loss: 0.599733, test loss: 0.622936, train acc: 0.722892, test acc: 0.728814\n",
      "1595, train loss: 0.599712, test loss: 0.622917, train acc: 0.722892, test acc: 0.728814\n",
      "1596, train loss: 0.599691, test loss: 0.622897, train acc: 0.722892, test acc: 0.728814\n",
      "1597, train loss: 0.59967, test loss: 0.622878, train acc: 0.722892, test acc: 0.728814\n",
      "1598, train loss: 0.599649, test loss: 0.622858, train acc: 0.722892, test acc: 0.728814\n",
      "1599, train loss: 0.599629, test loss: 0.622839, train acc: 0.722892, test acc: 0.728814\n",
      "1600, train loss: 0.599608, test loss: 0.622819, train acc: 0.722892, test acc: 0.728814\n",
      "1601, train loss: 0.599587, test loss: 0.622799, train acc: 0.722892, test acc: 0.728814\n",
      "1602, train loss: 0.599567, test loss: 0.62278, train acc: 0.722892, test acc: 0.728814\n",
      "1603, train loss: 0.599546, test loss: 0.62276, train acc: 0.722892, test acc: 0.728814\n",
      "1604, train loss: 0.599525, test loss: 0.622741, train acc: 0.722892, test acc: 0.728814\n",
      "1605, train loss: 0.599505, test loss: 0.622721, train acc: 0.722892, test acc: 0.728814\n",
      "1606, train loss: 0.599484, test loss: 0.622702, train acc: 0.722892, test acc: 0.728814\n",
      "1607, train loss: 0.599464, test loss: 0.622682, train acc: 0.722892, test acc: 0.728814\n",
      "1608, train loss: 0.599443, test loss: 0.622663, train acc: 0.722892, test acc: 0.728814\n",
      "1609, train loss: 0.599423, test loss: 0.622644, train acc: 0.722892, test acc: 0.728814\n",
      "1610, train loss: 0.599402, test loss: 0.622624, train acc: 0.722892, test acc: 0.728814\n",
      "1611, train loss: 0.599382, test loss: 0.622605, train acc: 0.722892, test acc: 0.728814\n",
      "1612, train loss: 0.599361, test loss: 0.622585, train acc: 0.722892, test acc: 0.728814\n",
      "1613, train loss: 0.599341, test loss: 0.622566, train acc: 0.722892, test acc: 0.728814\n",
      "1614, train loss: 0.59932, test loss: 0.622547, train acc: 0.722892, test acc: 0.728814\n",
      "1615, train loss: 0.5993, test loss: 0.622527, train acc: 0.722892, test acc: 0.728814\n",
      "1616, train loss: 0.599279, test loss: 0.622508, train acc: 0.722892, test acc: 0.728814\n",
      "1617, train loss: 0.599259, test loss: 0.622489, train acc: 0.722892, test acc: 0.728814\n",
      "1618, train loss: 0.599238, test loss: 0.622469, train acc: 0.722892, test acc: 0.728814\n",
      "1619, train loss: 0.599218, test loss: 0.62245, train acc: 0.722892, test acc: 0.728814\n",
      "1620, train loss: 0.599198, test loss: 0.622431, train acc: 0.722892, test acc: 0.728814\n",
      "1621, train loss: 0.599177, test loss: 0.622411, train acc: 0.722892, test acc: 0.728814\n",
      "1622, train loss: 0.599157, test loss: 0.622392, train acc: 0.722892, test acc: 0.728814\n",
      "1623, train loss: 0.599137, test loss: 0.622373, train acc: 0.722892, test acc: 0.728814\n",
      "1624, train loss: 0.599116, test loss: 0.622354, train acc: 0.722892, test acc: 0.728814\n",
      "1625, train loss: 0.599096, test loss: 0.622334, train acc: 0.722892, test acc: 0.728814\n",
      "1626, train loss: 0.599076, test loss: 0.622315, train acc: 0.722892, test acc: 0.728814\n",
      "1627, train loss: 0.599056, test loss: 0.622296, train acc: 0.722892, test acc: 0.728814\n",
      "1628, train loss: 0.599035, test loss: 0.622277, train acc: 0.722892, test acc: 0.728814\n",
      "1629, train loss: 0.599015, test loss: 0.622258, train acc: 0.722892, test acc: 0.728814\n",
      "1630, train loss: 0.598995, test loss: 0.622239, train acc: 0.722892, test acc: 0.728814\n",
      "1631, train loss: 0.598975, test loss: 0.622219, train acc: 0.722892, test acc: 0.728814\n",
      "1632, train loss: 0.598955, test loss: 0.6222, train acc: 0.722892, test acc: 0.728814\n",
      "1633, train loss: 0.598934, test loss: 0.622181, train acc: 0.722892, test acc: 0.728814\n",
      "1634, train loss: 0.598914, test loss: 0.622162, train acc: 0.722892, test acc: 0.728814\n",
      "1635, train loss: 0.598894, test loss: 0.622143, train acc: 0.722892, test acc: 0.728814\n",
      "1636, train loss: 0.598874, test loss: 0.622124, train acc: 0.722892, test acc: 0.728814\n",
      "1637, train loss: 0.598854, test loss: 0.622105, train acc: 0.722892, test acc: 0.728814\n",
      "1638, train loss: 0.598834, test loss: 0.622086, train acc: 0.722892, test acc: 0.728814\n",
      "1639, train loss: 0.598814, test loss: 0.622067, train acc: 0.722892, test acc: 0.728814\n",
      "1640, train loss: 0.598794, test loss: 0.622048, train acc: 0.722892, test acc: 0.728814\n",
      "1641, train loss: 0.598774, test loss: 0.622029, train acc: 0.722892, test acc: 0.728814\n",
      "1642, train loss: 0.598754, test loss: 0.62201, train acc: 0.722892, test acc: 0.728814\n",
      "1643, train loss: 0.598734, test loss: 0.621991, train acc: 0.722892, test acc: 0.728814\n",
      "1644, train loss: 0.598714, test loss: 0.621972, train acc: 0.722892, test acc: 0.728814\n",
      "1645, train loss: 0.598694, test loss: 0.621953, train acc: 0.722892, test acc: 0.728814\n",
      "1646, train loss: 0.598674, test loss: 0.621934, train acc: 0.722892, test acc: 0.728814\n",
      "1647, train loss: 0.598654, test loss: 0.621915, train acc: 0.722892, test acc: 0.728814\n",
      "1648, train loss: 0.598634, test loss: 0.621896, train acc: 0.722892, test acc: 0.728814\n",
      "1649, train loss: 0.598614, test loss: 0.621877, train acc: 0.722892, test acc: 0.728814\n",
      "1650, train loss: 0.598594, test loss: 0.621858, train acc: 0.722892, test acc: 0.728814\n",
      "1651, train loss: 0.598575, test loss: 0.621839, train acc: 0.722892, test acc: 0.728814\n",
      "1652, train loss: 0.598555, test loss: 0.62182, train acc: 0.722892, test acc: 0.728814\n",
      "1653, train loss: 0.598535, test loss: 0.621801, train acc: 0.722892, test acc: 0.728814\n",
      "1654, train loss: 0.598515, test loss: 0.621782, train acc: 0.722892, test acc: 0.728814\n",
      "1655, train loss: 0.598495, test loss: 0.621764, train acc: 0.722892, test acc: 0.728814\n",
      "1656, train loss: 0.598475, test loss: 0.621745, train acc: 0.722892, test acc: 0.728814\n",
      "1657, train loss: 0.598456, test loss: 0.621726, train acc: 0.722892, test acc: 0.728814\n",
      "1658, train loss: 0.598436, test loss: 0.621707, train acc: 0.722892, test acc: 0.728814\n",
      "1659, train loss: 0.598416, test loss: 0.621688, train acc: 0.722892, test acc: 0.728814\n",
      "1660, train loss: 0.598396, test loss: 0.621669, train acc: 0.722892, test acc: 0.728814\n",
      "1661, train loss: 0.598377, test loss: 0.621651, train acc: 0.722892, test acc: 0.728814\n",
      "1662, train loss: 0.598357, test loss: 0.621632, train acc: 0.722892, test acc: 0.728814\n",
      "1663, train loss: 0.598337, test loss: 0.621613, train acc: 0.722892, test acc: 0.728814\n",
      "1664, train loss: 0.598318, test loss: 0.621594, train acc: 0.722892, test acc: 0.728814\n",
      "1665, train loss: 0.598298, test loss: 0.621576, train acc: 0.722892, test acc: 0.728814\n",
      "1666, train loss: 0.598278, test loss: 0.621557, train acc: 0.722892, test acc: 0.728814\n",
      "1667, train loss: 0.598259, test loss: 0.621538, train acc: 0.722892, test acc: 0.728814\n",
      "1668, train loss: 0.598239, test loss: 0.621519, train acc: 0.722892, test acc: 0.728814\n",
      "1669, train loss: 0.59822, test loss: 0.621501, train acc: 0.722892, test acc: 0.728814\n",
      "1670, train loss: 0.5982, test loss: 0.621482, train acc: 0.722892, test acc: 0.728814\n",
      "1671, train loss: 0.59818, test loss: 0.621463, train acc: 0.722892, test acc: 0.728814\n",
      "1672, train loss: 0.598161, test loss: 0.621445, train acc: 0.722892, test acc: 0.728814\n",
      "1673, train loss: 0.598141, test loss: 0.621426, train acc: 0.722892, test acc: 0.728814\n",
      "1674, train loss: 0.598122, test loss: 0.621407, train acc: 0.722892, test acc: 0.728814\n",
      "1675, train loss: 0.598102, test loss: 0.621389, train acc: 0.722892, test acc: 0.728814\n",
      "1676, train loss: 0.598083, test loss: 0.62137, train acc: 0.722892, test acc: 0.728814\n",
      "1677, train loss: 0.598063, test loss: 0.621352, train acc: 0.722892, test acc: 0.728814\n",
      "1678, train loss: 0.598044, test loss: 0.621333, train acc: 0.722892, test acc: 0.728814\n",
      "1679, train loss: 0.598025, test loss: 0.621314, train acc: 0.722892, test acc: 0.728814\n",
      "1680, train loss: 0.598005, test loss: 0.621296, train acc: 0.722892, test acc: 0.728814\n",
      "1681, train loss: 0.597986, test loss: 0.621277, train acc: 0.722892, test acc: 0.728814\n",
      "1682, train loss: 0.597966, test loss: 0.621259, train acc: 0.722892, test acc: 0.728814\n",
      "1683, train loss: 0.597947, test loss: 0.62124, train acc: 0.722892, test acc: 0.728814\n",
      "1684, train loss: 0.597928, test loss: 0.621222, train acc: 0.722892, test acc: 0.728814\n",
      "1685, train loss: 0.597908, test loss: 0.621203, train acc: 0.722892, test acc: 0.728814\n",
      "1686, train loss: 0.597889, test loss: 0.621185, train acc: 0.722892, test acc: 0.728814\n",
      "1687, train loss: 0.59787, test loss: 0.621166, train acc: 0.722892, test acc: 0.728814\n",
      "1688, train loss: 0.59785, test loss: 0.621148, train acc: 0.722892, test acc: 0.728814\n",
      "1689, train loss: 0.597831, test loss: 0.621129, train acc: 0.725904, test acc: 0.728814\n",
      "1690, train loss: 0.597812, test loss: 0.621111, train acc: 0.725904, test acc: 0.728814\n",
      "1691, train loss: 0.597792, test loss: 0.621092, train acc: 0.725904, test acc: 0.728814\n",
      "1692, train loss: 0.597773, test loss: 0.621074, train acc: 0.725904, test acc: 0.728814\n",
      "1693, train loss: 0.597754, test loss: 0.621056, train acc: 0.725904, test acc: 0.728814\n",
      "1694, train loss: 0.597735, test loss: 0.621037, train acc: 0.725904, test acc: 0.728814\n",
      "1695, train loss: 0.597715, test loss: 0.621019, train acc: 0.725904, test acc: 0.728814\n",
      "1696, train loss: 0.597696, test loss: 0.621, train acc: 0.725904, test acc: 0.728814\n",
      "1697, train loss: 0.597677, test loss: 0.620982, train acc: 0.725904, test acc: 0.728814\n",
      "1698, train loss: 0.597658, test loss: 0.620964, train acc: 0.725904, test acc: 0.728814\n",
      "1699, train loss: 0.597639, test loss: 0.620945, train acc: 0.725904, test acc: 0.728814\n",
      "1700, train loss: 0.59762, test loss: 0.620927, train acc: 0.725904, test acc: 0.728814\n",
      "1701, train loss: 0.5976, test loss: 0.620908, train acc: 0.725904, test acc: 0.728814\n",
      "1702, train loss: 0.597581, test loss: 0.62089, train acc: 0.725904, test acc: 0.728814\n",
      "1703, train loss: 0.597562, test loss: 0.620872, train acc: 0.725904, test acc: 0.728814\n",
      "1704, train loss: 0.597543, test loss: 0.620854, train acc: 0.725904, test acc: 0.728814\n",
      "1705, train loss: 0.597524, test loss: 0.620835, train acc: 0.725904, test acc: 0.728814\n",
      "1706, train loss: 0.597505, test loss: 0.620817, train acc: 0.725904, test acc: 0.728814\n",
      "1707, train loss: 0.597486, test loss: 0.620799, train acc: 0.725904, test acc: 0.728814\n",
      "1708, train loss: 0.597467, test loss: 0.620781, train acc: 0.725904, test acc: 0.728814\n",
      "1709, train loss: 0.597448, test loss: 0.620762, train acc: 0.725904, test acc: 0.728814\n",
      "1710, train loss: 0.597429, test loss: 0.620744, train acc: 0.725904, test acc: 0.728814\n",
      "1711, train loss: 0.59741, test loss: 0.620726, train acc: 0.725904, test acc: 0.728814\n",
      "1712, train loss: 0.597391, test loss: 0.620708, train acc: 0.725904, test acc: 0.728814\n",
      "1713, train loss: 0.597372, test loss: 0.620689, train acc: 0.725904, test acc: 0.728814\n",
      "1714, train loss: 0.597353, test loss: 0.620671, train acc: 0.725904, test acc: 0.728814\n",
      "1715, train loss: 0.597334, test loss: 0.620653, train acc: 0.725904, test acc: 0.728814\n",
      "1716, train loss: 0.597315, test loss: 0.620635, train acc: 0.725904, test acc: 0.728814\n",
      "1717, train loss: 0.597296, test loss: 0.620617, train acc: 0.725904, test acc: 0.728814\n",
      "1718, train loss: 0.597277, test loss: 0.620598, train acc: 0.725904, test acc: 0.728814\n",
      "1719, train loss: 0.597259, test loss: 0.62058, train acc: 0.725904, test acc: 0.728814\n",
      "1720, train loss: 0.59724, test loss: 0.620562, train acc: 0.725904, test acc: 0.728814\n",
      "1721, train loss: 0.597221, test loss: 0.620544, train acc: 0.725904, test acc: 0.728814\n",
      "1722, train loss: 0.597202, test loss: 0.620526, train acc: 0.725904, test acc: 0.728814\n",
      "1723, train loss: 0.597183, test loss: 0.620508, train acc: 0.725904, test acc: 0.728814\n",
      "1724, train loss: 0.597164, test loss: 0.62049, train acc: 0.725904, test acc: 0.728814\n",
      "1725, train loss: 0.597146, test loss: 0.620472, train acc: 0.725904, test acc: 0.728814\n",
      "1726, train loss: 0.597127, test loss: 0.620454, train acc: 0.725904, test acc: 0.728814\n",
      "1727, train loss: 0.597108, test loss: 0.620435, train acc: 0.725904, test acc: 0.728814\n",
      "1728, train loss: 0.597089, test loss: 0.620417, train acc: 0.725904, test acc: 0.728814\n",
      "1729, train loss: 0.597071, test loss: 0.620399, train acc: 0.725904, test acc: 0.728814\n",
      "1730, train loss: 0.597052, test loss: 0.620381, train acc: 0.725904, test acc: 0.728814\n",
      "1731, train loss: 0.597033, test loss: 0.620363, train acc: 0.725904, test acc: 0.728814\n",
      "1732, train loss: 0.597014, test loss: 0.620345, train acc: 0.725904, test acc: 0.728814\n",
      "1733, train loss: 0.596996, test loss: 0.620327, train acc: 0.725904, test acc: 0.728814\n",
      "1734, train loss: 0.596977, test loss: 0.620309, train acc: 0.725904, test acc: 0.728814\n",
      "1735, train loss: 0.596958, test loss: 0.620291, train acc: 0.725904, test acc: 0.728814\n",
      "1736, train loss: 0.59694, test loss: 0.620273, train acc: 0.725904, test acc: 0.728814\n",
      "1737, train loss: 0.596921, test loss: 0.620255, train acc: 0.725904, test acc: 0.728814\n",
      "1738, train loss: 0.596903, test loss: 0.620238, train acc: 0.725904, test acc: 0.728814\n",
      "1739, train loss: 0.596884, test loss: 0.620219, train acc: 0.725904, test acc: 0.728814\n",
      "1740, train loss: 0.596865, test loss: 0.620202, train acc: 0.725904, test acc: 0.728814\n",
      "1741, train loss: 0.596847, test loss: 0.620184, train acc: 0.725904, test acc: 0.728814\n",
      "1742, train loss: 0.596828, test loss: 0.620166, train acc: 0.725904, test acc: 0.728814\n",
      "1743, train loss: 0.59681, test loss: 0.620148, train acc: 0.725904, test acc: 0.728814\n",
      "1744, train loss: 0.596791, test loss: 0.62013, train acc: 0.725904, test acc: 0.728814\n",
      "1745, train loss: 0.596773, test loss: 0.620112, train acc: 0.725904, test acc: 0.728814\n",
      "1746, train loss: 0.596754, test loss: 0.620094, train acc: 0.725904, test acc: 0.728814\n",
      "1747, train loss: 0.596736, test loss: 0.620076, train acc: 0.725904, test acc: 0.728814\n",
      "1748, train loss: 0.596717, test loss: 0.620058, train acc: 0.725904, test acc: 0.728814\n",
      "1749, train loss: 0.596699, test loss: 0.620041, train acc: 0.725904, test acc: 0.728814\n",
      "1750, train loss: 0.59668, test loss: 0.620023, train acc: 0.725904, test acc: 0.728814\n",
      "1751, train loss: 0.596662, test loss: 0.620005, train acc: 0.725904, test acc: 0.728814\n",
      "1752, train loss: 0.596643, test loss: 0.619987, train acc: 0.725904, test acc: 0.728814\n",
      "1753, train loss: 0.596625, test loss: 0.619969, train acc: 0.725904, test acc: 0.728814\n",
      "1754, train loss: 0.596606, test loss: 0.619952, train acc: 0.725904, test acc: 0.728814\n",
      "1755, train loss: 0.596588, test loss: 0.619934, train acc: 0.725904, test acc: 0.728814\n",
      "1756, train loss: 0.59657, test loss: 0.619916, train acc: 0.725904, test acc: 0.728814\n",
      "1757, train loss: 0.596551, test loss: 0.619898, train acc: 0.725904, test acc: 0.728814\n",
      "1758, train loss: 0.596533, test loss: 0.61988, train acc: 0.725904, test acc: 0.728814\n",
      "1759, train loss: 0.596515, test loss: 0.619863, train acc: 0.725904, test acc: 0.728814\n",
      "1760, train loss: 0.596496, test loss: 0.619845, train acc: 0.725904, test acc: 0.728814\n",
      "1761, train loss: 0.596478, test loss: 0.619827, train acc: 0.725904, test acc: 0.728814\n",
      "1762, train loss: 0.59646, test loss: 0.619809, train acc: 0.725904, test acc: 0.728814\n",
      "1763, train loss: 0.596441, test loss: 0.619792, train acc: 0.725904, test acc: 0.728814\n",
      "1764, train loss: 0.596423, test loss: 0.619774, train acc: 0.725904, test acc: 0.728814\n",
      "1765, train loss: 0.596405, test loss: 0.619756, train acc: 0.725904, test acc: 0.728814\n",
      "1766, train loss: 0.596386, test loss: 0.619739, train acc: 0.725904, test acc: 0.728814\n",
      "1767, train loss: 0.596368, test loss: 0.619721, train acc: 0.725904, test acc: 0.728814\n",
      "1768, train loss: 0.59635, test loss: 0.619703, train acc: 0.725904, test acc: 0.728814\n",
      "1769, train loss: 0.596332, test loss: 0.619686, train acc: 0.725904, test acc: 0.728814\n",
      "1770, train loss: 0.596314, test loss: 0.619668, train acc: 0.725904, test acc: 0.728814\n",
      "1771, train loss: 0.596295, test loss: 0.61965, train acc: 0.725904, test acc: 0.728814\n",
      "1772, train loss: 0.596277, test loss: 0.619633, train acc: 0.725904, test acc: 0.728814\n",
      "1773, train loss: 0.596259, test loss: 0.619615, train acc: 0.725904, test acc: 0.728814\n",
      "1774, train loss: 0.596241, test loss: 0.619597, train acc: 0.725904, test acc: 0.728814\n",
      "1775, train loss: 0.596223, test loss: 0.61958, train acc: 0.725904, test acc: 0.728814\n",
      "1776, train loss: 0.596205, test loss: 0.619562, train acc: 0.725904, test acc: 0.728814\n",
      "1777, train loss: 0.596186, test loss: 0.619545, train acc: 0.725904, test acc: 0.728814\n",
      "1778, train loss: 0.596168, test loss: 0.619527, train acc: 0.725904, test acc: 0.728814\n",
      "1779, train loss: 0.59615, test loss: 0.61951, train acc: 0.725904, test acc: 0.728814\n",
      "1780, train loss: 0.596132, test loss: 0.619492, train acc: 0.725904, test acc: 0.728814\n",
      "1781, train loss: 0.596114, test loss: 0.619475, train acc: 0.725904, test acc: 0.728814\n",
      "1782, train loss: 0.596096, test loss: 0.619457, train acc: 0.725904, test acc: 0.728814\n",
      "1783, train loss: 0.596078, test loss: 0.619439, train acc: 0.725904, test acc: 0.728814\n",
      "1784, train loss: 0.59606, test loss: 0.619422, train acc: 0.725904, test acc: 0.728814\n",
      "1785, train loss: 0.596042, test loss: 0.619404, train acc: 0.725904, test acc: 0.728814\n",
      "1786, train loss: 0.596024, test loss: 0.619387, train acc: 0.725904, test acc: 0.728814\n",
      "1787, train loss: 0.596006, test loss: 0.619369, train acc: 0.725904, test acc: 0.728814\n",
      "1788, train loss: 0.595988, test loss: 0.619352, train acc: 0.725904, test acc: 0.728814\n",
      "1789, train loss: 0.59597, test loss: 0.619335, train acc: 0.725904, test acc: 0.728814\n",
      "1790, train loss: 0.595952, test loss: 0.619317, train acc: 0.725904, test acc: 0.728814\n",
      "1791, train loss: 0.595934, test loss: 0.6193, train acc: 0.725904, test acc: 0.728814\n",
      "1792, train loss: 0.595916, test loss: 0.619282, train acc: 0.725904, test acc: 0.728814\n",
      "1793, train loss: 0.595898, test loss: 0.619265, train acc: 0.725904, test acc: 0.728814\n",
      "1794, train loss: 0.59588, test loss: 0.619247, train acc: 0.725904, test acc: 0.728814\n",
      "1795, train loss: 0.595862, test loss: 0.61923, train acc: 0.725904, test acc: 0.728814\n",
      "1796, train loss: 0.595844, test loss: 0.619212, train acc: 0.725904, test acc: 0.728814\n",
      "1797, train loss: 0.595826, test loss: 0.619195, train acc: 0.725904, test acc: 0.728814\n",
      "1798, train loss: 0.595809, test loss: 0.619178, train acc: 0.725904, test acc: 0.728814\n",
      "1799, train loss: 0.595791, test loss: 0.61916, train acc: 0.725904, test acc: 0.728814\n",
      "1800, train loss: 0.595773, test loss: 0.619143, train acc: 0.725904, test acc: 0.728814\n",
      "1801, train loss: 0.595755, test loss: 0.619125, train acc: 0.725904, test acc: 0.728814\n",
      "1802, train loss: 0.595737, test loss: 0.619108, train acc: 0.725904, test acc: 0.728814\n",
      "1803, train loss: 0.595719, test loss: 0.619091, train acc: 0.725904, test acc: 0.728814\n",
      "1804, train loss: 0.595702, test loss: 0.619073, train acc: 0.725904, test acc: 0.728814\n",
      "1805, train loss: 0.595684, test loss: 0.619056, train acc: 0.725904, test acc: 0.728814\n",
      "1806, train loss: 0.595666, test loss: 0.619039, train acc: 0.725904, test acc: 0.728814\n",
      "1807, train loss: 0.595648, test loss: 0.619021, train acc: 0.725904, test acc: 0.728814\n",
      "1808, train loss: 0.59563, test loss: 0.619004, train acc: 0.725904, test acc: 0.728814\n",
      "1809, train loss: 0.595613, test loss: 0.618987, train acc: 0.725904, test acc: 0.728814\n",
      "1810, train loss: 0.595595, test loss: 0.61897, train acc: 0.725904, test acc: 0.728814\n",
      "1811, train loss: 0.595577, test loss: 0.618952, train acc: 0.725904, test acc: 0.728814\n",
      "1812, train loss: 0.59556, test loss: 0.618935, train acc: 0.725904, test acc: 0.728814\n",
      "1813, train loss: 0.595542, test loss: 0.618918, train acc: 0.725904, test acc: 0.728814\n",
      "1814, train loss: 0.595524, test loss: 0.6189, train acc: 0.725904, test acc: 0.728814\n",
      "1815, train loss: 0.595506, test loss: 0.618883, train acc: 0.725904, test acc: 0.728814\n",
      "1816, train loss: 0.595489, test loss: 0.618866, train acc: 0.725904, test acc: 0.728814\n",
      "1817, train loss: 0.595471, test loss: 0.618849, train acc: 0.725904, test acc: 0.728814\n",
      "1818, train loss: 0.595454, test loss: 0.618832, train acc: 0.725904, test acc: 0.728814\n",
      "1819, train loss: 0.595436, test loss: 0.618814, train acc: 0.725904, test acc: 0.728814\n",
      "1820, train loss: 0.595418, test loss: 0.618797, train acc: 0.725904, test acc: 0.728814\n",
      "1821, train loss: 0.595401, test loss: 0.61878, train acc: 0.725904, test acc: 0.728814\n",
      "1822, train loss: 0.595383, test loss: 0.618763, train acc: 0.725904, test acc: 0.728814\n",
      "1823, train loss: 0.595365, test loss: 0.618746, train acc: 0.725904, test acc: 0.728814\n",
      "1824, train loss: 0.595348, test loss: 0.618728, train acc: 0.725904, test acc: 0.728814\n",
      "1825, train loss: 0.59533, test loss: 0.618711, train acc: 0.725904, test acc: 0.728814\n",
      "1826, train loss: 0.595313, test loss: 0.618694, train acc: 0.725904, test acc: 0.728814\n",
      "1827, train loss: 0.595295, test loss: 0.618677, train acc: 0.725904, test acc: 0.728814\n",
      "1828, train loss: 0.595278, test loss: 0.61866, train acc: 0.725904, test acc: 0.728814\n",
      "1829, train loss: 0.59526, test loss: 0.618643, train acc: 0.725904, test acc: 0.728814\n",
      "1830, train loss: 0.595243, test loss: 0.618626, train acc: 0.725904, test acc: 0.728814\n",
      "1831, train loss: 0.595225, test loss: 0.618608, train acc: 0.725904, test acc: 0.728814\n",
      "1832, train loss: 0.595208, test loss: 0.618591, train acc: 0.725904, test acc: 0.728814\n",
      "1833, train loss: 0.59519, test loss: 0.618574, train acc: 0.725904, test acc: 0.728814\n",
      "1834, train loss: 0.595173, test loss: 0.618557, train acc: 0.725904, test acc: 0.728814\n",
      "1835, train loss: 0.595155, test loss: 0.61854, train acc: 0.725904, test acc: 0.728814\n",
      "1836, train loss: 0.595138, test loss: 0.618523, train acc: 0.725904, test acc: 0.728814\n",
      "1837, train loss: 0.59512, test loss: 0.618506, train acc: 0.725904, test acc: 0.728814\n",
      "1838, train loss: 0.595103, test loss: 0.618489, train acc: 0.725904, test acc: 0.728814\n",
      "1839, train loss: 0.595086, test loss: 0.618472, train acc: 0.725904, test acc: 0.728814\n",
      "1840, train loss: 0.595068, test loss: 0.618455, train acc: 0.722892, test acc: 0.728814\n",
      "1841, train loss: 0.595051, test loss: 0.618438, train acc: 0.722892, test acc: 0.728814\n",
      "1842, train loss: 0.595033, test loss: 0.618421, train acc: 0.722892, test acc: 0.728814\n",
      "1843, train loss: 0.595016, test loss: 0.618404, train acc: 0.722892, test acc: 0.728814\n",
      "1844, train loss: 0.594999, test loss: 0.618387, train acc: 0.722892, test acc: 0.728814\n",
      "1845, train loss: 0.594981, test loss: 0.61837, train acc: 0.722892, test acc: 0.728814\n",
      "1846, train loss: 0.594964, test loss: 0.618353, train acc: 0.722892, test acc: 0.728814\n",
      "1847, train loss: 0.594947, test loss: 0.618336, train acc: 0.722892, test acc: 0.728814\n",
      "1848, train loss: 0.59493, test loss: 0.618319, train acc: 0.722892, test acc: 0.728814\n",
      "1849, train loss: 0.594912, test loss: 0.618302, train acc: 0.722892, test acc: 0.728814\n",
      "1850, train loss: 0.594895, test loss: 0.618285, train acc: 0.722892, test acc: 0.728814\n",
      "1851, train loss: 0.594878, test loss: 0.618268, train acc: 0.722892, test acc: 0.728814\n",
      "1852, train loss: 0.59486, test loss: 0.618251, train acc: 0.722892, test acc: 0.728814\n",
      "1853, train loss: 0.594843, test loss: 0.618234, train acc: 0.722892, test acc: 0.728814\n",
      "1854, train loss: 0.594826, test loss: 0.618217, train acc: 0.722892, test acc: 0.728814\n",
      "1855, train loss: 0.594809, test loss: 0.6182, train acc: 0.722892, test acc: 0.728814\n",
      "1856, train loss: 0.594791, test loss: 0.618183, train acc: 0.722892, test acc: 0.728814\n",
      "1857, train loss: 0.594774, test loss: 0.618166, train acc: 0.722892, test acc: 0.728814\n",
      "1858, train loss: 0.594757, test loss: 0.618149, train acc: 0.722892, test acc: 0.728814\n",
      "1859, train loss: 0.59474, test loss: 0.618132, train acc: 0.722892, test acc: 0.728814\n",
      "1860, train loss: 0.594723, test loss: 0.618116, train acc: 0.722892, test acc: 0.728814\n",
      "1861, train loss: 0.594705, test loss: 0.618099, train acc: 0.722892, test acc: 0.728814\n",
      "1862, train loss: 0.594688, test loss: 0.618082, train acc: 0.722892, test acc: 0.728814\n",
      "1863, train loss: 0.594671, test loss: 0.618065, train acc: 0.722892, test acc: 0.728814\n",
      "1864, train loss: 0.594654, test loss: 0.618048, train acc: 0.722892, test acc: 0.728814\n",
      "1865, train loss: 0.594637, test loss: 0.618031, train acc: 0.722892, test acc: 0.728814\n",
      "1866, train loss: 0.59462, test loss: 0.618014, train acc: 0.722892, test acc: 0.728814\n",
      "1867, train loss: 0.594603, test loss: 0.617998, train acc: 0.722892, test acc: 0.728814\n",
      "1868, train loss: 0.594586, test loss: 0.617981, train acc: 0.722892, test acc: 0.728814\n",
      "1869, train loss: 0.594568, test loss: 0.617964, train acc: 0.722892, test acc: 0.728814\n",
      "1870, train loss: 0.594551, test loss: 0.617947, train acc: 0.722892, test acc: 0.728814\n",
      "1871, train loss: 0.594534, test loss: 0.61793, train acc: 0.722892, test acc: 0.728814\n",
      "1872, train loss: 0.594517, test loss: 0.617914, train acc: 0.722892, test acc: 0.728814\n",
      "1873, train loss: 0.5945, test loss: 0.617897, train acc: 0.722892, test acc: 0.728814\n",
      "1874, train loss: 0.594483, test loss: 0.61788, train acc: 0.722892, test acc: 0.728814\n",
      "1875, train loss: 0.594466, test loss: 0.617863, train acc: 0.722892, test acc: 0.728814\n",
      "1876, train loss: 0.594449, test loss: 0.617846, train acc: 0.722892, test acc: 0.728814\n",
      "1877, train loss: 0.594432, test loss: 0.61783, train acc: 0.722892, test acc: 0.728814\n",
      "1878, train loss: 0.594415, test loss: 0.617813, train acc: 0.722892, test acc: 0.728814\n",
      "1879, train loss: 0.594398, test loss: 0.617796, train acc: 0.722892, test acc: 0.728814\n",
      "1880, train loss: 0.594381, test loss: 0.61778, train acc: 0.722892, test acc: 0.728814\n",
      "1881, train loss: 0.594364, test loss: 0.617763, train acc: 0.722892, test acc: 0.728814\n",
      "1882, train loss: 0.594347, test loss: 0.617746, train acc: 0.722892, test acc: 0.728814\n",
      "1883, train loss: 0.59433, test loss: 0.617729, train acc: 0.722892, test acc: 0.728814\n",
      "1884, train loss: 0.594313, test loss: 0.617713, train acc: 0.722892, test acc: 0.728814\n",
      "1885, train loss: 0.594297, test loss: 0.617696, train acc: 0.722892, test acc: 0.728814\n",
      "1886, train loss: 0.59428, test loss: 0.617679, train acc: 0.722892, test acc: 0.728814\n",
      "1887, train loss: 0.594263, test loss: 0.617663, train acc: 0.722892, test acc: 0.728814\n",
      "1888, train loss: 0.594246, test loss: 0.617646, train acc: 0.722892, test acc: 0.728814\n",
      "1889, train loss: 0.594229, test loss: 0.617629, train acc: 0.722892, test acc: 0.728814\n",
      "1890, train loss: 0.594212, test loss: 0.617613, train acc: 0.722892, test acc: 0.728814\n",
      "1891, train loss: 0.594195, test loss: 0.617596, train acc: 0.722892, test acc: 0.728814\n",
      "1892, train loss: 0.594178, test loss: 0.617579, train acc: 0.722892, test acc: 0.728814\n",
      "1893, train loss: 0.594162, test loss: 0.617563, train acc: 0.722892, test acc: 0.728814\n",
      "1894, train loss: 0.594145, test loss: 0.617546, train acc: 0.722892, test acc: 0.728814\n",
      "1895, train loss: 0.594128, test loss: 0.617529, train acc: 0.722892, test acc: 0.728814\n",
      "1896, train loss: 0.594111, test loss: 0.617513, train acc: 0.722892, test acc: 0.728814\n",
      "1897, train loss: 0.594094, test loss: 0.617496, train acc: 0.722892, test acc: 0.728814\n",
      "1898, train loss: 0.594077, test loss: 0.61748, train acc: 0.722892, test acc: 0.728814\n",
      "1899, train loss: 0.594061, test loss: 0.617463, train acc: 0.722892, test acc: 0.728814\n",
      "1900, train loss: 0.594044, test loss: 0.617446, train acc: 0.722892, test acc: 0.728814\n",
      "1901, train loss: 0.594027, test loss: 0.61743, train acc: 0.722892, test acc: 0.728814\n",
      "1902, train loss: 0.59401, test loss: 0.617413, train acc: 0.722892, test acc: 0.728814\n",
      "1903, train loss: 0.593994, test loss: 0.617397, train acc: 0.722892, test acc: 0.728814\n",
      "1904, train loss: 0.593977, test loss: 0.61738, train acc: 0.722892, test acc: 0.728814\n",
      "1905, train loss: 0.59396, test loss: 0.617364, train acc: 0.722892, test acc: 0.728814\n",
      "1906, train loss: 0.593943, test loss: 0.617347, train acc: 0.722892, test acc: 0.728814\n",
      "1907, train loss: 0.593927, test loss: 0.617331, train acc: 0.722892, test acc: 0.728814\n",
      "1908, train loss: 0.59391, test loss: 0.617314, train acc: 0.722892, test acc: 0.728814\n",
      "1909, train loss: 0.593893, test loss: 0.617298, train acc: 0.722892, test acc: 0.728814\n",
      "1910, train loss: 0.593877, test loss: 0.617281, train acc: 0.722892, test acc: 0.728814\n",
      "1911, train loss: 0.59386, test loss: 0.617265, train acc: 0.722892, test acc: 0.728814\n",
      "1912, train loss: 0.593843, test loss: 0.617248, train acc: 0.722892, test acc: 0.728814\n",
      "1913, train loss: 0.593827, test loss: 0.617232, train acc: 0.722892, test acc: 0.728814\n",
      "1914, train loss: 0.59381, test loss: 0.617215, train acc: 0.722892, test acc: 0.728814\n",
      "1915, train loss: 0.593793, test loss: 0.617199, train acc: 0.722892, test acc: 0.728814\n",
      "1916, train loss: 0.593777, test loss: 0.617182, train acc: 0.722892, test acc: 0.728814\n",
      "1917, train loss: 0.59376, test loss: 0.617166, train acc: 0.722892, test acc: 0.728814\n",
      "1918, train loss: 0.593744, test loss: 0.617149, train acc: 0.722892, test acc: 0.728814\n",
      "1919, train loss: 0.593727, test loss: 0.617133, train acc: 0.722892, test acc: 0.728814\n",
      "1920, train loss: 0.59371, test loss: 0.617116, train acc: 0.722892, test acc: 0.728814\n",
      "1921, train loss: 0.593694, test loss: 0.6171, train acc: 0.722892, test acc: 0.728814\n",
      "1922, train loss: 0.593677, test loss: 0.617083, train acc: 0.722892, test acc: 0.728814\n",
      "1923, train loss: 0.593661, test loss: 0.617067, train acc: 0.722892, test acc: 0.728814\n",
      "1924, train loss: 0.593644, test loss: 0.617051, train acc: 0.722892, test acc: 0.728814\n",
      "1925, train loss: 0.593628, test loss: 0.617034, train acc: 0.722892, test acc: 0.728814\n",
      "1926, train loss: 0.593611, test loss: 0.617018, train acc: 0.722892, test acc: 0.728814\n",
      "1927, train loss: 0.593595, test loss: 0.617001, train acc: 0.722892, test acc: 0.728814\n",
      "1928, train loss: 0.593578, test loss: 0.616985, train acc: 0.722892, test acc: 0.728814\n",
      "1929, train loss: 0.593562, test loss: 0.616969, train acc: 0.722892, test acc: 0.728814\n",
      "1930, train loss: 0.593545, test loss: 0.616952, train acc: 0.722892, test acc: 0.728814\n",
      "1931, train loss: 0.593529, test loss: 0.616936, train acc: 0.722892, test acc: 0.728814\n",
      "1932, train loss: 0.593512, test loss: 0.61692, train acc: 0.722892, test acc: 0.728814\n",
      "1933, train loss: 0.593496, test loss: 0.616903, train acc: 0.722892, test acc: 0.728814\n",
      "1934, train loss: 0.593479, test loss: 0.616887, train acc: 0.722892, test acc: 0.728814\n",
      "1935, train loss: 0.593463, test loss: 0.61687, train acc: 0.722892, test acc: 0.728814\n",
      "1936, train loss: 0.593447, test loss: 0.616854, train acc: 0.722892, test acc: 0.728814\n",
      "1937, train loss: 0.59343, test loss: 0.616838, train acc: 0.722892, test acc: 0.728814\n",
      "1938, train loss: 0.593414, test loss: 0.616821, train acc: 0.722892, test acc: 0.728814\n",
      "1939, train loss: 0.593397, test loss: 0.616805, train acc: 0.722892, test acc: 0.728814\n",
      "1940, train loss: 0.593381, test loss: 0.616789, train acc: 0.722892, test acc: 0.728814\n",
      "1941, train loss: 0.593365, test loss: 0.616773, train acc: 0.722892, test acc: 0.728814\n",
      "1942, train loss: 0.593348, test loss: 0.616756, train acc: 0.722892, test acc: 0.728814\n",
      "1943, train loss: 0.593332, test loss: 0.61674, train acc: 0.722892, test acc: 0.728814\n",
      "1944, train loss: 0.593315, test loss: 0.616724, train acc: 0.722892, test acc: 0.728814\n",
      "1945, train loss: 0.593299, test loss: 0.616707, train acc: 0.722892, test acc: 0.728814\n",
      "1946, train loss: 0.593283, test loss: 0.616691, train acc: 0.722892, test acc: 0.728814\n",
      "1947, train loss: 0.593266, test loss: 0.616675, train acc: 0.722892, test acc: 0.728814\n",
      "1948, train loss: 0.59325, test loss: 0.616659, train acc: 0.722892, test acc: 0.728814\n",
      "1949, train loss: 0.593234, test loss: 0.616642, train acc: 0.722892, test acc: 0.728814\n",
      "1950, train loss: 0.593217, test loss: 0.616626, train acc: 0.722892, test acc: 0.728814\n",
      "1951, train loss: 0.593201, test loss: 0.61661, train acc: 0.722892, test acc: 0.728814\n",
      "1952, train loss: 0.593185, test loss: 0.616594, train acc: 0.722892, test acc: 0.728814\n",
      "1953, train loss: 0.593169, test loss: 0.616577, train acc: 0.722892, test acc: 0.728814\n",
      "1954, train loss: 0.593152, test loss: 0.616561, train acc: 0.722892, test acc: 0.728814\n",
      "1955, train loss: 0.593136, test loss: 0.616545, train acc: 0.722892, test acc: 0.728814\n",
      "1956, train loss: 0.59312, test loss: 0.616529, train acc: 0.722892, test acc: 0.728814\n",
      "1957, train loss: 0.593104, test loss: 0.616513, train acc: 0.722892, test acc: 0.728814\n",
      "1958, train loss: 0.593087, test loss: 0.616496, train acc: 0.722892, test acc: 0.728814\n",
      "1959, train loss: 0.593071, test loss: 0.61648, train acc: 0.722892, test acc: 0.728814\n",
      "1960, train loss: 0.593055, test loss: 0.616464, train acc: 0.722892, test acc: 0.728814\n",
      "1961, train loss: 0.593039, test loss: 0.616448, train acc: 0.722892, test acc: 0.728814\n",
      "1962, train loss: 0.593023, test loss: 0.616432, train acc: 0.722892, test acc: 0.728814\n",
      "1963, train loss: 0.593006, test loss: 0.616416, train acc: 0.722892, test acc: 0.728814\n",
      "1964, train loss: 0.59299, test loss: 0.616399, train acc: 0.722892, test acc: 0.728814\n",
      "1965, train loss: 0.592974, test loss: 0.616383, train acc: 0.722892, test acc: 0.728814\n",
      "1966, train loss: 0.592958, test loss: 0.616367, train acc: 0.722892, test acc: 0.728814\n",
      "1967, train loss: 0.592942, test loss: 0.616351, train acc: 0.722892, test acc: 0.728814\n",
      "1968, train loss: 0.592926, test loss: 0.616335, train acc: 0.722892, test acc: 0.728814\n",
      "1969, train loss: 0.59291, test loss: 0.616319, train acc: 0.722892, test acc: 0.728814\n",
      "1970, train loss: 0.592893, test loss: 0.616303, train acc: 0.722892, test acc: 0.728814\n",
      "1971, train loss: 0.592877, test loss: 0.616287, train acc: 0.722892, test acc: 0.728814\n",
      "1972, train loss: 0.592861, test loss: 0.61627, train acc: 0.722892, test acc: 0.728814\n",
      "1973, train loss: 0.592845, test loss: 0.616254, train acc: 0.722892, test acc: 0.728814\n",
      "1974, train loss: 0.592829, test loss: 0.616238, train acc: 0.722892, test acc: 0.728814\n",
      "1975, train loss: 0.592813, test loss: 0.616222, train acc: 0.722892, test acc: 0.728814\n",
      "1976, train loss: 0.592797, test loss: 0.616206, train acc: 0.722892, test acc: 0.728814\n",
      "1977, train loss: 0.592781, test loss: 0.61619, train acc: 0.722892, test acc: 0.728814\n",
      "1978, train loss: 0.592765, test loss: 0.616174, train acc: 0.722892, test acc: 0.728814\n",
      "1979, train loss: 0.592749, test loss: 0.616158, train acc: 0.722892, test acc: 0.728814\n",
      "1980, train loss: 0.592733, test loss: 0.616142, train acc: 0.722892, test acc: 0.728814\n",
      "1981, train loss: 0.592717, test loss: 0.616126, train acc: 0.722892, test acc: 0.728814\n",
      "1982, train loss: 0.592701, test loss: 0.61611, train acc: 0.722892, test acc: 0.728814\n",
      "1983, train loss: 0.592685, test loss: 0.616094, train acc: 0.722892, test acc: 0.728814\n",
      "1984, train loss: 0.592669, test loss: 0.616078, train acc: 0.722892, test acc: 0.728814\n",
      "1985, train loss: 0.592653, test loss: 0.616062, train acc: 0.722892, test acc: 0.728814\n",
      "1986, train loss: 0.592637, test loss: 0.616046, train acc: 0.722892, test acc: 0.728814\n",
      "1987, train loss: 0.592621, test loss: 0.61603, train acc: 0.722892, test acc: 0.728814\n",
      "1988, train loss: 0.592605, test loss: 0.616014, train acc: 0.722892, test acc: 0.728814\n",
      "1989, train loss: 0.592589, test loss: 0.615998, train acc: 0.722892, test acc: 0.728814\n",
      "1990, train loss: 0.592573, test loss: 0.615982, train acc: 0.722892, test acc: 0.728814\n",
      "1991, train loss: 0.592557, test loss: 0.615966, train acc: 0.722892, test acc: 0.728814\n",
      "1992, train loss: 0.592541, test loss: 0.61595, train acc: 0.722892, test acc: 0.728814\n",
      "1993, train loss: 0.592525, test loss: 0.615934, train acc: 0.722892, test acc: 0.728814\n",
      "1994, train loss: 0.592509, test loss: 0.615918, train acc: 0.722892, test acc: 0.728814\n",
      "1995, train loss: 0.592493, test loss: 0.615902, train acc: 0.722892, test acc: 0.728814\n",
      "1996, train loss: 0.592477, test loss: 0.615886, train acc: 0.722892, test acc: 0.728814\n",
      "1997, train loss: 0.592462, test loss: 0.61587, train acc: 0.722892, test acc: 0.728814\n",
      "1998, train loss: 0.592446, test loss: 0.615854, train acc: 0.722892, test acc: 0.728814\n",
      "1999, train loss: 0.59243, test loss: 0.615838, train acc: 0.722892, test acc: 0.728814\n",
      "2000, train loss: 0.592414, test loss: 0.615822, train acc: 0.722892, test acc: 0.728814\n",
      "2001, train loss: 0.592398, test loss: 0.615806, train acc: 0.722892, test acc: 0.728814\n",
      "2002, train loss: 0.592382, test loss: 0.61579, train acc: 0.722892, test acc: 0.728814\n",
      "2003, train loss: 0.592366, test loss: 0.615774, train acc: 0.722892, test acc: 0.728814\n",
      "2004, train loss: 0.59235, test loss: 0.615758, train acc: 0.722892, test acc: 0.728814\n",
      "2005, train loss: 0.592335, test loss: 0.615743, train acc: 0.722892, test acc: 0.728814\n",
      "2006, train loss: 0.592319, test loss: 0.615727, train acc: 0.722892, test acc: 0.728814\n",
      "2007, train loss: 0.592303, test loss: 0.615711, train acc: 0.722892, test acc: 0.728814\n",
      "2008, train loss: 0.592287, test loss: 0.615695, train acc: 0.722892, test acc: 0.728814\n",
      "2009, train loss: 0.592272, test loss: 0.615679, train acc: 0.722892, test acc: 0.728814\n",
      "2010, train loss: 0.592256, test loss: 0.615663, train acc: 0.722892, test acc: 0.728814\n",
      "2011, train loss: 0.59224, test loss: 0.615647, train acc: 0.722892, test acc: 0.728814\n",
      "2012, train loss: 0.592224, test loss: 0.615631, train acc: 0.722892, test acc: 0.728814\n",
      "2013, train loss: 0.592209, test loss: 0.615616, train acc: 0.722892, test acc: 0.728814\n",
      "2014, train loss: 0.592193, test loss: 0.6156, train acc: 0.722892, test acc: 0.728814\n",
      "2015, train loss: 0.592177, test loss: 0.615584, train acc: 0.722892, test acc: 0.728814\n",
      "2016, train loss: 0.592161, test loss: 0.615568, train acc: 0.722892, test acc: 0.728814\n",
      "2017, train loss: 0.592145, test loss: 0.615552, train acc: 0.722892, test acc: 0.728814\n",
      "2018, train loss: 0.59213, test loss: 0.615536, train acc: 0.722892, test acc: 0.728814\n",
      "2019, train loss: 0.592114, test loss: 0.615521, train acc: 0.722892, test acc: 0.728814\n",
      "2020, train loss: 0.592098, test loss: 0.615505, train acc: 0.722892, test acc: 0.728814\n",
      "2021, train loss: 0.592083, test loss: 0.615489, train acc: 0.722892, test acc: 0.728814\n",
      "2022, train loss: 0.592067, test loss: 0.615473, train acc: 0.722892, test acc: 0.728814\n",
      "2023, train loss: 0.592051, test loss: 0.615457, train acc: 0.722892, test acc: 0.728814\n",
      "2024, train loss: 0.592036, test loss: 0.615442, train acc: 0.722892, test acc: 0.728814\n",
      "2025, train loss: 0.59202, test loss: 0.615426, train acc: 0.722892, test acc: 0.728814\n",
      "2026, train loss: 0.592004, test loss: 0.61541, train acc: 0.722892, test acc: 0.728814\n",
      "2027, train loss: 0.591989, test loss: 0.615394, train acc: 0.722892, test acc: 0.728814\n",
      "2028, train loss: 0.591973, test loss: 0.615378, train acc: 0.722892, test acc: 0.728814\n",
      "2029, train loss: 0.591958, test loss: 0.615363, train acc: 0.722892, test acc: 0.728814\n",
      "2030, train loss: 0.591942, test loss: 0.615347, train acc: 0.722892, test acc: 0.728814\n",
      "2031, train loss: 0.591926, test loss: 0.615331, train acc: 0.722892, test acc: 0.728814\n",
      "2032, train loss: 0.591911, test loss: 0.615315, train acc: 0.725904, test acc: 0.728814\n",
      "2033, train loss: 0.591895, test loss: 0.6153, train acc: 0.725904, test acc: 0.728814\n",
      "2034, train loss: 0.591879, test loss: 0.615284, train acc: 0.725904, test acc: 0.728814\n",
      "2035, train loss: 0.591864, test loss: 0.615268, train acc: 0.725904, test acc: 0.728814\n",
      "2036, train loss: 0.591848, test loss: 0.615253, train acc: 0.725904, test acc: 0.728814\n",
      "2037, train loss: 0.591833, test loss: 0.615237, train acc: 0.725904, test acc: 0.728814\n",
      "2038, train loss: 0.591817, test loss: 0.615221, train acc: 0.725904, test acc: 0.728814\n",
      "2039, train loss: 0.591802, test loss: 0.615205, train acc: 0.725904, test acc: 0.728814\n",
      "2040, train loss: 0.591786, test loss: 0.61519, train acc: 0.725904, test acc: 0.728814\n",
      "2041, train loss: 0.591771, test loss: 0.615174, train acc: 0.725904, test acc: 0.728814\n",
      "2042, train loss: 0.591755, test loss: 0.615158, train acc: 0.725904, test acc: 0.728814\n",
      "2043, train loss: 0.59174, test loss: 0.615143, train acc: 0.725904, test acc: 0.728814\n",
      "2044, train loss: 0.591724, test loss: 0.615127, train acc: 0.725904, test acc: 0.728814\n",
      "2045, train loss: 0.591709, test loss: 0.615111, train acc: 0.725904, test acc: 0.728814\n",
      "2046, train loss: 0.591693, test loss: 0.615096, train acc: 0.725904, test acc: 0.728814\n",
      "2047, train loss: 0.591678, test loss: 0.61508, train acc: 0.725904, test acc: 0.728814\n",
      "2048, train loss: 0.591662, test loss: 0.615064, train acc: 0.725904, test acc: 0.728814\n",
      "2049, train loss: 0.591647, test loss: 0.615049, train acc: 0.725904, test acc: 0.728814\n",
      "2050, train loss: 0.591631, test loss: 0.615033, train acc: 0.725904, test acc: 0.728814\n",
      "2051, train loss: 0.591616, test loss: 0.615017, train acc: 0.725904, test acc: 0.728814\n",
      "2052, train loss: 0.5916, test loss: 0.615002, train acc: 0.725904, test acc: 0.728814\n",
      "2053, train loss: 0.591585, test loss: 0.614986, train acc: 0.725904, test acc: 0.728814\n",
      "2054, train loss: 0.591569, test loss: 0.614971, train acc: 0.725904, test acc: 0.728814\n",
      "2055, train loss: 0.591554, test loss: 0.614955, train acc: 0.725904, test acc: 0.728814\n",
      "2056, train loss: 0.591539, test loss: 0.614939, train acc: 0.725904, test acc: 0.728814\n",
      "2057, train loss: 0.591523, test loss: 0.614924, train acc: 0.725904, test acc: 0.728814\n",
      "2058, train loss: 0.591508, test loss: 0.614908, train acc: 0.725904, test acc: 0.728814\n",
      "2059, train loss: 0.591492, test loss: 0.614892, train acc: 0.725904, test acc: 0.728814\n",
      "2060, train loss: 0.591477, test loss: 0.614877, train acc: 0.725904, test acc: 0.728814\n",
      "2061, train loss: 0.591462, test loss: 0.614861, train acc: 0.725904, test acc: 0.728814\n",
      "2062, train loss: 0.591446, test loss: 0.614846, train acc: 0.725904, test acc: 0.728814\n",
      "2063, train loss: 0.591431, test loss: 0.61483, train acc: 0.725904, test acc: 0.728814\n",
      "2064, train loss: 0.591416, test loss: 0.614815, train acc: 0.725904, test acc: 0.728814\n",
      "2065, train loss: 0.5914, test loss: 0.614799, train acc: 0.725904, test acc: 0.728814\n",
      "2066, train loss: 0.591385, test loss: 0.614784, train acc: 0.725904, test acc: 0.728814\n",
      "2067, train loss: 0.59137, test loss: 0.614768, train acc: 0.725904, test acc: 0.728814\n",
      "2068, train loss: 0.591354, test loss: 0.614752, train acc: 0.725904, test acc: 0.728814\n",
      "2069, train loss: 0.591339, test loss: 0.614737, train acc: 0.725904, test acc: 0.728814\n",
      "2070, train loss: 0.591324, test loss: 0.614721, train acc: 0.725904, test acc: 0.728814\n",
      "2071, train loss: 0.591308, test loss: 0.614706, train acc: 0.725904, test acc: 0.728814\n",
      "2072, train loss: 0.591293, test loss: 0.61469, train acc: 0.725904, test acc: 0.728814\n",
      "2073, train loss: 0.591278, test loss: 0.614675, train acc: 0.725904, test acc: 0.728814\n",
      "2074, train loss: 0.591263, test loss: 0.614659, train acc: 0.725904, test acc: 0.728814\n",
      "2075, train loss: 0.591247, test loss: 0.614644, train acc: 0.725904, test acc: 0.728814\n",
      "2076, train loss: 0.591232, test loss: 0.614628, train acc: 0.725904, test acc: 0.728814\n",
      "2077, train loss: 0.591217, test loss: 0.614613, train acc: 0.725904, test acc: 0.728814\n",
      "2078, train loss: 0.591201, test loss: 0.614597, train acc: 0.725904, test acc: 0.728814\n",
      "2079, train loss: 0.591186, test loss: 0.614582, train acc: 0.725904, test acc: 0.728814\n",
      "2080, train loss: 0.591171, test loss: 0.614566, train acc: 0.725904, test acc: 0.728814\n",
      "2081, train loss: 0.591156, test loss: 0.614551, train acc: 0.725904, test acc: 0.728814\n",
      "2082, train loss: 0.591141, test loss: 0.614535, train acc: 0.725904, test acc: 0.728814\n",
      "2083, train loss: 0.591125, test loss: 0.61452, train acc: 0.725904, test acc: 0.728814\n",
      "2084, train loss: 0.59111, test loss: 0.614504, train acc: 0.725904, test acc: 0.728814\n",
      "2085, train loss: 0.591095, test loss: 0.614489, train acc: 0.725904, test acc: 0.728814\n",
      "2086, train loss: 0.59108, test loss: 0.614474, train acc: 0.725904, test acc: 0.728814\n",
      "2087, train loss: 0.591065, test loss: 0.614458, train acc: 0.725904, test acc: 0.728814\n",
      "2088, train loss: 0.591049, test loss: 0.614443, train acc: 0.725904, test acc: 0.728814\n",
      "2089, train loss: 0.591034, test loss: 0.614427, train acc: 0.725904, test acc: 0.728814\n",
      "2090, train loss: 0.591019, test loss: 0.614412, train acc: 0.725904, test acc: 0.728814\n",
      "2091, train loss: 0.591004, test loss: 0.614396, train acc: 0.725904, test acc: 0.728814\n",
      "2092, train loss: 0.590989, test loss: 0.614381, train acc: 0.725904, test acc: 0.728814\n",
      "2093, train loss: 0.590974, test loss: 0.614365, train acc: 0.725904, test acc: 0.728814\n",
      "2094, train loss: 0.590959, test loss: 0.61435, train acc: 0.725904, test acc: 0.728814\n",
      "2095, train loss: 0.590943, test loss: 0.614335, train acc: 0.725904, test acc: 0.728814\n",
      "2096, train loss: 0.590928, test loss: 0.614319, train acc: 0.725904, test acc: 0.728814\n",
      "2097, train loss: 0.590913, test loss: 0.614304, train acc: 0.725904, test acc: 0.728814\n",
      "2098, train loss: 0.590898, test loss: 0.614289, train acc: 0.725904, test acc: 0.728814\n",
      "2099, train loss: 0.590883, test loss: 0.614273, train acc: 0.725904, test acc: 0.728814\n",
      "2100, train loss: 0.590868, test loss: 0.614258, train acc: 0.725904, test acc: 0.728814\n",
      "2101, train loss: 0.590853, test loss: 0.614242, train acc: 0.725904, test acc: 0.728814\n",
      "2102, train loss: 0.590838, test loss: 0.614227, train acc: 0.725904, test acc: 0.728814\n",
      "2103, train loss: 0.590823, test loss: 0.614212, train acc: 0.725904, test acc: 0.728814\n",
      "2104, train loss: 0.590808, test loss: 0.614196, train acc: 0.725904, test acc: 0.728814\n",
      "2105, train loss: 0.590793, test loss: 0.614181, train acc: 0.725904, test acc: 0.728814\n",
      "2106, train loss: 0.590778, test loss: 0.614166, train acc: 0.725904, test acc: 0.728814\n",
      "2107, train loss: 0.590762, test loss: 0.61415, train acc: 0.725904, test acc: 0.728814\n",
      "2108, train loss: 0.590748, test loss: 0.614135, train acc: 0.725904, test acc: 0.728814\n",
      "2109, train loss: 0.590732, test loss: 0.61412, train acc: 0.725904, test acc: 0.728814\n",
      "2110, train loss: 0.590717, test loss: 0.614104, train acc: 0.725904, test acc: 0.728814\n",
      "2111, train loss: 0.590702, test loss: 0.614089, train acc: 0.725904, test acc: 0.728814\n",
      "2112, train loss: 0.590688, test loss: 0.614074, train acc: 0.725904, test acc: 0.728814\n",
      "2113, train loss: 0.590672, test loss: 0.614058, train acc: 0.725904, test acc: 0.728814\n",
      "2114, train loss: 0.590657, test loss: 0.614043, train acc: 0.725904, test acc: 0.728814\n",
      "2115, train loss: 0.590643, test loss: 0.614028, train acc: 0.725904, test acc: 0.728814\n",
      "2116, train loss: 0.590627, test loss: 0.614012, train acc: 0.725904, test acc: 0.728814\n",
      "2117, train loss: 0.590613, test loss: 0.613997, train acc: 0.725904, test acc: 0.728814\n",
      "2118, train loss: 0.590598, test loss: 0.613982, train acc: 0.725904, test acc: 0.728814\n",
      "2119, train loss: 0.590583, test loss: 0.613966, train acc: 0.725904, test acc: 0.728814\n",
      "2120, train loss: 0.590568, test loss: 0.613951, train acc: 0.725904, test acc: 0.728814\n",
      "2121, train loss: 0.590553, test loss: 0.613936, train acc: 0.725904, test acc: 0.728814\n",
      "2122, train loss: 0.590538, test loss: 0.613921, train acc: 0.725904, test acc: 0.728814\n",
      "2123, train loss: 0.590523, test loss: 0.613905, train acc: 0.725904, test acc: 0.728814\n",
      "2124, train loss: 0.590508, test loss: 0.61389, train acc: 0.725904, test acc: 0.728814\n",
      "2125, train loss: 0.590493, test loss: 0.613875, train acc: 0.725904, test acc: 0.728814\n",
      "2126, train loss: 0.590478, test loss: 0.61386, train acc: 0.725904, test acc: 0.728814\n",
      "2127, train loss: 0.590463, test loss: 0.613844, train acc: 0.725904, test acc: 0.728814\n",
      "2128, train loss: 0.590448, test loss: 0.613829, train acc: 0.725904, test acc: 0.728814\n",
      "2129, train loss: 0.590433, test loss: 0.613814, train acc: 0.725904, test acc: 0.728814\n",
      "2130, train loss: 0.590419, test loss: 0.613799, train acc: 0.725904, test acc: 0.728814\n",
      "2131, train loss: 0.590404, test loss: 0.613783, train acc: 0.725904, test acc: 0.728814\n",
      "2132, train loss: 0.590389, test loss: 0.613768, train acc: 0.725904, test acc: 0.728814\n",
      "2133, train loss: 0.590374, test loss: 0.613753, train acc: 0.725904, test acc: 0.728814\n",
      "2134, train loss: 0.590359, test loss: 0.613738, train acc: 0.725904, test acc: 0.728814\n",
      "2135, train loss: 0.590344, test loss: 0.613723, train acc: 0.725904, test acc: 0.728814\n",
      "2136, train loss: 0.59033, test loss: 0.613707, train acc: 0.725904, test acc: 0.728814\n",
      "2137, train loss: 0.590315, test loss: 0.613692, train acc: 0.725904, test acc: 0.728814\n",
      "2138, train loss: 0.5903, test loss: 0.613677, train acc: 0.725904, test acc: 0.728814\n",
      "2139, train loss: 0.590285, test loss: 0.613662, train acc: 0.725904, test acc: 0.728814\n",
      "2140, train loss: 0.59027, test loss: 0.613647, train acc: 0.725904, test acc: 0.728814\n",
      "2141, train loss: 0.590255, test loss: 0.613631, train acc: 0.725904, test acc: 0.728814\n",
      "2142, train loss: 0.590241, test loss: 0.613616, train acc: 0.725904, test acc: 0.728814\n",
      "2143, train loss: 0.590226, test loss: 0.613601, train acc: 0.725904, test acc: 0.728814\n",
      "2144, train loss: 0.590211, test loss: 0.613586, train acc: 0.725904, test acc: 0.728814\n",
      "2145, train loss: 0.590196, test loss: 0.613571, train acc: 0.725904, test acc: 0.728814\n",
      "2146, train loss: 0.590182, test loss: 0.613556, train acc: 0.725904, test acc: 0.728814\n",
      "2147, train loss: 0.590167, test loss: 0.61354, train acc: 0.725904, test acc: 0.728814\n",
      "2148, train loss: 0.590152, test loss: 0.613525, train acc: 0.725904, test acc: 0.728814\n",
      "2149, train loss: 0.590137, test loss: 0.61351, train acc: 0.725904, test acc: 0.728814\n",
      "2150, train loss: 0.590123, test loss: 0.613495, train acc: 0.725904, test acc: 0.728814\n",
      "2151, train loss: 0.590108, test loss: 0.61348, train acc: 0.725904, test acc: 0.728814\n",
      "2152, train loss: 0.590093, test loss: 0.613465, train acc: 0.725904, test acc: 0.728814\n",
      "2153, train loss: 0.590079, test loss: 0.61345, train acc: 0.725904, test acc: 0.728814\n",
      "2154, train loss: 0.590064, test loss: 0.613435, train acc: 0.725904, test acc: 0.728814\n",
      "2155, train loss: 0.590049, test loss: 0.613419, train acc: 0.725904, test acc: 0.728814\n",
      "2156, train loss: 0.590034, test loss: 0.613404, train acc: 0.725904, test acc: 0.728814\n",
      "2157, train loss: 0.59002, test loss: 0.613389, train acc: 0.725904, test acc: 0.728814\n",
      "2158, train loss: 0.590005, test loss: 0.613374, train acc: 0.725904, test acc: 0.728814\n",
      "2159, train loss: 0.58999, test loss: 0.613359, train acc: 0.725904, test acc: 0.728814\n",
      "2160, train loss: 0.589976, test loss: 0.613344, train acc: 0.725904, test acc: 0.728814\n",
      "2161, train loss: 0.589961, test loss: 0.613329, train acc: 0.725904, test acc: 0.728814\n",
      "2162, train loss: 0.589946, test loss: 0.613314, train acc: 0.725904, test acc: 0.728814\n",
      "2163, train loss: 0.589932, test loss: 0.613299, train acc: 0.725904, test acc: 0.728814\n",
      "2164, train loss: 0.589917, test loss: 0.613284, train acc: 0.725904, test acc: 0.728814\n",
      "2165, train loss: 0.589902, test loss: 0.613268, train acc: 0.725904, test acc: 0.728814\n",
      "2166, train loss: 0.589888, test loss: 0.613254, train acc: 0.725904, test acc: 0.728814\n",
      "2167, train loss: 0.589873, test loss: 0.613238, train acc: 0.725904, test acc: 0.728814\n",
      "2168, train loss: 0.589858, test loss: 0.613223, train acc: 0.725904, test acc: 0.728814\n",
      "2169, train loss: 0.589844, test loss: 0.613208, train acc: 0.725904, test acc: 0.728814\n",
      "2170, train loss: 0.589829, test loss: 0.613193, train acc: 0.725904, test acc: 0.728814\n",
      "2171, train loss: 0.589815, test loss: 0.613178, train acc: 0.725904, test acc: 0.728814\n",
      "2172, train loss: 0.5898, test loss: 0.613163, train acc: 0.725904, test acc: 0.728814\n",
      "2173, train loss: 0.589785, test loss: 0.613148, train acc: 0.725904, test acc: 0.728814\n",
      "2174, train loss: 0.589771, test loss: 0.613133, train acc: 0.725904, test acc: 0.728814\n",
      "2175, train loss: 0.589756, test loss: 0.613118, train acc: 0.725904, test acc: 0.728814\n",
      "2176, train loss: 0.589742, test loss: 0.613103, train acc: 0.725904, test acc: 0.728814\n",
      "2177, train loss: 0.589727, test loss: 0.613088, train acc: 0.725904, test acc: 0.728814\n",
      "2178, train loss: 0.589713, test loss: 0.613073, train acc: 0.725904, test acc: 0.728814\n",
      "2179, train loss: 0.589698, test loss: 0.613058, train acc: 0.725904, test acc: 0.728814\n",
      "2180, train loss: 0.589684, test loss: 0.613043, train acc: 0.725904, test acc: 0.728814\n",
      "2181, train loss: 0.589669, test loss: 0.613028, train acc: 0.725904, test acc: 0.728814\n",
      "2182, train loss: 0.589655, test loss: 0.613013, train acc: 0.725904, test acc: 0.728814\n",
      "2183, train loss: 0.58964, test loss: 0.612998, train acc: 0.725904, test acc: 0.728814\n",
      "2184, train loss: 0.589625, test loss: 0.612983, train acc: 0.725904, test acc: 0.728814\n",
      "2185, train loss: 0.589611, test loss: 0.612968, train acc: 0.725904, test acc: 0.728814\n",
      "2186, train loss: 0.589596, test loss: 0.612953, train acc: 0.725904, test acc: 0.728814\n",
      "2187, train loss: 0.589582, test loss: 0.612938, train acc: 0.725904, test acc: 0.728814\n",
      "2188, train loss: 0.589567, test loss: 0.612923, train acc: 0.725904, test acc: 0.728814\n",
      "2189, train loss: 0.589553, test loss: 0.612908, train acc: 0.725904, test acc: 0.728814\n",
      "2190, train loss: 0.589538, test loss: 0.612893, train acc: 0.725904, test acc: 0.728814\n",
      "2191, train loss: 0.589524, test loss: 0.612878, train acc: 0.725904, test acc: 0.728814\n",
      "2192, train loss: 0.58951, test loss: 0.612863, train acc: 0.725904, test acc: 0.728814\n",
      "2193, train loss: 0.589495, test loss: 0.612849, train acc: 0.725904, test acc: 0.728814\n",
      "2194, train loss: 0.589481, test loss: 0.612834, train acc: 0.725904, test acc: 0.728814\n",
      "2195, train loss: 0.589466, test loss: 0.612819, train acc: 0.725904, test acc: 0.728814\n",
      "2196, train loss: 0.589452, test loss: 0.612804, train acc: 0.725904, test acc: 0.728814\n",
      "2197, train loss: 0.589437, test loss: 0.612789, train acc: 0.725904, test acc: 0.728814\n",
      "2198, train loss: 0.589423, test loss: 0.612774, train acc: 0.725904, test acc: 0.728814\n",
      "2199, train loss: 0.589408, test loss: 0.612759, train acc: 0.725904, test acc: 0.728814\n",
      "2200, train loss: 0.589394, test loss: 0.612744, train acc: 0.725904, test acc: 0.728814\n",
      "2201, train loss: 0.58938, test loss: 0.612729, train acc: 0.725904, test acc: 0.728814\n",
      "2202, train loss: 0.589365, test loss: 0.612714, train acc: 0.725904, test acc: 0.728814\n",
      "2203, train loss: 0.589351, test loss: 0.612699, train acc: 0.725904, test acc: 0.728814\n",
      "2204, train loss: 0.589336, test loss: 0.612684, train acc: 0.725904, test acc: 0.728814\n",
      "2205, train loss: 0.589322, test loss: 0.61267, train acc: 0.725904, test acc: 0.728814\n",
      "2206, train loss: 0.589308, test loss: 0.612655, train acc: 0.725904, test acc: 0.728814\n",
      "2207, train loss: 0.589293, test loss: 0.61264, train acc: 0.725904, test acc: 0.728814\n",
      "2208, train loss: 0.589279, test loss: 0.612625, train acc: 0.725904, test acc: 0.728814\n",
      "2209, train loss: 0.589265, test loss: 0.61261, train acc: 0.725904, test acc: 0.728814\n",
      "2210, train loss: 0.58925, test loss: 0.612595, train acc: 0.725904, test acc: 0.728814\n",
      "2211, train loss: 0.589236, test loss: 0.61258, train acc: 0.725904, test acc: 0.728814\n",
      "2212, train loss: 0.589222, test loss: 0.612565, train acc: 0.725904, test acc: 0.728814\n",
      "2213, train loss: 0.589207, test loss: 0.612551, train acc: 0.725904, test acc: 0.728814\n",
      "2214, train loss: 0.589193, test loss: 0.612536, train acc: 0.725904, test acc: 0.728814\n",
      "2215, train loss: 0.589179, test loss: 0.612521, train acc: 0.725904, test acc: 0.728814\n",
      "2216, train loss: 0.589164, test loss: 0.612506, train acc: 0.725904, test acc: 0.728814\n",
      "2217, train loss: 0.58915, test loss: 0.612491, train acc: 0.725904, test acc: 0.728814\n",
      "2218, train loss: 0.589136, test loss: 0.612476, train acc: 0.725904, test acc: 0.728814\n",
      "2219, train loss: 0.589121, test loss: 0.612462, train acc: 0.725904, test acc: 0.728814\n",
      "2220, train loss: 0.589107, test loss: 0.612447, train acc: 0.725904, test acc: 0.728814\n",
      "2221, train loss: 0.589093, test loss: 0.612432, train acc: 0.725904, test acc: 0.728814\n",
      "2222, train loss: 0.589078, test loss: 0.612417, train acc: 0.725904, test acc: 0.728814\n",
      "2223, train loss: 0.589064, test loss: 0.612402, train acc: 0.725904, test acc: 0.728814\n",
      "2224, train loss: 0.58905, test loss: 0.612387, train acc: 0.725904, test acc: 0.728814\n",
      "2225, train loss: 0.589036, test loss: 0.612373, train acc: 0.725904, test acc: 0.728814\n",
      "2226, train loss: 0.589021, test loss: 0.612358, train acc: 0.725904, test acc: 0.728814\n",
      "2227, train loss: 0.589007, test loss: 0.612343, train acc: 0.725904, test acc: 0.728814\n",
      "2228, train loss: 0.588993, test loss: 0.612328, train acc: 0.725904, test acc: 0.728814\n",
      "2229, train loss: 0.588979, test loss: 0.612313, train acc: 0.725904, test acc: 0.728814\n",
      "2230, train loss: 0.588964, test loss: 0.612299, train acc: 0.725904, test acc: 0.728814\n",
      "2231, train loss: 0.58895, test loss: 0.612284, train acc: 0.725904, test acc: 0.728814\n",
      "2232, train loss: 0.588936, test loss: 0.612269, train acc: 0.725904, test acc: 0.728814\n",
      "2233, train loss: 0.588922, test loss: 0.612254, train acc: 0.725904, test acc: 0.728814\n",
      "2234, train loss: 0.588908, test loss: 0.61224, train acc: 0.725904, test acc: 0.728814\n",
      "2235, train loss: 0.588893, test loss: 0.612225, train acc: 0.725904, test acc: 0.728814\n",
      "2236, train loss: 0.588879, test loss: 0.61221, train acc: 0.725904, test acc: 0.728814\n",
      "2237, train loss: 0.588865, test loss: 0.612195, train acc: 0.725904, test acc: 0.728814\n",
      "2238, train loss: 0.588851, test loss: 0.612181, train acc: 0.725904, test acc: 0.728814\n",
      "2239, train loss: 0.588837, test loss: 0.612166, train acc: 0.725904, test acc: 0.728814\n",
      "2240, train loss: 0.588822, test loss: 0.612151, train acc: 0.725904, test acc: 0.728814\n",
      "2241, train loss: 0.588808, test loss: 0.612136, train acc: 0.725904, test acc: 0.728814\n",
      "2242, train loss: 0.588794, test loss: 0.612122, train acc: 0.725904, test acc: 0.728814\n",
      "2243, train loss: 0.58878, test loss: 0.612107, train acc: 0.725904, test acc: 0.728814\n",
      "2244, train loss: 0.588766, test loss: 0.612092, train acc: 0.725904, test acc: 0.728814\n",
      "2245, train loss: 0.588752, test loss: 0.612077, train acc: 0.725904, test acc: 0.728814\n",
      "2246, train loss: 0.588738, test loss: 0.612063, train acc: 0.725904, test acc: 0.728814\n",
      "2247, train loss: 0.588723, test loss: 0.612048, train acc: 0.725904, test acc: 0.728814\n",
      "2248, train loss: 0.588709, test loss: 0.612033, train acc: 0.725904, test acc: 0.728814\n",
      "2249, train loss: 0.588695, test loss: 0.612019, train acc: 0.725904, test acc: 0.728814\n",
      "2250, train loss: 0.588681, test loss: 0.612004, train acc: 0.725904, test acc: 0.728814\n",
      "2251, train loss: 0.588667, test loss: 0.611989, train acc: 0.725904, test acc: 0.728814\n",
      "2252, train loss: 0.588653, test loss: 0.611974, train acc: 0.725904, test acc: 0.728814\n",
      "2253, train loss: 0.588639, test loss: 0.61196, train acc: 0.725904, test acc: 0.728814\n",
      "2254, train loss: 0.588625, test loss: 0.611945, train acc: 0.725904, test acc: 0.728814\n",
      "2255, train loss: 0.588611, test loss: 0.61193, train acc: 0.725904, test acc: 0.728814\n",
      "2256, train loss: 0.588597, test loss: 0.611916, train acc: 0.725904, test acc: 0.728814\n",
      "2257, train loss: 0.588582, test loss: 0.611901, train acc: 0.725904, test acc: 0.728814\n",
      "2258, train loss: 0.588568, test loss: 0.611886, train acc: 0.725904, test acc: 0.728814\n",
      "2259, train loss: 0.588554, test loss: 0.611872, train acc: 0.725904, test acc: 0.728814\n",
      "2260, train loss: 0.58854, test loss: 0.611857, train acc: 0.725904, test acc: 0.728814\n",
      "2261, train loss: 0.588526, test loss: 0.611842, train acc: 0.725904, test acc: 0.728814\n",
      "2262, train loss: 0.588512, test loss: 0.611828, train acc: 0.725904, test acc: 0.728814\n",
      "2263, train loss: 0.588498, test loss: 0.611813, train acc: 0.725904, test acc: 0.728814\n",
      "2264, train loss: 0.588484, test loss: 0.611798, train acc: 0.725904, test acc: 0.728814\n",
      "2265, train loss: 0.58847, test loss: 0.611784, train acc: 0.725904, test acc: 0.728814\n",
      "2266, train loss: 0.588456, test loss: 0.611769, train acc: 0.725904, test acc: 0.728814\n",
      "2267, train loss: 0.588442, test loss: 0.611754, train acc: 0.725904, test acc: 0.728814\n",
      "2268, train loss: 0.588428, test loss: 0.61174, train acc: 0.725904, test acc: 0.728814\n",
      "2269, train loss: 0.588414, test loss: 0.611725, train acc: 0.725904, test acc: 0.728814\n",
      "2270, train loss: 0.5884, test loss: 0.611711, train acc: 0.725904, test acc: 0.728814\n",
      "2271, train loss: 0.588386, test loss: 0.611696, train acc: 0.725904, test acc: 0.728814\n",
      "2272, train loss: 0.588372, test loss: 0.611681, train acc: 0.725904, test acc: 0.728814\n",
      "2273, train loss: 0.588358, test loss: 0.611667, train acc: 0.725904, test acc: 0.728814\n",
      "2274, train loss: 0.588344, test loss: 0.611652, train acc: 0.725904, test acc: 0.728814\n",
      "2275, train loss: 0.58833, test loss: 0.611638, train acc: 0.725904, test acc: 0.728814\n",
      "2276, train loss: 0.588316, test loss: 0.611623, train acc: 0.725904, test acc: 0.728814\n",
      "2277, train loss: 0.588302, test loss: 0.611608, train acc: 0.725904, test acc: 0.728814\n",
      "2278, train loss: 0.588288, test loss: 0.611594, train acc: 0.725904, test acc: 0.728814\n",
      "2279, train loss: 0.588274, test loss: 0.611579, train acc: 0.725904, test acc: 0.728814\n",
      "2280, train loss: 0.58826, test loss: 0.611565, train acc: 0.725904, test acc: 0.728814\n",
      "2281, train loss: 0.588246, test loss: 0.61155, train acc: 0.725904, test acc: 0.728814\n",
      "2282, train loss: 0.588232, test loss: 0.611535, train acc: 0.725904, test acc: 0.728814\n",
      "2283, train loss: 0.588219, test loss: 0.611521, train acc: 0.725904, test acc: 0.728814\n",
      "2284, train loss: 0.588205, test loss: 0.611506, train acc: 0.725904, test acc: 0.728814\n",
      "2285, train loss: 0.588191, test loss: 0.611492, train acc: 0.725904, test acc: 0.728814\n",
      "2286, train loss: 0.588177, test loss: 0.611477, train acc: 0.725904, test acc: 0.728814\n",
      "2287, train loss: 0.588163, test loss: 0.611463, train acc: 0.725904, test acc: 0.728814\n",
      "2288, train loss: 0.588149, test loss: 0.611448, train acc: 0.725904, test acc: 0.728814\n",
      "2289, train loss: 0.588135, test loss: 0.611434, train acc: 0.725904, test acc: 0.728814\n",
      "2290, train loss: 0.588121, test loss: 0.611419, train acc: 0.725904, test acc: 0.728814\n",
      "2291, train loss: 0.588107, test loss: 0.611404, train acc: 0.725904, test acc: 0.728814\n",
      "2292, train loss: 0.588093, test loss: 0.61139, train acc: 0.725904, test acc: 0.728814\n",
      "2293, train loss: 0.58808, test loss: 0.611375, train acc: 0.725904, test acc: 0.728814\n",
      "2294, train loss: 0.588066, test loss: 0.611361, train acc: 0.725904, test acc: 0.728814\n",
      "2295, train loss: 0.588052, test loss: 0.611346, train acc: 0.725904, test acc: 0.728814\n",
      "2296, train loss: 0.588038, test loss: 0.611332, train acc: 0.725904, test acc: 0.728814\n",
      "2297, train loss: 0.588024, test loss: 0.611317, train acc: 0.725904, test acc: 0.728814\n",
      "2298, train loss: 0.58801, test loss: 0.611303, train acc: 0.725904, test acc: 0.728814\n",
      "2299, train loss: 0.587996, test loss: 0.611288, train acc: 0.725904, test acc: 0.728814\n",
      "2300, train loss: 0.587983, test loss: 0.611274, train acc: 0.725904, test acc: 0.728814\n",
      "2301, train loss: 0.587969, test loss: 0.611259, train acc: 0.725904, test acc: 0.728814\n",
      "2302, train loss: 0.587955, test loss: 0.611245, train acc: 0.725904, test acc: 0.728814\n",
      "2303, train loss: 0.587941, test loss: 0.61123, train acc: 0.725904, test acc: 0.728814\n",
      "2304, train loss: 0.587927, test loss: 0.611216, train acc: 0.725904, test acc: 0.728814\n",
      "2305, train loss: 0.587914, test loss: 0.611201, train acc: 0.725904, test acc: 0.728814\n",
      "2306, train loss: 0.5879, test loss: 0.611187, train acc: 0.725904, test acc: 0.728814\n",
      "2307, train loss: 0.587886, test loss: 0.611172, train acc: 0.725904, test acc: 0.728814\n",
      "2308, train loss: 0.587872, test loss: 0.611158, train acc: 0.725904, test acc: 0.728814\n",
      "2309, train loss: 0.587858, test loss: 0.611143, train acc: 0.725904, test acc: 0.728814\n",
      "2310, train loss: 0.587845, test loss: 0.611129, train acc: 0.725904, test acc: 0.728814\n",
      "2311, train loss: 0.587831, test loss: 0.611114, train acc: 0.725904, test acc: 0.728814\n",
      "2312, train loss: 0.587817, test loss: 0.6111, train acc: 0.725904, test acc: 0.728814\n",
      "2313, train loss: 0.587803, test loss: 0.611085, train acc: 0.725904, test acc: 0.728814\n",
      "2314, train loss: 0.58779, test loss: 0.611071, train acc: 0.725904, test acc: 0.728814\n",
      "2315, train loss: 0.587776, test loss: 0.611057, train acc: 0.725904, test acc: 0.728814\n",
      "2316, train loss: 0.587762, test loss: 0.611042, train acc: 0.725904, test acc: 0.728814\n",
      "2317, train loss: 0.587748, test loss: 0.611028, train acc: 0.725904, test acc: 0.728814\n",
      "2318, train loss: 0.587735, test loss: 0.611013, train acc: 0.725904, test acc: 0.728814\n",
      "2319, train loss: 0.587721, test loss: 0.610999, train acc: 0.725904, test acc: 0.728814\n",
      "2320, train loss: 0.587707, test loss: 0.610984, train acc: 0.725904, test acc: 0.728814\n",
      "2321, train loss: 0.587693, test loss: 0.61097, train acc: 0.725904, test acc: 0.728814\n",
      "2322, train loss: 0.58768, test loss: 0.610955, train acc: 0.725904, test acc: 0.728814\n",
      "2323, train loss: 0.587666, test loss: 0.610941, train acc: 0.725904, test acc: 0.728814\n",
      "2324, train loss: 0.587652, test loss: 0.610927, train acc: 0.725904, test acc: 0.728814\n",
      "2325, train loss: 0.587639, test loss: 0.610912, train acc: 0.725904, test acc: 0.728814\n",
      "2326, train loss: 0.587625, test loss: 0.610898, train acc: 0.725904, test acc: 0.728814\n",
      "2327, train loss: 0.587611, test loss: 0.610883, train acc: 0.725904, test acc: 0.728814\n",
      "2328, train loss: 0.587597, test loss: 0.610869, train acc: 0.725904, test acc: 0.728814\n",
      "2329, train loss: 0.587584, test loss: 0.610855, train acc: 0.725904, test acc: 0.728814\n",
      "2330, train loss: 0.58757, test loss: 0.61084, train acc: 0.725904, test acc: 0.728814\n",
      "2331, train loss: 0.587556, test loss: 0.610826, train acc: 0.725904, test acc: 0.728814\n",
      "2332, train loss: 0.587543, test loss: 0.610811, train acc: 0.725904, test acc: 0.728814\n",
      "2333, train loss: 0.587529, test loss: 0.610797, train acc: 0.725904, test acc: 0.728814\n",
      "2334, train loss: 0.587516, test loss: 0.610783, train acc: 0.725904, test acc: 0.728814\n",
      "2335, train loss: 0.587502, test loss: 0.610768, train acc: 0.725904, test acc: 0.728814\n",
      "2336, train loss: 0.587488, test loss: 0.610754, train acc: 0.725904, test acc: 0.728814\n",
      "2337, train loss: 0.587475, test loss: 0.610739, train acc: 0.725904, test acc: 0.728814\n",
      "2338, train loss: 0.587461, test loss: 0.610725, train acc: 0.725904, test acc: 0.728814\n",
      "2339, train loss: 0.587447, test loss: 0.610711, train acc: 0.725904, test acc: 0.728814\n",
      "2340, train loss: 0.587434, test loss: 0.610696, train acc: 0.725904, test acc: 0.728814\n",
      "2341, train loss: 0.58742, test loss: 0.610682, train acc: 0.725904, test acc: 0.728814\n",
      "2342, train loss: 0.587407, test loss: 0.610668, train acc: 0.725904, test acc: 0.728814\n",
      "2343, train loss: 0.587393, test loss: 0.610653, train acc: 0.725904, test acc: 0.728814\n",
      "2344, train loss: 0.587379, test loss: 0.610639, train acc: 0.725904, test acc: 0.728814\n",
      "2345, train loss: 0.587366, test loss: 0.610625, train acc: 0.725904, test acc: 0.728814\n",
      "2346, train loss: 0.587352, test loss: 0.61061, train acc: 0.725904, test acc: 0.728814\n",
      "2347, train loss: 0.587339, test loss: 0.610596, train acc: 0.725904, test acc: 0.728814\n",
      "2348, train loss: 0.587325, test loss: 0.610582, train acc: 0.725904, test acc: 0.728814\n",
      "2349, train loss: 0.587311, test loss: 0.610567, train acc: 0.725904, test acc: 0.728814\n",
      "2350, train loss: 0.587298, test loss: 0.610553, train acc: 0.725904, test acc: 0.728814\n",
      "2351, train loss: 0.587284, test loss: 0.610539, train acc: 0.725904, test acc: 0.728814\n",
      "2352, train loss: 0.587271, test loss: 0.610524, train acc: 0.725904, test acc: 0.728814\n",
      "2353, train loss: 0.587257, test loss: 0.61051, train acc: 0.725904, test acc: 0.728814\n",
      "2354, train loss: 0.587244, test loss: 0.610496, train acc: 0.725904, test acc: 0.728814\n",
      "2355, train loss: 0.58723, test loss: 0.610481, train acc: 0.725904, test acc: 0.728814\n",
      "2356, train loss: 0.587216, test loss: 0.610467, train acc: 0.725904, test acc: 0.728814\n",
      "2357, train loss: 0.587203, test loss: 0.610453, train acc: 0.725904, test acc: 0.728814\n",
      "2358, train loss: 0.587189, test loss: 0.610438, train acc: 0.725904, test acc: 0.728814\n",
      "2359, train loss: 0.587176, test loss: 0.610424, train acc: 0.725904, test acc: 0.728814\n",
      "2360, train loss: 0.587162, test loss: 0.61041, train acc: 0.725904, test acc: 0.728814\n",
      "2361, train loss: 0.587149, test loss: 0.610396, train acc: 0.725904, test acc: 0.728814\n",
      "2362, train loss: 0.587135, test loss: 0.610381, train acc: 0.725904, test acc: 0.728814\n",
      "2363, train loss: 0.587122, test loss: 0.610367, train acc: 0.725904, test acc: 0.728814\n",
      "2364, train loss: 0.587108, test loss: 0.610353, train acc: 0.725904, test acc: 0.728814\n",
      "2365, train loss: 0.587095, test loss: 0.610339, train acc: 0.725904, test acc: 0.728814\n",
      "2366, train loss: 0.587081, test loss: 0.610324, train acc: 0.725904, test acc: 0.728814\n",
      "2367, train loss: 0.587068, test loss: 0.61031, train acc: 0.725904, test acc: 0.728814\n",
      "2368, train loss: 0.587054, test loss: 0.610296, train acc: 0.725904, test acc: 0.728814\n",
      "2369, train loss: 0.587041, test loss: 0.610281, train acc: 0.725904, test acc: 0.728814\n",
      "2370, train loss: 0.587028, test loss: 0.610267, train acc: 0.725904, test acc: 0.728814\n",
      "2371, train loss: 0.587014, test loss: 0.610253, train acc: 0.725904, test acc: 0.728814\n",
      "2372, train loss: 0.587001, test loss: 0.610239, train acc: 0.725904, test acc: 0.728814\n",
      "2373, train loss: 0.586987, test loss: 0.610224, train acc: 0.725904, test acc: 0.728814\n",
      "2374, train loss: 0.586974, test loss: 0.61021, train acc: 0.725904, test acc: 0.728814\n",
      "2375, train loss: 0.58696, test loss: 0.610196, train acc: 0.725904, test acc: 0.728814\n",
      "2376, train loss: 0.586947, test loss: 0.610182, train acc: 0.725904, test acc: 0.728814\n",
      "2377, train loss: 0.586933, test loss: 0.610168, train acc: 0.725904, test acc: 0.728814\n",
      "2378, train loss: 0.58692, test loss: 0.610153, train acc: 0.725904, test acc: 0.728814\n",
      "2379, train loss: 0.586907, test loss: 0.610139, train acc: 0.725904, test acc: 0.728814\n",
      "2380, train loss: 0.586893, test loss: 0.610125, train acc: 0.725904, test acc: 0.728814\n",
      "2381, train loss: 0.58688, test loss: 0.610111, train acc: 0.725904, test acc: 0.728814\n",
      "2382, train loss: 0.586866, test loss: 0.610096, train acc: 0.725904, test acc: 0.728814\n",
      "2383, train loss: 0.586853, test loss: 0.610082, train acc: 0.725904, test acc: 0.728814\n",
      "2384, train loss: 0.586839, test loss: 0.610068, train acc: 0.725904, test acc: 0.728814\n",
      "2385, train loss: 0.586826, test loss: 0.610054, train acc: 0.725904, test acc: 0.728814\n",
      "2386, train loss: 0.586813, test loss: 0.610039, train acc: 0.725904, test acc: 0.728814\n",
      "2387, train loss: 0.586799, test loss: 0.610025, train acc: 0.725904, test acc: 0.728814\n",
      "2388, train loss: 0.586786, test loss: 0.610011, train acc: 0.725904, test acc: 0.728814\n",
      "2389, train loss: 0.586773, test loss: 0.609997, train acc: 0.725904, test acc: 0.728814\n",
      "2390, train loss: 0.586759, test loss: 0.609983, train acc: 0.725904, test acc: 0.728814\n",
      "2391, train loss: 0.586746, test loss: 0.609969, train acc: 0.725904, test acc: 0.728814\n",
      "2392, train loss: 0.586732, test loss: 0.609954, train acc: 0.725904, test acc: 0.728814\n",
      "2393, train loss: 0.586719, test loss: 0.60994, train acc: 0.725904, test acc: 0.728814\n",
      "2394, train loss: 0.586706, test loss: 0.609926, train acc: 0.725904, test acc: 0.728814\n",
      "2395, train loss: 0.586692, test loss: 0.609912, train acc: 0.725904, test acc: 0.728814\n",
      "2396, train loss: 0.586679, test loss: 0.609898, train acc: 0.725904, test acc: 0.728814\n",
      "2397, train loss: 0.586666, test loss: 0.609884, train acc: 0.725904, test acc: 0.728814\n",
      "2398, train loss: 0.586652, test loss: 0.609869, train acc: 0.725904, test acc: 0.728814\n",
      "2399, train loss: 0.586639, test loss: 0.609855, train acc: 0.725904, test acc: 0.728814\n",
      "2400, train loss: 0.586626, test loss: 0.609841, train acc: 0.725904, test acc: 0.728814\n",
      "2401, train loss: 0.586612, test loss: 0.609827, train acc: 0.725904, test acc: 0.728814\n",
      "2402, train loss: 0.586599, test loss: 0.609813, train acc: 0.725904, test acc: 0.728814\n",
      "2403, train loss: 0.586586, test loss: 0.609798, train acc: 0.725904, test acc: 0.728814\n",
      "2404, train loss: 0.586573, test loss: 0.609784, train acc: 0.725904, test acc: 0.728814\n",
      "2405, train loss: 0.586559, test loss: 0.60977, train acc: 0.725904, test acc: 0.728814\n",
      "2406, train loss: 0.586546, test loss: 0.609756, train acc: 0.725904, test acc: 0.728814\n",
      "2407, train loss: 0.586533, test loss: 0.609742, train acc: 0.725904, test acc: 0.728814\n",
      "2408, train loss: 0.586519, test loss: 0.609728, train acc: 0.725904, test acc: 0.728814\n",
      "2409, train loss: 0.586506, test loss: 0.609714, train acc: 0.725904, test acc: 0.728814\n",
      "2410, train loss: 0.586493, test loss: 0.6097, train acc: 0.725904, test acc: 0.728814\n",
      "2411, train loss: 0.58648, test loss: 0.609685, train acc: 0.725904, test acc: 0.728814\n",
      "2412, train loss: 0.586466, test loss: 0.609671, train acc: 0.725904, test acc: 0.728814\n",
      "2413, train loss: 0.586453, test loss: 0.609657, train acc: 0.725904, test acc: 0.728814\n",
      "2414, train loss: 0.58644, test loss: 0.609643, train acc: 0.725904, test acc: 0.728814\n",
      "2415, train loss: 0.586427, test loss: 0.609629, train acc: 0.725904, test acc: 0.728814\n",
      "2416, train loss: 0.586413, test loss: 0.609615, train acc: 0.725904, test acc: 0.728814\n",
      "2417, train loss: 0.5864, test loss: 0.609601, train acc: 0.725904, test acc: 0.728814\n",
      "2418, train loss: 0.586387, test loss: 0.609587, train acc: 0.725904, test acc: 0.728814\n",
      "2419, train loss: 0.586374, test loss: 0.609573, train acc: 0.725904, test acc: 0.728814\n",
      "2420, train loss: 0.58636, test loss: 0.609559, train acc: 0.725904, test acc: 0.728814\n",
      "2421, train loss: 0.586347, test loss: 0.609544, train acc: 0.725904, test acc: 0.728814\n",
      "2422, train loss: 0.586334, test loss: 0.60953, train acc: 0.725904, test acc: 0.728814\n",
      "2423, train loss: 0.586321, test loss: 0.609516, train acc: 0.725904, test acc: 0.728814\n",
      "2424, train loss: 0.586308, test loss: 0.609502, train acc: 0.725904, test acc: 0.728814\n",
      "2425, train loss: 0.586294, test loss: 0.609488, train acc: 0.725904, test acc: 0.728814\n",
      "2426, train loss: 0.586281, test loss: 0.609474, train acc: 0.725904, test acc: 0.728814\n",
      "2427, train loss: 0.586268, test loss: 0.60946, train acc: 0.725904, test acc: 0.728814\n",
      "2428, train loss: 0.586255, test loss: 0.609446, train acc: 0.725904, test acc: 0.728814\n",
      "2429, train loss: 0.586242, test loss: 0.609432, train acc: 0.725904, test acc: 0.728814\n",
      "2430, train loss: 0.586228, test loss: 0.609418, train acc: 0.725904, test acc: 0.728814\n",
      "2431, train loss: 0.586215, test loss: 0.609404, train acc: 0.725904, test acc: 0.728814\n",
      "2432, train loss: 0.586202, test loss: 0.60939, train acc: 0.725904, test acc: 0.728814\n",
      "2433, train loss: 0.586189, test loss: 0.609376, train acc: 0.725904, test acc: 0.728814\n",
      "2434, train loss: 0.586176, test loss: 0.609362, train acc: 0.725904, test acc: 0.728814\n",
      "2435, train loss: 0.586163, test loss: 0.609348, train acc: 0.725904, test acc: 0.728814\n",
      "2436, train loss: 0.586149, test loss: 0.609334, train acc: 0.725904, test acc: 0.728814\n",
      "2437, train loss: 0.586136, test loss: 0.609319, train acc: 0.725904, test acc: 0.728814\n",
      "2438, train loss: 0.586123, test loss: 0.609305, train acc: 0.725904, test acc: 0.728814\n",
      "2439, train loss: 0.58611, test loss: 0.609291, train acc: 0.725904, test acc: 0.728814\n",
      "2440, train loss: 0.586097, test loss: 0.609277, train acc: 0.725904, test acc: 0.728814\n",
      "2441, train loss: 0.586084, test loss: 0.609263, train acc: 0.725904, test acc: 0.728814\n",
      "2442, train loss: 0.586071, test loss: 0.609249, train acc: 0.725904, test acc: 0.728814\n",
      "2443, train loss: 0.586057, test loss: 0.609235, train acc: 0.725904, test acc: 0.728814\n",
      "2444, train loss: 0.586044, test loss: 0.609221, train acc: 0.725904, test acc: 0.728814\n",
      "2445, train loss: 0.586031, test loss: 0.609207, train acc: 0.725904, test acc: 0.728814\n",
      "2446, train loss: 0.586018, test loss: 0.609193, train acc: 0.725904, test acc: 0.728814\n",
      "2447, train loss: 0.586005, test loss: 0.609179, train acc: 0.725904, test acc: 0.728814\n",
      "2448, train loss: 0.585992, test loss: 0.609165, train acc: 0.725904, test acc: 0.728814\n",
      "2449, train loss: 0.585979, test loss: 0.609151, train acc: 0.725904, test acc: 0.728814\n",
      "2450, train loss: 0.585966, test loss: 0.609137, train acc: 0.725904, test acc: 0.728814\n",
      "2451, train loss: 0.585953, test loss: 0.609123, train acc: 0.725904, test acc: 0.728814\n",
      "2452, train loss: 0.58594, test loss: 0.609109, train acc: 0.725904, test acc: 0.728814\n",
      "2453, train loss: 0.585927, test loss: 0.609095, train acc: 0.725904, test acc: 0.728814\n",
      "2454, train loss: 0.585913, test loss: 0.609081, train acc: 0.725904, test acc: 0.728814\n",
      "2455, train loss: 0.5859, test loss: 0.609067, train acc: 0.725904, test acc: 0.728814\n",
      "2456, train loss: 0.585887, test loss: 0.609053, train acc: 0.725904, test acc: 0.728814\n",
      "2457, train loss: 0.585874, test loss: 0.609039, train acc: 0.725904, test acc: 0.728814\n",
      "2458, train loss: 0.585861, test loss: 0.609025, train acc: 0.725904, test acc: 0.728814\n",
      "2459, train loss: 0.585848, test loss: 0.609011, train acc: 0.725904, test acc: 0.728814\n",
      "2460, train loss: 0.585835, test loss: 0.608997, train acc: 0.725904, test acc: 0.728814\n",
      "2461, train loss: 0.585822, test loss: 0.608984, train acc: 0.725904, test acc: 0.728814\n",
      "2462, train loss: 0.585809, test loss: 0.608969, train acc: 0.725904, test acc: 0.728814\n",
      "2463, train loss: 0.585796, test loss: 0.608956, train acc: 0.725904, test acc: 0.728814\n",
      "2464, train loss: 0.585783, test loss: 0.608942, train acc: 0.725904, test acc: 0.728814\n",
      "2465, train loss: 0.58577, test loss: 0.608928, train acc: 0.725904, test acc: 0.728814\n",
      "2466, train loss: 0.585757, test loss: 0.608914, train acc: 0.725904, test acc: 0.728814\n",
      "2467, train loss: 0.585744, test loss: 0.6089, train acc: 0.725904, test acc: 0.728814\n",
      "2468, train loss: 0.585731, test loss: 0.608886, train acc: 0.725904, test acc: 0.728814\n",
      "2469, train loss: 0.585718, test loss: 0.608872, train acc: 0.725904, test acc: 0.728814\n",
      "2470, train loss: 0.585705, test loss: 0.608858, train acc: 0.725904, test acc: 0.728814\n",
      "2471, train loss: 0.585692, test loss: 0.608844, train acc: 0.725904, test acc: 0.728814\n",
      "2472, train loss: 0.585679, test loss: 0.60883, train acc: 0.725904, test acc: 0.728814\n",
      "2473, train loss: 0.585666, test loss: 0.608816, train acc: 0.725904, test acc: 0.728814\n",
      "2474, train loss: 0.585653, test loss: 0.608802, train acc: 0.725904, test acc: 0.728814\n",
      "2475, train loss: 0.58564, test loss: 0.608788, train acc: 0.725904, test acc: 0.728814\n",
      "2476, train loss: 0.585627, test loss: 0.608774, train acc: 0.725904, test acc: 0.728814\n",
      "2477, train loss: 0.585614, test loss: 0.608761, train acc: 0.725904, test acc: 0.745763\n",
      "2478, train loss: 0.585601, test loss: 0.608747, train acc: 0.725904, test acc: 0.745763\n",
      "2479, train loss: 0.585588, test loss: 0.608733, train acc: 0.725904, test acc: 0.745763\n",
      "2480, train loss: 0.585575, test loss: 0.608719, train acc: 0.725904, test acc: 0.745763\n",
      "2481, train loss: 0.585562, test loss: 0.608705, train acc: 0.725904, test acc: 0.745763\n",
      "2482, train loss: 0.585549, test loss: 0.608691, train acc: 0.725904, test acc: 0.745763\n",
      "2483, train loss: 0.585536, test loss: 0.608677, train acc: 0.725904, test acc: 0.745763\n",
      "2484, train loss: 0.585523, test loss: 0.608663, train acc: 0.725904, test acc: 0.745763\n",
      "2485, train loss: 0.585511, test loss: 0.608649, train acc: 0.725904, test acc: 0.745763\n",
      "2486, train loss: 0.585498, test loss: 0.608635, train acc: 0.725904, test acc: 0.745763\n",
      "2487, train loss: 0.585485, test loss: 0.608621, train acc: 0.725904, test acc: 0.745763\n",
      "2488, train loss: 0.585472, test loss: 0.608608, train acc: 0.725904, test acc: 0.745763\n",
      "2489, train loss: 0.585459, test loss: 0.608594, train acc: 0.725904, test acc: 0.745763\n",
      "2490, train loss: 0.585446, test loss: 0.60858, train acc: 0.725904, test acc: 0.745763\n",
      "2491, train loss: 0.585433, test loss: 0.608566, train acc: 0.725904, test acc: 0.745763\n",
      "2492, train loss: 0.58542, test loss: 0.608552, train acc: 0.725904, test acc: 0.745763\n",
      "2493, train loss: 0.585407, test loss: 0.608538, train acc: 0.725904, test acc: 0.745763\n",
      "2494, train loss: 0.585394, test loss: 0.608524, train acc: 0.725904, test acc: 0.745763\n",
      "2495, train loss: 0.585381, test loss: 0.60851, train acc: 0.725904, test acc: 0.745763\n",
      "2496, train loss: 0.585369, test loss: 0.608497, train acc: 0.725904, test acc: 0.745763\n",
      "2497, train loss: 0.585356, test loss: 0.608483, train acc: 0.725904, test acc: 0.745763\n",
      "2498, train loss: 0.585343, test loss: 0.608469, train acc: 0.725904, test acc: 0.745763\n",
      "2499, train loss: 0.58533, test loss: 0.608455, train acc: 0.725904, test acc: 0.745763\n",
      "2500, train loss: 0.585317, test loss: 0.608441, train acc: 0.725904, test acc: 0.745763\n",
      "2501, train loss: 0.585304, test loss: 0.608427, train acc: 0.725904, test acc: 0.745763\n",
      "2502, train loss: 0.585291, test loss: 0.608414, train acc: 0.725904, test acc: 0.745763\n",
      "2503, train loss: 0.585278, test loss: 0.6084, train acc: 0.725904, test acc: 0.745763\n",
      "2504, train loss: 0.585266, test loss: 0.608386, train acc: 0.725904, test acc: 0.745763\n",
      "2505, train loss: 0.585253, test loss: 0.608372, train acc: 0.725904, test acc: 0.745763\n",
      "2506, train loss: 0.58524, test loss: 0.608358, train acc: 0.725904, test acc: 0.745763\n",
      "2507, train loss: 0.585227, test loss: 0.608344, train acc: 0.725904, test acc: 0.745763\n",
      "2508, train loss: 0.585214, test loss: 0.608331, train acc: 0.725904, test acc: 0.745763\n",
      "2509, train loss: 0.585201, test loss: 0.608317, train acc: 0.725904, test acc: 0.745763\n",
      "2510, train loss: 0.585189, test loss: 0.608303, train acc: 0.725904, test acc: 0.745763\n",
      "2511, train loss: 0.585176, test loss: 0.608289, train acc: 0.725904, test acc: 0.745763\n",
      "2512, train loss: 0.585163, test loss: 0.608275, train acc: 0.725904, test acc: 0.745763\n",
      "2513, train loss: 0.58515, test loss: 0.608262, train acc: 0.725904, test acc: 0.745763\n",
      "2514, train loss: 0.585137, test loss: 0.608248, train acc: 0.725904, test acc: 0.745763\n",
      "2515, train loss: 0.585125, test loss: 0.608234, train acc: 0.725904, test acc: 0.745763\n",
      "2516, train loss: 0.585112, test loss: 0.60822, train acc: 0.725904, test acc: 0.745763\n",
      "2517, train loss: 0.585099, test loss: 0.608206, train acc: 0.725904, test acc: 0.745763\n",
      "2518, train loss: 0.585086, test loss: 0.608193, train acc: 0.725904, test acc: 0.745763\n",
      "2519, train loss: 0.585073, test loss: 0.608179, train acc: 0.725904, test acc: 0.745763\n",
      "2520, train loss: 0.585061, test loss: 0.608165, train acc: 0.725904, test acc: 0.745763\n",
      "2521, train loss: 0.585048, test loss: 0.608151, train acc: 0.725904, test acc: 0.745763\n",
      "2522, train loss: 0.585035, test loss: 0.608137, train acc: 0.725904, test acc: 0.745763\n",
      "2523, train loss: 0.585022, test loss: 0.608124, train acc: 0.725904, test acc: 0.745763\n",
      "2524, train loss: 0.58501, test loss: 0.60811, train acc: 0.725904, test acc: 0.745763\n",
      "2525, train loss: 0.584997, test loss: 0.608096, train acc: 0.725904, test acc: 0.745763\n",
      "2526, train loss: 0.584984, test loss: 0.608082, train acc: 0.725904, test acc: 0.745763\n",
      "2527, train loss: 0.584971, test loss: 0.608068, train acc: 0.725904, test acc: 0.745763\n",
      "2528, train loss: 0.584959, test loss: 0.608055, train acc: 0.725904, test acc: 0.745763\n",
      "2529, train loss: 0.584946, test loss: 0.608041, train acc: 0.725904, test acc: 0.745763\n",
      "2530, train loss: 0.584933, test loss: 0.608027, train acc: 0.725904, test acc: 0.745763\n",
      "2531, train loss: 0.58492, test loss: 0.608013, train acc: 0.725904, test acc: 0.745763\n",
      "2532, train loss: 0.584908, test loss: 0.608, train acc: 0.725904, test acc: 0.745763\n",
      "2533, train loss: 0.584895, test loss: 0.607986, train acc: 0.725904, test acc: 0.745763\n",
      "2534, train loss: 0.584882, test loss: 0.607972, train acc: 0.725904, test acc: 0.745763\n",
      "2535, train loss: 0.584869, test loss: 0.607958, train acc: 0.725904, test acc: 0.745763\n",
      "2536, train loss: 0.584857, test loss: 0.607945, train acc: 0.725904, test acc: 0.745763\n",
      "2537, train loss: 0.584844, test loss: 0.607931, train acc: 0.725904, test acc: 0.745763\n",
      "2538, train loss: 0.584831, test loss: 0.607917, train acc: 0.725904, test acc: 0.745763\n",
      "2539, train loss: 0.584819, test loss: 0.607903, train acc: 0.725904, test acc: 0.745763\n",
      "2540, train loss: 0.584806, test loss: 0.60789, train acc: 0.725904, test acc: 0.745763\n",
      "2541, train loss: 0.584793, test loss: 0.607876, train acc: 0.725904, test acc: 0.745763\n",
      "2542, train loss: 0.58478, test loss: 0.607862, train acc: 0.725904, test acc: 0.745763\n",
      "2543, train loss: 0.584768, test loss: 0.607849, train acc: 0.725904, test acc: 0.745763\n",
      "2544, train loss: 0.584755, test loss: 0.607835, train acc: 0.725904, test acc: 0.745763\n",
      "2545, train loss: 0.584742, test loss: 0.607821, train acc: 0.725904, test acc: 0.745763\n",
      "2546, train loss: 0.58473, test loss: 0.607807, train acc: 0.725904, test acc: 0.745763\n",
      "2547, train loss: 0.584717, test loss: 0.607794, train acc: 0.725904, test acc: 0.745763\n",
      "2548, train loss: 0.584704, test loss: 0.60778, train acc: 0.725904, test acc: 0.745763\n",
      "2549, train loss: 0.584692, test loss: 0.607766, train acc: 0.725904, test acc: 0.745763\n",
      "2550, train loss: 0.584679, test loss: 0.607753, train acc: 0.725904, test acc: 0.745763\n",
      "2551, train loss: 0.584666, test loss: 0.607739, train acc: 0.725904, test acc: 0.745763\n",
      "2552, train loss: 0.584654, test loss: 0.607725, train acc: 0.725904, test acc: 0.745763\n",
      "2553, train loss: 0.584641, test loss: 0.607711, train acc: 0.725904, test acc: 0.745763\n",
      "2554, train loss: 0.584628, test loss: 0.607698, train acc: 0.725904, test acc: 0.745763\n",
      "2555, train loss: 0.584616, test loss: 0.607684, train acc: 0.725904, test acc: 0.745763\n",
      "2556, train loss: 0.584603, test loss: 0.60767, train acc: 0.725904, test acc: 0.745763\n",
      "2557, train loss: 0.584591, test loss: 0.607657, train acc: 0.725904, test acc: 0.745763\n",
      "2558, train loss: 0.584578, test loss: 0.607643, train acc: 0.725904, test acc: 0.745763\n",
      "2559, train loss: 0.584565, test loss: 0.607629, train acc: 0.725904, test acc: 0.745763\n",
      "2560, train loss: 0.584553, test loss: 0.607616, train acc: 0.725904, test acc: 0.745763\n",
      "2561, train loss: 0.58454, test loss: 0.607602, train acc: 0.725904, test acc: 0.745763\n",
      "2562, train loss: 0.584527, test loss: 0.607588, train acc: 0.725904, test acc: 0.745763\n",
      "2563, train loss: 0.584515, test loss: 0.607575, train acc: 0.725904, test acc: 0.745763\n",
      "2564, train loss: 0.584502, test loss: 0.607561, train acc: 0.725904, test acc: 0.745763\n",
      "2565, train loss: 0.58449, test loss: 0.607547, train acc: 0.725904, test acc: 0.745763\n",
      "2566, train loss: 0.584477, test loss: 0.607534, train acc: 0.725904, test acc: 0.745763\n",
      "2567, train loss: 0.584464, test loss: 0.60752, train acc: 0.725904, test acc: 0.745763\n",
      "2568, train loss: 0.584452, test loss: 0.607506, train acc: 0.725904, test acc: 0.745763\n",
      "2569, train loss: 0.584439, test loss: 0.607493, train acc: 0.725904, test acc: 0.745763\n",
      "2570, train loss: 0.584427, test loss: 0.607479, train acc: 0.725904, test acc: 0.745763\n",
      "2571, train loss: 0.584414, test loss: 0.607465, train acc: 0.725904, test acc: 0.745763\n",
      "2572, train loss: 0.584401, test loss: 0.607452, train acc: 0.725904, test acc: 0.745763\n",
      "2573, train loss: 0.584389, test loss: 0.607438, train acc: 0.725904, test acc: 0.745763\n",
      "2574, train loss: 0.584376, test loss: 0.607424, train acc: 0.725904, test acc: 0.745763\n",
      "2575, train loss: 0.584364, test loss: 0.607411, train acc: 0.725904, test acc: 0.745763\n",
      "2576, train loss: 0.584351, test loss: 0.607397, train acc: 0.725904, test acc: 0.745763\n",
      "2577, train loss: 0.584339, test loss: 0.607384, train acc: 0.725904, test acc: 0.745763\n",
      "2578, train loss: 0.584326, test loss: 0.60737, train acc: 0.725904, test acc: 0.745763\n",
      "2579, train loss: 0.584314, test loss: 0.607356, train acc: 0.725904, test acc: 0.745763\n",
      "2580, train loss: 0.584301, test loss: 0.607343, train acc: 0.725904, test acc: 0.745763\n",
      "2581, train loss: 0.584289, test loss: 0.607329, train acc: 0.725904, test acc: 0.745763\n",
      "2582, train loss: 0.584276, test loss: 0.607315, train acc: 0.725904, test acc: 0.745763\n",
      "2583, train loss: 0.584264, test loss: 0.607302, train acc: 0.725904, test acc: 0.745763\n",
      "2584, train loss: 0.584251, test loss: 0.607288, train acc: 0.725904, test acc: 0.745763\n",
      "2585, train loss: 0.584238, test loss: 0.607275, train acc: 0.725904, test acc: 0.745763\n",
      "2586, train loss: 0.584226, test loss: 0.607261, train acc: 0.725904, test acc: 0.745763\n",
      "2587, train loss: 0.584213, test loss: 0.607247, train acc: 0.725904, test acc: 0.745763\n",
      "2588, train loss: 0.584201, test loss: 0.607234, train acc: 0.725904, test acc: 0.745763\n",
      "2589, train loss: 0.584188, test loss: 0.60722, train acc: 0.725904, test acc: 0.745763\n",
      "2590, train loss: 0.584176, test loss: 0.607207, train acc: 0.725904, test acc: 0.745763\n",
      "2591, train loss: 0.584163, test loss: 0.607193, train acc: 0.725904, test acc: 0.745763\n",
      "2592, train loss: 0.584151, test loss: 0.607179, train acc: 0.725904, test acc: 0.745763\n",
      "2593, train loss: 0.584138, test loss: 0.607166, train acc: 0.725904, test acc: 0.745763\n",
      "2594, train loss: 0.584126, test loss: 0.607152, train acc: 0.725904, test acc: 0.745763\n",
      "2595, train loss: 0.584113, test loss: 0.607139, train acc: 0.725904, test acc: 0.745763\n",
      "2596, train loss: 0.584101, test loss: 0.607125, train acc: 0.725904, test acc: 0.745763\n",
      "2597, train loss: 0.584089, test loss: 0.607112, train acc: 0.725904, test acc: 0.745763\n",
      "2598, train loss: 0.584076, test loss: 0.607098, train acc: 0.725904, test acc: 0.745763\n",
      "2599, train loss: 0.584064, test loss: 0.607084, train acc: 0.725904, test acc: 0.745763\n",
      "2600, train loss: 0.584051, test loss: 0.607071, train acc: 0.725904, test acc: 0.745763\n",
      "2601, train loss: 0.584039, test loss: 0.607057, train acc: 0.725904, test acc: 0.745763\n",
      "2602, train loss: 0.584026, test loss: 0.607044, train acc: 0.725904, test acc: 0.745763\n",
      "2603, train loss: 0.584014, test loss: 0.60703, train acc: 0.725904, test acc: 0.745763\n",
      "2604, train loss: 0.584001, test loss: 0.607017, train acc: 0.725904, test acc: 0.745763\n",
      "2605, train loss: 0.583989, test loss: 0.607003, train acc: 0.725904, test acc: 0.745763\n",
      "2606, train loss: 0.583976, test loss: 0.606989, train acc: 0.725904, test acc: 0.745763\n",
      "2607, train loss: 0.583964, test loss: 0.606976, train acc: 0.725904, test acc: 0.745763\n",
      "2608, train loss: 0.583952, test loss: 0.606962, train acc: 0.725904, test acc: 0.745763\n",
      "2609, train loss: 0.583939, test loss: 0.606949, train acc: 0.725904, test acc: 0.745763\n",
      "2610, train loss: 0.583927, test loss: 0.606935, train acc: 0.725904, test acc: 0.745763\n",
      "2611, train loss: 0.583914, test loss: 0.606922, train acc: 0.725904, test acc: 0.745763\n",
      "2612, train loss: 0.583902, test loss: 0.606908, train acc: 0.725904, test acc: 0.745763\n",
      "2613, train loss: 0.583889, test loss: 0.606895, train acc: 0.725904, test acc: 0.745763\n",
      "2614, train loss: 0.583877, test loss: 0.606881, train acc: 0.725904, test acc: 0.745763\n",
      "2615, train loss: 0.583865, test loss: 0.606868, train acc: 0.725904, test acc: 0.745763\n",
      "2616, train loss: 0.583852, test loss: 0.606854, train acc: 0.725904, test acc: 0.745763\n",
      "2617, train loss: 0.58384, test loss: 0.606841, train acc: 0.725904, test acc: 0.745763\n",
      "2618, train loss: 0.583827, test loss: 0.606827, train acc: 0.725904, test acc: 0.745763\n",
      "2619, train loss: 0.583815, test loss: 0.606813, train acc: 0.725904, test acc: 0.745763\n",
      "2620, train loss: 0.583803, test loss: 0.6068, train acc: 0.725904, test acc: 0.745763\n",
      "2621, train loss: 0.58379, test loss: 0.606786, train acc: 0.725904, test acc: 0.745763\n",
      "2622, train loss: 0.583778, test loss: 0.606773, train acc: 0.725904, test acc: 0.745763\n",
      "2623, train loss: 0.583766, test loss: 0.606759, train acc: 0.725904, test acc: 0.745763\n",
      "2624, train loss: 0.583753, test loss: 0.606746, train acc: 0.725904, test acc: 0.745763\n",
      "2625, train loss: 0.583741, test loss: 0.606732, train acc: 0.725904, test acc: 0.745763\n",
      "2626, train loss: 0.583728, test loss: 0.606719, train acc: 0.725904, test acc: 0.745763\n",
      "2627, train loss: 0.583716, test loss: 0.606705, train acc: 0.725904, test acc: 0.745763\n",
      "2628, train loss: 0.583704, test loss: 0.606692, train acc: 0.725904, test acc: 0.745763\n",
      "2629, train loss: 0.583691, test loss: 0.606678, train acc: 0.725904, test acc: 0.745763\n",
      "2630, train loss: 0.583679, test loss: 0.606665, train acc: 0.725904, test acc: 0.745763\n",
      "2631, train loss: 0.583667, test loss: 0.606651, train acc: 0.725904, test acc: 0.745763\n",
      "2632, train loss: 0.583654, test loss: 0.606638, train acc: 0.725904, test acc: 0.745763\n",
      "2633, train loss: 0.583642, test loss: 0.606625, train acc: 0.725904, test acc: 0.745763\n",
      "2634, train loss: 0.58363, test loss: 0.606611, train acc: 0.725904, test acc: 0.745763\n",
      "2635, train loss: 0.583617, test loss: 0.606598, train acc: 0.725904, test acc: 0.745763\n",
      "2636, train loss: 0.583605, test loss: 0.606584, train acc: 0.725904, test acc: 0.745763\n",
      "2637, train loss: 0.583593, test loss: 0.606571, train acc: 0.725904, test acc: 0.745763\n",
      "2638, train loss: 0.58358, test loss: 0.606557, train acc: 0.725904, test acc: 0.745763\n",
      "2639, train loss: 0.583568, test loss: 0.606544, train acc: 0.725904, test acc: 0.745763\n",
      "2640, train loss: 0.583556, test loss: 0.60653, train acc: 0.725904, test acc: 0.745763\n",
      "2641, train loss: 0.583543, test loss: 0.606517, train acc: 0.725904, test acc: 0.745763\n",
      "2642, train loss: 0.583531, test loss: 0.606503, train acc: 0.725904, test acc: 0.745763\n",
      "2643, train loss: 0.583519, test loss: 0.60649, train acc: 0.725904, test acc: 0.745763\n",
      "2644, train loss: 0.583506, test loss: 0.606476, train acc: 0.725904, test acc: 0.745763\n",
      "2645, train loss: 0.583494, test loss: 0.606463, train acc: 0.725904, test acc: 0.745763\n",
      "2646, train loss: 0.583482, test loss: 0.606449, train acc: 0.725904, test acc: 0.745763\n",
      "2647, train loss: 0.58347, test loss: 0.606436, train acc: 0.725904, test acc: 0.745763\n",
      "2648, train loss: 0.583457, test loss: 0.606422, train acc: 0.725904, test acc: 0.745763\n",
      "2649, train loss: 0.583445, test loss: 0.606409, train acc: 0.725904, test acc: 0.745763\n",
      "2650, train loss: 0.583433, test loss: 0.606396, train acc: 0.725904, test acc: 0.745763\n",
      "2651, train loss: 0.583421, test loss: 0.606382, train acc: 0.725904, test acc: 0.745763\n",
      "2652, train loss: 0.583408, test loss: 0.606369, train acc: 0.725904, test acc: 0.745763\n",
      "2653, train loss: 0.583396, test loss: 0.606355, train acc: 0.725904, test acc: 0.745763\n",
      "2654, train loss: 0.583384, test loss: 0.606342, train acc: 0.725904, test acc: 0.745763\n",
      "2655, train loss: 0.583371, test loss: 0.606328, train acc: 0.725904, test acc: 0.745763\n",
      "2656, train loss: 0.583359, test loss: 0.606315, train acc: 0.725904, test acc: 0.745763\n",
      "2657, train loss: 0.583347, test loss: 0.606302, train acc: 0.725904, test acc: 0.745763\n",
      "2658, train loss: 0.583335, test loss: 0.606288, train acc: 0.725904, test acc: 0.745763\n",
      "2659, train loss: 0.583322, test loss: 0.606275, train acc: 0.725904, test acc: 0.745763\n",
      "2660, train loss: 0.58331, test loss: 0.606261, train acc: 0.725904, test acc: 0.745763\n",
      "2661, train loss: 0.583298, test loss: 0.606248, train acc: 0.725904, test acc: 0.745763\n",
      "2662, train loss: 0.583286, test loss: 0.606235, train acc: 0.725904, test acc: 0.745763\n",
      "2663, train loss: 0.583274, test loss: 0.606221, train acc: 0.725904, test acc: 0.745763\n",
      "2664, train loss: 0.583261, test loss: 0.606208, train acc: 0.725904, test acc: 0.745763\n",
      "2665, train loss: 0.583249, test loss: 0.606194, train acc: 0.725904, test acc: 0.745763\n",
      "2666, train loss: 0.583237, test loss: 0.606181, train acc: 0.725904, test acc: 0.745763\n",
      "2667, train loss: 0.583225, test loss: 0.606168, train acc: 0.725904, test acc: 0.745763\n",
      "2668, train loss: 0.583212, test loss: 0.606154, train acc: 0.725904, test acc: 0.745763\n",
      "2669, train loss: 0.5832, test loss: 0.606141, train acc: 0.725904, test acc: 0.745763\n",
      "2670, train loss: 0.583188, test loss: 0.606127, train acc: 0.725904, test acc: 0.745763\n",
      "2671, train loss: 0.583176, test loss: 0.606114, train acc: 0.725904, test acc: 0.745763\n",
      "2672, train loss: 0.583164, test loss: 0.606101, train acc: 0.725904, test acc: 0.728814\n",
      "2673, train loss: 0.583151, test loss: 0.606087, train acc: 0.725904, test acc: 0.728814\n",
      "2674, train loss: 0.583139, test loss: 0.606074, train acc: 0.725904, test acc: 0.728814\n",
      "2675, train loss: 0.583127, test loss: 0.60606, train acc: 0.725904, test acc: 0.728814\n",
      "2676, train loss: 0.583115, test loss: 0.606047, train acc: 0.725904, test acc: 0.728814\n",
      "2677, train loss: 0.583103, test loss: 0.606034, train acc: 0.725904, test acc: 0.728814\n",
      "2678, train loss: 0.58309, test loss: 0.60602, train acc: 0.725904, test acc: 0.728814\n",
      "2679, train loss: 0.583078, test loss: 0.606007, train acc: 0.725904, test acc: 0.728814\n",
      "2680, train loss: 0.583066, test loss: 0.605994, train acc: 0.725904, test acc: 0.728814\n",
      "2681, train loss: 0.583054, test loss: 0.60598, train acc: 0.725904, test acc: 0.728814\n",
      "2682, train loss: 0.583042, test loss: 0.605967, train acc: 0.725904, test acc: 0.728814\n",
      "2683, train loss: 0.58303, test loss: 0.605953, train acc: 0.725904, test acc: 0.728814\n",
      "2684, train loss: 0.583018, test loss: 0.60594, train acc: 0.725904, test acc: 0.728814\n",
      "2685, train loss: 0.583005, test loss: 0.605927, train acc: 0.725904, test acc: 0.728814\n",
      "2686, train loss: 0.582993, test loss: 0.605913, train acc: 0.725904, test acc: 0.728814\n",
      "2687, train loss: 0.582981, test loss: 0.6059, train acc: 0.725904, test acc: 0.728814\n",
      "2688, train loss: 0.582969, test loss: 0.605887, train acc: 0.725904, test acc: 0.728814\n",
      "2689, train loss: 0.582957, test loss: 0.605873, train acc: 0.725904, test acc: 0.728814\n",
      "2690, train loss: 0.582945, test loss: 0.60586, train acc: 0.725904, test acc: 0.728814\n",
      "2691, train loss: 0.582933, test loss: 0.605847, train acc: 0.725904, test acc: 0.728814\n",
      "2692, train loss: 0.58292, test loss: 0.605833, train acc: 0.725904, test acc: 0.728814\n",
      "2693, train loss: 0.582908, test loss: 0.60582, train acc: 0.725904, test acc: 0.728814\n",
      "2694, train loss: 0.582896, test loss: 0.605807, train acc: 0.725904, test acc: 0.728814\n",
      "2695, train loss: 0.582884, test loss: 0.605793, train acc: 0.725904, test acc: 0.728814\n",
      "2696, train loss: 0.582872, test loss: 0.60578, train acc: 0.725904, test acc: 0.728814\n",
      "2697, train loss: 0.58286, test loss: 0.605767, train acc: 0.725904, test acc: 0.728814\n",
      "2698, train loss: 0.582848, test loss: 0.605753, train acc: 0.725904, test acc: 0.728814\n",
      "2699, train loss: 0.582836, test loss: 0.60574, train acc: 0.725904, test acc: 0.728814\n",
      "2700, train loss: 0.582823, test loss: 0.605727, train acc: 0.725904, test acc: 0.728814\n",
      "2701, train loss: 0.582811, test loss: 0.605713, train acc: 0.725904, test acc: 0.728814\n",
      "2702, train loss: 0.582799, test loss: 0.6057, train acc: 0.725904, test acc: 0.728814\n",
      "2703, train loss: 0.582787, test loss: 0.605687, train acc: 0.725904, test acc: 0.728814\n",
      "2704, train loss: 0.582775, test loss: 0.605673, train acc: 0.725904, test acc: 0.728814\n",
      "2705, train loss: 0.582763, test loss: 0.60566, train acc: 0.725904, test acc: 0.728814\n",
      "2706, train loss: 0.582751, test loss: 0.605647, train acc: 0.725904, test acc: 0.728814\n",
      "2707, train loss: 0.582739, test loss: 0.605634, train acc: 0.725904, test acc: 0.728814\n",
      "2708, train loss: 0.582727, test loss: 0.60562, train acc: 0.725904, test acc: 0.728814\n",
      "2709, train loss: 0.582715, test loss: 0.605607, train acc: 0.725904, test acc: 0.728814\n",
      "2710, train loss: 0.582703, test loss: 0.605594, train acc: 0.725904, test acc: 0.728814\n",
      "2711, train loss: 0.582691, test loss: 0.60558, train acc: 0.725904, test acc: 0.728814\n",
      "2712, train loss: 0.582679, test loss: 0.605567, train acc: 0.725904, test acc: 0.728814\n",
      "2713, train loss: 0.582667, test loss: 0.605554, train acc: 0.725904, test acc: 0.728814\n",
      "2714, train loss: 0.582654, test loss: 0.605541, train acc: 0.725904, test acc: 0.728814\n",
      "2715, train loss: 0.582642, test loss: 0.605527, train acc: 0.725904, test acc: 0.728814\n",
      "2716, train loss: 0.58263, test loss: 0.605514, train acc: 0.725904, test acc: 0.728814\n",
      "2717, train loss: 0.582618, test loss: 0.605501, train acc: 0.725904, test acc: 0.728814\n",
      "2718, train loss: 0.582606, test loss: 0.605487, train acc: 0.725904, test acc: 0.728814\n",
      "2719, train loss: 0.582594, test loss: 0.605474, train acc: 0.725904, test acc: 0.728814\n",
      "2720, train loss: 0.582582, test loss: 0.605461, train acc: 0.725904, test acc: 0.728814\n",
      "2721, train loss: 0.58257, test loss: 0.605448, train acc: 0.725904, test acc: 0.728814\n",
      "2722, train loss: 0.582558, test loss: 0.605434, train acc: 0.725904, test acc: 0.728814\n",
      "2723, train loss: 0.582546, test loss: 0.605421, train acc: 0.725904, test acc: 0.728814\n",
      "2724, train loss: 0.582534, test loss: 0.605408, train acc: 0.725904, test acc: 0.728814\n",
      "2725, train loss: 0.582522, test loss: 0.605395, train acc: 0.725904, test acc: 0.728814\n",
      "2726, train loss: 0.58251, test loss: 0.605381, train acc: 0.725904, test acc: 0.728814\n",
      "2727, train loss: 0.582498, test loss: 0.605368, train acc: 0.725904, test acc: 0.728814\n",
      "2728, train loss: 0.582486, test loss: 0.605355, train acc: 0.725904, test acc: 0.728814\n",
      "2729, train loss: 0.582474, test loss: 0.605342, train acc: 0.725904, test acc: 0.728814\n",
      "2730, train loss: 0.582462, test loss: 0.605328, train acc: 0.725904, test acc: 0.728814\n",
      "2731, train loss: 0.58245, test loss: 0.605315, train acc: 0.725904, test acc: 0.728814\n",
      "2732, train loss: 0.582438, test loss: 0.605302, train acc: 0.725904, test acc: 0.728814\n",
      "2733, train loss: 0.582426, test loss: 0.605289, train acc: 0.725904, test acc: 0.728814\n",
      "2734, train loss: 0.582414, test loss: 0.605275, train acc: 0.725904, test acc: 0.728814\n",
      "2735, train loss: 0.582402, test loss: 0.605262, train acc: 0.725904, test acc: 0.728814\n",
      "2736, train loss: 0.58239, test loss: 0.605249, train acc: 0.725904, test acc: 0.728814\n",
      "2737, train loss: 0.582378, test loss: 0.605236, train acc: 0.725904, test acc: 0.728814\n",
      "2738, train loss: 0.582366, test loss: 0.605222, train acc: 0.725904, test acc: 0.728814\n",
      "2739, train loss: 0.582354, test loss: 0.605209, train acc: 0.725904, test acc: 0.728814\n",
      "2740, train loss: 0.582342, test loss: 0.605196, train acc: 0.725904, test acc: 0.728814\n",
      "2741, train loss: 0.58233, test loss: 0.605183, train acc: 0.725904, test acc: 0.728814\n",
      "2742, train loss: 0.582318, test loss: 0.60517, train acc: 0.725904, test acc: 0.728814\n",
      "2743, train loss: 0.582306, test loss: 0.605156, train acc: 0.725904, test acc: 0.728814\n",
      "2744, train loss: 0.582294, test loss: 0.605143, train acc: 0.725904, test acc: 0.728814\n",
      "2745, train loss: 0.582283, test loss: 0.60513, train acc: 0.725904, test acc: 0.728814\n",
      "2746, train loss: 0.582271, test loss: 0.605117, train acc: 0.725904, test acc: 0.728814\n",
      "2747, train loss: 0.582259, test loss: 0.605103, train acc: 0.725904, test acc: 0.728814\n",
      "2748, train loss: 0.582247, test loss: 0.60509, train acc: 0.725904, test acc: 0.728814\n",
      "2749, train loss: 0.582235, test loss: 0.605077, train acc: 0.725904, test acc: 0.728814\n",
      "2750, train loss: 0.582223, test loss: 0.605064, train acc: 0.725904, test acc: 0.728814\n",
      "2751, train loss: 0.582211, test loss: 0.605051, train acc: 0.725904, test acc: 0.728814\n",
      "2752, train loss: 0.582199, test loss: 0.605037, train acc: 0.725904, test acc: 0.728814\n",
      "2753, train loss: 0.582187, test loss: 0.605024, train acc: 0.725904, test acc: 0.728814\n",
      "2754, train loss: 0.582175, test loss: 0.605011, train acc: 0.725904, test acc: 0.728814\n",
      "2755, train loss: 0.582163, test loss: 0.604998, train acc: 0.725904, test acc: 0.728814\n",
      "2756, train loss: 0.582151, test loss: 0.604985, train acc: 0.725904, test acc: 0.728814\n",
      "2757, train loss: 0.582139, test loss: 0.604971, train acc: 0.725904, test acc: 0.728814\n",
      "2758, train loss: 0.582128, test loss: 0.604958, train acc: 0.725904, test acc: 0.728814\n",
      "2759, train loss: 0.582116, test loss: 0.604945, train acc: 0.725904, test acc: 0.728814\n",
      "2760, train loss: 0.582104, test loss: 0.604932, train acc: 0.725904, test acc: 0.728814\n",
      "2761, train loss: 0.582092, test loss: 0.604919, train acc: 0.725904, test acc: 0.728814\n",
      "2762, train loss: 0.58208, test loss: 0.604906, train acc: 0.725904, test acc: 0.728814\n",
      "2763, train loss: 0.582068, test loss: 0.604892, train acc: 0.725904, test acc: 0.728814\n",
      "2764, train loss: 0.582056, test loss: 0.604879, train acc: 0.725904, test acc: 0.728814\n",
      "2765, train loss: 0.582044, test loss: 0.604866, train acc: 0.725904, test acc: 0.728814\n",
      "2766, train loss: 0.582032, test loss: 0.604853, train acc: 0.725904, test acc: 0.728814\n",
      "2767, train loss: 0.582021, test loss: 0.60484, train acc: 0.725904, test acc: 0.728814\n",
      "2768, train loss: 0.582009, test loss: 0.604827, train acc: 0.725904, test acc: 0.728814\n",
      "2769, train loss: 0.581997, test loss: 0.604813, train acc: 0.725904, test acc: 0.728814\n",
      "2770, train loss: 0.581985, test loss: 0.6048, train acc: 0.725904, test acc: 0.728814\n",
      "2771, train loss: 0.581973, test loss: 0.604787, train acc: 0.725904, test acc: 0.728814\n",
      "2772, train loss: 0.581961, test loss: 0.604774, train acc: 0.725904, test acc: 0.728814\n",
      "2773, train loss: 0.581949, test loss: 0.604761, train acc: 0.725904, test acc: 0.728814\n",
      "2774, train loss: 0.581937, test loss: 0.604748, train acc: 0.725904, test acc: 0.728814\n",
      "2775, train loss: 0.581926, test loss: 0.604735, train acc: 0.725904, test acc: 0.728814\n",
      "2776, train loss: 0.581914, test loss: 0.604721, train acc: 0.725904, test acc: 0.728814\n",
      "2777, train loss: 0.581902, test loss: 0.604708, train acc: 0.725904, test acc: 0.728814\n",
      "2778, train loss: 0.58189, test loss: 0.604695, train acc: 0.725904, test acc: 0.728814\n",
      "2779, train loss: 0.581878, test loss: 0.604682, train acc: 0.725904, test acc: 0.728814\n",
      "2780, train loss: 0.581866, test loss: 0.604669, train acc: 0.725904, test acc: 0.728814\n",
      "2781, train loss: 0.581855, test loss: 0.604656, train acc: 0.725904, test acc: 0.728814\n",
      "2782, train loss: 0.581843, test loss: 0.604643, train acc: 0.725904, test acc: 0.728814\n",
      "2783, train loss: 0.581831, test loss: 0.604629, train acc: 0.725904, test acc: 0.728814\n",
      "2784, train loss: 0.581819, test loss: 0.604616, train acc: 0.725904, test acc: 0.728814\n",
      "2785, train loss: 0.581807, test loss: 0.604603, train acc: 0.725904, test acc: 0.728814\n",
      "2786, train loss: 0.581795, test loss: 0.60459, train acc: 0.725904, test acc: 0.728814\n",
      "2787, train loss: 0.581784, test loss: 0.604577, train acc: 0.725904, test acc: 0.728814\n",
      "2788, train loss: 0.581772, test loss: 0.604564, train acc: 0.725904, test acc: 0.728814\n",
      "2789, train loss: 0.58176, test loss: 0.604551, train acc: 0.725904, test acc: 0.728814\n",
      "2790, train loss: 0.581748, test loss: 0.604538, train acc: 0.725904, test acc: 0.728814\n",
      "2791, train loss: 0.581736, test loss: 0.604525, train acc: 0.725904, test acc: 0.728814\n",
      "2792, train loss: 0.581725, test loss: 0.604511, train acc: 0.725904, test acc: 0.728814\n",
      "2793, train loss: 0.581713, test loss: 0.604498, train acc: 0.725904, test acc: 0.728814\n",
      "2794, train loss: 0.581701, test loss: 0.604485, train acc: 0.725904, test acc: 0.728814\n",
      "2795, train loss: 0.581689, test loss: 0.604472, train acc: 0.725904, test acc: 0.728814\n",
      "2796, train loss: 0.581677, test loss: 0.604459, train acc: 0.725904, test acc: 0.728814\n",
      "2797, train loss: 0.581666, test loss: 0.604446, train acc: 0.725904, test acc: 0.728814\n",
      "2798, train loss: 0.581654, test loss: 0.604433, train acc: 0.725904, test acc: 0.728814\n",
      "2799, train loss: 0.581642, test loss: 0.60442, train acc: 0.725904, test acc: 0.728814\n",
      "2800, train loss: 0.58163, test loss: 0.604407, train acc: 0.725904, test acc: 0.728814\n",
      "2801, train loss: 0.581619, test loss: 0.604394, train acc: 0.725904, test acc: 0.728814\n",
      "2802, train loss: 0.581607, test loss: 0.60438, train acc: 0.725904, test acc: 0.728814\n",
      "2803, train loss: 0.581595, test loss: 0.604367, train acc: 0.725904, test acc: 0.728814\n",
      "2804, train loss: 0.581583, test loss: 0.604354, train acc: 0.725904, test acc: 0.728814\n",
      "2805, train loss: 0.581572, test loss: 0.604341, train acc: 0.725904, test acc: 0.728814\n",
      "2806, train loss: 0.58156, test loss: 0.604328, train acc: 0.725904, test acc: 0.728814\n",
      "2807, train loss: 0.581548, test loss: 0.604315, train acc: 0.725904, test acc: 0.728814\n",
      "2808, train loss: 0.581536, test loss: 0.604302, train acc: 0.725904, test acc: 0.728814\n",
      "2809, train loss: 0.581525, test loss: 0.604289, train acc: 0.725904, test acc: 0.728814\n",
      "2810, train loss: 0.581513, test loss: 0.604276, train acc: 0.725904, test acc: 0.728814\n",
      "2811, train loss: 0.581501, test loss: 0.604263, train acc: 0.725904, test acc: 0.728814\n",
      "2812, train loss: 0.581489, test loss: 0.60425, train acc: 0.725904, test acc: 0.728814\n",
      "2813, train loss: 0.581478, test loss: 0.604237, train acc: 0.725904, test acc: 0.728814\n",
      "2814, train loss: 0.581466, test loss: 0.604224, train acc: 0.725904, test acc: 0.728814\n",
      "2815, train loss: 0.581454, test loss: 0.604211, train acc: 0.725904, test acc: 0.728814\n",
      "2816, train loss: 0.581442, test loss: 0.604198, train acc: 0.725904, test acc: 0.728814\n",
      "2817, train loss: 0.581431, test loss: 0.604185, train acc: 0.725904, test acc: 0.728814\n",
      "2818, train loss: 0.581419, test loss: 0.604171, train acc: 0.725904, test acc: 0.728814\n",
      "2819, train loss: 0.581407, test loss: 0.604158, train acc: 0.725904, test acc: 0.728814\n",
      "2820, train loss: 0.581396, test loss: 0.604145, train acc: 0.725904, test acc: 0.728814\n",
      "2821, train loss: 0.581384, test loss: 0.604132, train acc: 0.725904, test acc: 0.728814\n",
      "2822, train loss: 0.581372, test loss: 0.604119, train acc: 0.725904, test acc: 0.728814\n",
      "2823, train loss: 0.581361, test loss: 0.604106, train acc: 0.725904, test acc: 0.728814\n",
      "2824, train loss: 0.581349, test loss: 0.604093, train acc: 0.725904, test acc: 0.728814\n",
      "2825, train loss: 0.581337, test loss: 0.60408, train acc: 0.725904, test acc: 0.728814\n",
      "2826, train loss: 0.581325, test loss: 0.604067, train acc: 0.725904, test acc: 0.728814\n",
      "2827, train loss: 0.581314, test loss: 0.604054, train acc: 0.725904, test acc: 0.728814\n",
      "2828, train loss: 0.581302, test loss: 0.604041, train acc: 0.725904, test acc: 0.728814\n",
      "2829, train loss: 0.58129, test loss: 0.604028, train acc: 0.725904, test acc: 0.728814\n",
      "2830, train loss: 0.581279, test loss: 0.604015, train acc: 0.725904, test acc: 0.728814\n",
      "2831, train loss: 0.581267, test loss: 0.604002, train acc: 0.725904, test acc: 0.728814\n",
      "2832, train loss: 0.581255, test loss: 0.603989, train acc: 0.725904, test acc: 0.728814\n",
      "2833, train loss: 0.581244, test loss: 0.603976, train acc: 0.725904, test acc: 0.728814\n",
      "2834, train loss: 0.581232, test loss: 0.603963, train acc: 0.725904, test acc: 0.728814\n",
      "2835, train loss: 0.58122, test loss: 0.60395, train acc: 0.725904, test acc: 0.728814\n",
      "2836, train loss: 0.581209, test loss: 0.603937, train acc: 0.725904, test acc: 0.728814\n",
      "2837, train loss: 0.581197, test loss: 0.603924, train acc: 0.725904, test acc: 0.728814\n",
      "2838, train loss: 0.581185, test loss: 0.603911, train acc: 0.725904, test acc: 0.728814\n",
      "2839, train loss: 0.581174, test loss: 0.603898, train acc: 0.725904, test acc: 0.728814\n",
      "2840, train loss: 0.581162, test loss: 0.603885, train acc: 0.725904, test acc: 0.728814\n",
      "2841, train loss: 0.581151, test loss: 0.603872, train acc: 0.725904, test acc: 0.728814\n",
      "2842, train loss: 0.581139, test loss: 0.603859, train acc: 0.725904, test acc: 0.728814\n",
      "2843, train loss: 0.581127, test loss: 0.603846, train acc: 0.725904, test acc: 0.728814\n",
      "2844, train loss: 0.581116, test loss: 0.603833, train acc: 0.725904, test acc: 0.728814\n",
      "2845, train loss: 0.581104, test loss: 0.60382, train acc: 0.725904, test acc: 0.728814\n",
      "2846, train loss: 0.581092, test loss: 0.603807, train acc: 0.725904, test acc: 0.728814\n",
      "2847, train loss: 0.581081, test loss: 0.603794, train acc: 0.725904, test acc: 0.728814\n",
      "2848, train loss: 0.581069, test loss: 0.603781, train acc: 0.725904, test acc: 0.728814\n",
      "2849, train loss: 0.581057, test loss: 0.603768, train acc: 0.725904, test acc: 0.728814\n",
      "2850, train loss: 0.581046, test loss: 0.603755, train acc: 0.725904, test acc: 0.728814\n",
      "2851, train loss: 0.581034, test loss: 0.603742, train acc: 0.725904, test acc: 0.728814\n",
      "2852, train loss: 0.581023, test loss: 0.603729, train acc: 0.725904, test acc: 0.728814\n",
      "2853, train loss: 0.581011, test loss: 0.603716, train acc: 0.725904, test acc: 0.728814\n",
      "2854, train loss: 0.580999, test loss: 0.603703, train acc: 0.725904, test acc: 0.728814\n",
      "2855, train loss: 0.580988, test loss: 0.60369, train acc: 0.725904, test acc: 0.728814\n",
      "2856, train loss: 0.580976, test loss: 0.603677, train acc: 0.725904, test acc: 0.728814\n",
      "2857, train loss: 0.580965, test loss: 0.603664, train acc: 0.725904, test acc: 0.728814\n",
      "2858, train loss: 0.580953, test loss: 0.603651, train acc: 0.725904, test acc: 0.728814\n",
      "2859, train loss: 0.580941, test loss: 0.603638, train acc: 0.725904, test acc: 0.728814\n",
      "2860, train loss: 0.58093, test loss: 0.603625, train acc: 0.725904, test acc: 0.728814\n",
      "2861, train loss: 0.580918, test loss: 0.603612, train acc: 0.725904, test acc: 0.728814\n",
      "2862, train loss: 0.580907, test loss: 0.603599, train acc: 0.725904, test acc: 0.728814\n",
      "2863, train loss: 0.580895, test loss: 0.603586, train acc: 0.725904, test acc: 0.728814\n",
      "2864, train loss: 0.580884, test loss: 0.603573, train acc: 0.725904, test acc: 0.728814\n",
      "2865, train loss: 0.580872, test loss: 0.603561, train acc: 0.725904, test acc: 0.728814\n",
      "2866, train loss: 0.58086, test loss: 0.603548, train acc: 0.725904, test acc: 0.728814\n",
      "2867, train loss: 0.580849, test loss: 0.603535, train acc: 0.725904, test acc: 0.728814\n",
      "2868, train loss: 0.580837, test loss: 0.603522, train acc: 0.725904, test acc: 0.728814\n",
      "2869, train loss: 0.580826, test loss: 0.603509, train acc: 0.725904, test acc: 0.728814\n",
      "2870, train loss: 0.580814, test loss: 0.603496, train acc: 0.725904, test acc: 0.728814\n",
      "2871, train loss: 0.580803, test loss: 0.603483, train acc: 0.725904, test acc: 0.728814\n",
      "2872, train loss: 0.580791, test loss: 0.60347, train acc: 0.725904, test acc: 0.728814\n",
      "2873, train loss: 0.580779, test loss: 0.603457, train acc: 0.725904, test acc: 0.728814\n",
      "2874, train loss: 0.580768, test loss: 0.603444, train acc: 0.725904, test acc: 0.728814\n",
      "2875, train loss: 0.580756, test loss: 0.603431, train acc: 0.725904, test acc: 0.728814\n",
      "2876, train loss: 0.580745, test loss: 0.603418, train acc: 0.725904, test acc: 0.728814\n",
      "2877, train loss: 0.580733, test loss: 0.603405, train acc: 0.725904, test acc: 0.728814\n",
      "2878, train loss: 0.580722, test loss: 0.603392, train acc: 0.725904, test acc: 0.728814\n",
      "2879, train loss: 0.58071, test loss: 0.603379, train acc: 0.725904, test acc: 0.728814\n",
      "2880, train loss: 0.580699, test loss: 0.603366, train acc: 0.725904, test acc: 0.728814\n",
      "2881, train loss: 0.580687, test loss: 0.603354, train acc: 0.725904, test acc: 0.728814\n",
      "2882, train loss: 0.580676, test loss: 0.603341, train acc: 0.725904, test acc: 0.728814\n",
      "2883, train loss: 0.580664, test loss: 0.603328, train acc: 0.725904, test acc: 0.728814\n",
      "2884, train loss: 0.580653, test loss: 0.603315, train acc: 0.725904, test acc: 0.728814\n",
      "2885, train loss: 0.580641, test loss: 0.603302, train acc: 0.725904, test acc: 0.728814\n",
      "2886, train loss: 0.58063, test loss: 0.603289, train acc: 0.725904, test acc: 0.728814\n",
      "2887, train loss: 0.580618, test loss: 0.603276, train acc: 0.725904, test acc: 0.728814\n",
      "2888, train loss: 0.580607, test loss: 0.603263, train acc: 0.725904, test acc: 0.728814\n",
      "2889, train loss: 0.580595, test loss: 0.60325, train acc: 0.725904, test acc: 0.728814\n",
      "2890, train loss: 0.580584, test loss: 0.603237, train acc: 0.725904, test acc: 0.728814\n",
      "2891, train loss: 0.580572, test loss: 0.603224, train acc: 0.725904, test acc: 0.728814\n",
      "2892, train loss: 0.580561, test loss: 0.603212, train acc: 0.725904, test acc: 0.728814\n",
      "2893, train loss: 0.580549, test loss: 0.603199, train acc: 0.725904, test acc: 0.728814\n",
      "2894, train loss: 0.580538, test loss: 0.603186, train acc: 0.725904, test acc: 0.728814\n",
      "2895, train loss: 0.580526, test loss: 0.603173, train acc: 0.725904, test acc: 0.728814\n",
      "2896, train loss: 0.580515, test loss: 0.60316, train acc: 0.725904, test acc: 0.728814\n",
      "2897, train loss: 0.580503, test loss: 0.603147, train acc: 0.725904, test acc: 0.728814\n",
      "2898, train loss: 0.580492, test loss: 0.603134, train acc: 0.725904, test acc: 0.728814\n",
      "2899, train loss: 0.58048, test loss: 0.603121, train acc: 0.725904, test acc: 0.728814\n",
      "2900, train loss: 0.580469, test loss: 0.603109, train acc: 0.725904, test acc: 0.728814\n",
      "2901, train loss: 0.580457, test loss: 0.603096, train acc: 0.725904, test acc: 0.728814\n",
      "2902, train loss: 0.580446, test loss: 0.603083, train acc: 0.725904, test acc: 0.728814\n",
      "2903, train loss: 0.580434, test loss: 0.60307, train acc: 0.725904, test acc: 0.728814\n",
      "2904, train loss: 0.580423, test loss: 0.603057, train acc: 0.725904, test acc: 0.728814\n",
      "2905, train loss: 0.580411, test loss: 0.603044, train acc: 0.725904, test acc: 0.728814\n",
      "2906, train loss: 0.5804, test loss: 0.603031, train acc: 0.725904, test acc: 0.728814\n",
      "2907, train loss: 0.580389, test loss: 0.603018, train acc: 0.725904, test acc: 0.728814\n",
      "2908, train loss: 0.580377, test loss: 0.603006, train acc: 0.725904, test acc: 0.728814\n",
      "2909, train loss: 0.580366, test loss: 0.602993, train acc: 0.725904, test acc: 0.728814\n",
      "2910, train loss: 0.580354, test loss: 0.60298, train acc: 0.725904, test acc: 0.728814\n",
      "2911, train loss: 0.580343, test loss: 0.602967, train acc: 0.725904, test acc: 0.728814\n",
      "2912, train loss: 0.580331, test loss: 0.602954, train acc: 0.725904, test acc: 0.728814\n",
      "2913, train loss: 0.58032, test loss: 0.602941, train acc: 0.725904, test acc: 0.728814\n",
      "2914, train loss: 0.580308, test loss: 0.602928, train acc: 0.725904, test acc: 0.728814\n",
      "2915, train loss: 0.580297, test loss: 0.602916, train acc: 0.725904, test acc: 0.728814\n",
      "2916, train loss: 0.580286, test loss: 0.602903, train acc: 0.725904, test acc: 0.728814\n",
      "2917, train loss: 0.580274, test loss: 0.60289, train acc: 0.725904, test acc: 0.728814\n",
      "2918, train loss: 0.580263, test loss: 0.602877, train acc: 0.725904, test acc: 0.728814\n",
      "2919, train loss: 0.580251, test loss: 0.602864, train acc: 0.725904, test acc: 0.728814\n",
      "2920, train loss: 0.58024, test loss: 0.602851, train acc: 0.725904, test acc: 0.728814\n",
      "2921, train loss: 0.580229, test loss: 0.602839, train acc: 0.725904, test acc: 0.728814\n",
      "2922, train loss: 0.580217, test loss: 0.602826, train acc: 0.725904, test acc: 0.728814\n",
      "2923, train loss: 0.580206, test loss: 0.602813, train acc: 0.725904, test acc: 0.728814\n",
      "2924, train loss: 0.580194, test loss: 0.6028, train acc: 0.725904, test acc: 0.728814\n",
      "2925, train loss: 0.580183, test loss: 0.602787, train acc: 0.725904, test acc: 0.728814\n",
      "2926, train loss: 0.580172, test loss: 0.602774, train acc: 0.725904, test acc: 0.728814\n",
      "2927, train loss: 0.58016, test loss: 0.602762, train acc: 0.725904, test acc: 0.728814\n",
      "2928, train loss: 0.580149, test loss: 0.602749, train acc: 0.725904, test acc: 0.728814\n",
      "2929, train loss: 0.580137, test loss: 0.602736, train acc: 0.725904, test acc: 0.728814\n",
      "2930, train loss: 0.580126, test loss: 0.602723, train acc: 0.725904, test acc: 0.728814\n",
      "2931, train loss: 0.580115, test loss: 0.60271, train acc: 0.725904, test acc: 0.728814\n",
      "2932, train loss: 0.580103, test loss: 0.602697, train acc: 0.725904, test acc: 0.728814\n",
      "2933, train loss: 0.580092, test loss: 0.602685, train acc: 0.725904, test acc: 0.728814\n",
      "2934, train loss: 0.58008, test loss: 0.602672, train acc: 0.725904, test acc: 0.728814\n",
      "2935, train loss: 0.580069, test loss: 0.602659, train acc: 0.725904, test acc: 0.728814\n",
      "2936, train loss: 0.580058, test loss: 0.602646, train acc: 0.725904, test acc: 0.728814\n",
      "2937, train loss: 0.580046, test loss: 0.602633, train acc: 0.725904, test acc: 0.728814\n",
      "2938, train loss: 0.580035, test loss: 0.602621, train acc: 0.725904, test acc: 0.728814\n",
      "2939, train loss: 0.580024, test loss: 0.602608, train acc: 0.725904, test acc: 0.728814\n",
      "2940, train loss: 0.580012, test loss: 0.602595, train acc: 0.725904, test acc: 0.728814\n",
      "2941, train loss: 0.580001, test loss: 0.602582, train acc: 0.725904, test acc: 0.728814\n",
      "2942, train loss: 0.57999, test loss: 0.602569, train acc: 0.725904, test acc: 0.728814\n",
      "2943, train loss: 0.579978, test loss: 0.602557, train acc: 0.725904, test acc: 0.728814\n",
      "2944, train loss: 0.579967, test loss: 0.602544, train acc: 0.725904, test acc: 0.728814\n",
      "2945, train loss: 0.579956, test loss: 0.602531, train acc: 0.725904, test acc: 0.728814\n",
      "2946, train loss: 0.579944, test loss: 0.602518, train acc: 0.725904, test acc: 0.728814\n",
      "2947, train loss: 0.579933, test loss: 0.602506, train acc: 0.725904, test acc: 0.728814\n",
      "2948, train loss: 0.579922, test loss: 0.602493, train acc: 0.725904, test acc: 0.728814\n",
      "2949, train loss: 0.57991, test loss: 0.60248, train acc: 0.725904, test acc: 0.728814\n",
      "2950, train loss: 0.579899, test loss: 0.602467, train acc: 0.725904, test acc: 0.728814\n",
      "2951, train loss: 0.579888, test loss: 0.602454, train acc: 0.725904, test acc: 0.728814\n",
      "2952, train loss: 0.579876, test loss: 0.602442, train acc: 0.725904, test acc: 0.728814\n",
      "2953, train loss: 0.579865, test loss: 0.602429, train acc: 0.725904, test acc: 0.728814\n",
      "2954, train loss: 0.579854, test loss: 0.602416, train acc: 0.725904, test acc: 0.728814\n",
      "2955, train loss: 0.579842, test loss: 0.602403, train acc: 0.725904, test acc: 0.728814\n",
      "2956, train loss: 0.579831, test loss: 0.602391, train acc: 0.725904, test acc: 0.728814\n",
      "2957, train loss: 0.57982, test loss: 0.602378, train acc: 0.725904, test acc: 0.728814\n",
      "2958, train loss: 0.579808, test loss: 0.602365, train acc: 0.725904, test acc: 0.728814\n",
      "2959, train loss: 0.579797, test loss: 0.602352, train acc: 0.725904, test acc: 0.728814\n",
      "2960, train loss: 0.579786, test loss: 0.602339, train acc: 0.725904, test acc: 0.728814\n",
      "2961, train loss: 0.579774, test loss: 0.602327, train acc: 0.725904, test acc: 0.728814\n",
      "2962, train loss: 0.579763, test loss: 0.602314, train acc: 0.725904, test acc: 0.728814\n",
      "2963, train loss: 0.579752, test loss: 0.602301, train acc: 0.725904, test acc: 0.728814\n",
      "2964, train loss: 0.579741, test loss: 0.602288, train acc: 0.725904, test acc: 0.728814\n",
      "2965, train loss: 0.579729, test loss: 0.602276, train acc: 0.725904, test acc: 0.728814\n",
      "2966, train loss: 0.579718, test loss: 0.602263, train acc: 0.725904, test acc: 0.728814\n",
      "2967, train loss: 0.579707, test loss: 0.60225, train acc: 0.725904, test acc: 0.728814\n",
      "2968, train loss: 0.579695, test loss: 0.602237, train acc: 0.725904, test acc: 0.728814\n",
      "2969, train loss: 0.579684, test loss: 0.602225, train acc: 0.725904, test acc: 0.728814\n",
      "2970, train loss: 0.579673, test loss: 0.602212, train acc: 0.725904, test acc: 0.728814\n",
      "2971, train loss: 0.579662, test loss: 0.602199, train acc: 0.725904, test acc: 0.728814\n",
      "2972, train loss: 0.57965, test loss: 0.602187, train acc: 0.725904, test acc: 0.728814\n",
      "2973, train loss: 0.579639, test loss: 0.602174, train acc: 0.725904, test acc: 0.728814\n",
      "2974, train loss: 0.579628, test loss: 0.602161, train acc: 0.725904, test acc: 0.728814\n",
      "2975, train loss: 0.579616, test loss: 0.602148, train acc: 0.725904, test acc: 0.728814\n",
      "2976, train loss: 0.579605, test loss: 0.602136, train acc: 0.725904, test acc: 0.728814\n",
      "2977, train loss: 0.579594, test loss: 0.602123, train acc: 0.725904, test acc: 0.728814\n",
      "2978, train loss: 0.579583, test loss: 0.60211, train acc: 0.725904, test acc: 0.728814\n",
      "2979, train loss: 0.579571, test loss: 0.602097, train acc: 0.725904, test acc: 0.728814\n",
      "2980, train loss: 0.57956, test loss: 0.602085, train acc: 0.725904, test acc: 0.728814\n",
      "2981, train loss: 0.579549, test loss: 0.602072, train acc: 0.725904, test acc: 0.728814\n",
      "2982, train loss: 0.579538, test loss: 0.602059, train acc: 0.725904, test acc: 0.728814\n",
      "2983, train loss: 0.579526, test loss: 0.602047, train acc: 0.725904, test acc: 0.728814\n",
      "2984, train loss: 0.579515, test loss: 0.602034, train acc: 0.725904, test acc: 0.728814\n",
      "2985, train loss: 0.579504, test loss: 0.602021, train acc: 0.725904, test acc: 0.728814\n",
      "2986, train loss: 0.579493, test loss: 0.602008, train acc: 0.725904, test acc: 0.728814\n",
      "2987, train loss: 0.579482, test loss: 0.601996, train acc: 0.725904, test acc: 0.728814\n",
      "2988, train loss: 0.57947, test loss: 0.601983, train acc: 0.725904, test acc: 0.728814\n",
      "2989, train loss: 0.579459, test loss: 0.60197, train acc: 0.725904, test acc: 0.728814\n",
      "2990, train loss: 0.579448, test loss: 0.601958, train acc: 0.725904, test acc: 0.728814\n",
      "2991, train loss: 0.579437, test loss: 0.601945, train acc: 0.725904, test acc: 0.728814\n",
      "2992, train loss: 0.579425, test loss: 0.601932, train acc: 0.725904, test acc: 0.728814\n",
      "2993, train loss: 0.579414, test loss: 0.60192, train acc: 0.725904, test acc: 0.728814\n",
      "2994, train loss: 0.579403, test loss: 0.601907, train acc: 0.725904, test acc: 0.728814\n",
      "2995, train loss: 0.579392, test loss: 0.601894, train acc: 0.725904, test acc: 0.728814\n",
      "2996, train loss: 0.579381, test loss: 0.601881, train acc: 0.725904, test acc: 0.728814\n",
      "2997, train loss: 0.579369, test loss: 0.601869, train acc: 0.725904, test acc: 0.728814\n",
      "2998, train loss: 0.579358, test loss: 0.601856, train acc: 0.725904, test acc: 0.728814\n",
      "2999, train loss: 0.579347, test loss: 0.601843, train acc: 0.725904, test acc: 0.728814\n",
      "3000, train loss: 0.579336, test loss: 0.601831, train acc: 0.725904, test acc: 0.728814\n",
      "3001, train loss: 0.579325, test loss: 0.601818, train acc: 0.725904, test acc: 0.728814\n",
      "3002, train loss: 0.579313, test loss: 0.601805, train acc: 0.725904, test acc: 0.728814\n",
      "3003, train loss: 0.579302, test loss: 0.601793, train acc: 0.725904, test acc: 0.728814\n",
      "3004, train loss: 0.579291, test loss: 0.60178, train acc: 0.725904, test acc: 0.728814\n",
      "3005, train loss: 0.57928, test loss: 0.601767, train acc: 0.725904, test acc: 0.728814\n",
      "3006, train loss: 0.579269, test loss: 0.601755, train acc: 0.725904, test acc: 0.728814\n",
      "3007, train loss: 0.579258, test loss: 0.601742, train acc: 0.725904, test acc: 0.728814\n",
      "3008, train loss: 0.579246, test loss: 0.601729, train acc: 0.725904, test acc: 0.728814\n",
      "3009, train loss: 0.579235, test loss: 0.601717, train acc: 0.725904, test acc: 0.728814\n",
      "3010, train loss: 0.579224, test loss: 0.601704, train acc: 0.725904, test acc: 0.728814\n",
      "3011, train loss: 0.579213, test loss: 0.601691, train acc: 0.725904, test acc: 0.728814\n",
      "3012, train loss: 0.579202, test loss: 0.601679, train acc: 0.725904, test acc: 0.728814\n",
      "3013, train loss: 0.579191, test loss: 0.601666, train acc: 0.725904, test acc: 0.728814\n",
      "3014, train loss: 0.579179, test loss: 0.601653, train acc: 0.725904, test acc: 0.728814\n",
      "3015, train loss: 0.579168, test loss: 0.601641, train acc: 0.725904, test acc: 0.728814\n",
      "3016, train loss: 0.579157, test loss: 0.601628, train acc: 0.725904, test acc: 0.728814\n",
      "3017, train loss: 0.579146, test loss: 0.601615, train acc: 0.725904, test acc: 0.728814\n",
      "3018, train loss: 0.579135, test loss: 0.601603, train acc: 0.725904, test acc: 0.728814\n",
      "3019, train loss: 0.579124, test loss: 0.60159, train acc: 0.725904, test acc: 0.728814\n",
      "3020, train loss: 0.579112, test loss: 0.601577, train acc: 0.725904, test acc: 0.728814\n",
      "3021, train loss: 0.579101, test loss: 0.601565, train acc: 0.725904, test acc: 0.728814\n",
      "3022, train loss: 0.57909, test loss: 0.601552, train acc: 0.725904, test acc: 0.728814\n",
      "3023, train loss: 0.579079, test loss: 0.601539, train acc: 0.725904, test acc: 0.728814\n",
      "3024, train loss: 0.579068, test loss: 0.601527, train acc: 0.725904, test acc: 0.728814\n",
      "3025, train loss: 0.579057, test loss: 0.601514, train acc: 0.725904, test acc: 0.728814\n",
      "3026, train loss: 0.579046, test loss: 0.601502, train acc: 0.725904, test acc: 0.728814\n",
      "3027, train loss: 0.579035, test loss: 0.601489, train acc: 0.725904, test acc: 0.728814\n",
      "3028, train loss: 0.579023, test loss: 0.601476, train acc: 0.725904, test acc: 0.728814\n",
      "3029, train loss: 0.579012, test loss: 0.601464, train acc: 0.725904, test acc: 0.728814\n",
      "3030, train loss: 0.579001, test loss: 0.601451, train acc: 0.725904, test acc: 0.728814\n",
      "3031, train loss: 0.57899, test loss: 0.601438, train acc: 0.725904, test acc: 0.728814\n",
      "3032, train loss: 0.578979, test loss: 0.601426, train acc: 0.725904, test acc: 0.728814\n",
      "3033, train loss: 0.578968, test loss: 0.601413, train acc: 0.725904, test acc: 0.728814\n",
      "3034, train loss: 0.578957, test loss: 0.601401, train acc: 0.725904, test acc: 0.728814\n",
      "3035, train loss: 0.578946, test loss: 0.601388, train acc: 0.725904, test acc: 0.728814\n",
      "3036, train loss: 0.578935, test loss: 0.601375, train acc: 0.725904, test acc: 0.728814\n",
      "3037, train loss: 0.578923, test loss: 0.601363, train acc: 0.725904, test acc: 0.728814\n",
      "3038, train loss: 0.578912, test loss: 0.60135, train acc: 0.725904, test acc: 0.728814\n",
      "3039, train loss: 0.578901, test loss: 0.601338, train acc: 0.725904, test acc: 0.728814\n",
      "3040, train loss: 0.57889, test loss: 0.601325, train acc: 0.725904, test acc: 0.728814\n",
      "3041, train loss: 0.578879, test loss: 0.601312, train acc: 0.725904, test acc: 0.728814\n",
      "3042, train loss: 0.578868, test loss: 0.6013, train acc: 0.725904, test acc: 0.728814\n",
      "3043, train loss: 0.578857, test loss: 0.601287, train acc: 0.725904, test acc: 0.728814\n",
      "3044, train loss: 0.578846, test loss: 0.601275, train acc: 0.725904, test acc: 0.728814\n",
      "3045, train loss: 0.578835, test loss: 0.601262, train acc: 0.725904, test acc: 0.728814\n",
      "3046, train loss: 0.578824, test loss: 0.601249, train acc: 0.725904, test acc: 0.728814\n",
      "3047, train loss: 0.578813, test loss: 0.601237, train acc: 0.725904, test acc: 0.728814\n",
      "3048, train loss: 0.578802, test loss: 0.601224, train acc: 0.725904, test acc: 0.728814\n",
      "3049, train loss: 0.57879, test loss: 0.601212, train acc: 0.725904, test acc: 0.728814\n",
      "3050, train loss: 0.578779, test loss: 0.601199, train acc: 0.725904, test acc: 0.728814\n",
      "3051, train loss: 0.578768, test loss: 0.601186, train acc: 0.725904, test acc: 0.728814\n",
      "3052, train loss: 0.578757, test loss: 0.601174, train acc: 0.725904, test acc: 0.728814\n",
      "3053, train loss: 0.578746, test loss: 0.601161, train acc: 0.725904, test acc: 0.728814\n",
      "3054, train loss: 0.578735, test loss: 0.601149, train acc: 0.725904, test acc: 0.728814\n",
      "3055, train loss: 0.578724, test loss: 0.601136, train acc: 0.725904, test acc: 0.728814\n",
      "3056, train loss: 0.578713, test loss: 0.601124, train acc: 0.725904, test acc: 0.728814\n",
      "3057, train loss: 0.578702, test loss: 0.601111, train acc: 0.725904, test acc: 0.728814\n",
      "3058, train loss: 0.578691, test loss: 0.601098, train acc: 0.725904, test acc: 0.728814\n",
      "3059, train loss: 0.57868, test loss: 0.601086, train acc: 0.725904, test acc: 0.728814\n",
      "3060, train loss: 0.578669, test loss: 0.601073, train acc: 0.725904, test acc: 0.728814\n",
      "3061, train loss: 0.578658, test loss: 0.601061, train acc: 0.725904, test acc: 0.728814\n",
      "3062, train loss: 0.578647, test loss: 0.601048, train acc: 0.725904, test acc: 0.728814\n",
      "3063, train loss: 0.578636, test loss: 0.601036, train acc: 0.725904, test acc: 0.728814\n",
      "3064, train loss: 0.578625, test loss: 0.601023, train acc: 0.725904, test acc: 0.728814\n",
      "3065, train loss: 0.578614, test loss: 0.60101, train acc: 0.725904, test acc: 0.728814\n",
      "3066, train loss: 0.578603, test loss: 0.600998, train acc: 0.725904, test acc: 0.728814\n",
      "3067, train loss: 0.578592, test loss: 0.600985, train acc: 0.725904, test acc: 0.728814\n",
      "3068, train loss: 0.578581, test loss: 0.600973, train acc: 0.725904, test acc: 0.728814\n",
      "3069, train loss: 0.57857, test loss: 0.60096, train acc: 0.725904, test acc: 0.728814\n",
      "3070, train loss: 0.578559, test loss: 0.600948, train acc: 0.725904, test acc: 0.728814\n",
      "3071, train loss: 0.578548, test loss: 0.600935, train acc: 0.725904, test acc: 0.728814\n",
      "3072, train loss: 0.578537, test loss: 0.600922, train acc: 0.725904, test acc: 0.728814\n",
      "3073, train loss: 0.578526, test loss: 0.60091, train acc: 0.725904, test acc: 0.728814\n",
      "3074, train loss: 0.578515, test loss: 0.600897, train acc: 0.725904, test acc: 0.728814\n",
      "3075, train loss: 0.578504, test loss: 0.600885, train acc: 0.725904, test acc: 0.728814\n",
      "3076, train loss: 0.578493, test loss: 0.600872, train acc: 0.725904, test acc: 0.728814\n",
      "3077, train loss: 0.578481, test loss: 0.60086, train acc: 0.725904, test acc: 0.728814\n",
      "3078, train loss: 0.578471, test loss: 0.600847, train acc: 0.725904, test acc: 0.728814\n",
      "3079, train loss: 0.57846, test loss: 0.600835, train acc: 0.725904, test acc: 0.728814\n",
      "3080, train loss: 0.578449, test loss: 0.600822, train acc: 0.725904, test acc: 0.728814\n",
      "3081, train loss: 0.578438, test loss: 0.60081, train acc: 0.725904, test acc: 0.728814\n",
      "3082, train loss: 0.578427, test loss: 0.600797, train acc: 0.725904, test acc: 0.728814\n",
      "3083, train loss: 0.578416, test loss: 0.600785, train acc: 0.725904, test acc: 0.728814\n",
      "3084, train loss: 0.578405, test loss: 0.600772, train acc: 0.725904, test acc: 0.728814\n",
      "3085, train loss: 0.578394, test loss: 0.60076, train acc: 0.725904, test acc: 0.728814\n",
      "3086, train loss: 0.578383, test loss: 0.600747, train acc: 0.725904, test acc: 0.728814\n",
      "3087, train loss: 0.578372, test loss: 0.600734, train acc: 0.725904, test acc: 0.728814\n",
      "3088, train loss: 0.578361, test loss: 0.600722, train acc: 0.725904, test acc: 0.728814\n",
      "3089, train loss: 0.57835, test loss: 0.600709, train acc: 0.725904, test acc: 0.728814\n",
      "3090, train loss: 0.578339, test loss: 0.600697, train acc: 0.725904, test acc: 0.728814\n",
      "3091, train loss: 0.578328, test loss: 0.600684, train acc: 0.725904, test acc: 0.728814\n",
      "3092, train loss: 0.578317, test loss: 0.600672, train acc: 0.725904, test acc: 0.728814\n",
      "3093, train loss: 0.578306, test loss: 0.600659, train acc: 0.725904, test acc: 0.728814\n",
      "3094, train loss: 0.578295, test loss: 0.600647, train acc: 0.725904, test acc: 0.728814\n",
      "3095, train loss: 0.578284, test loss: 0.600634, train acc: 0.725904, test acc: 0.728814\n",
      "3096, train loss: 0.578273, test loss: 0.600622, train acc: 0.725904, test acc: 0.728814\n",
      "3097, train loss: 0.578262, test loss: 0.600609, train acc: 0.725904, test acc: 0.728814\n",
      "3098, train loss: 0.578251, test loss: 0.600597, train acc: 0.725904, test acc: 0.728814\n",
      "3099, train loss: 0.57824, test loss: 0.600584, train acc: 0.725904, test acc: 0.728814\n",
      "3100, train loss: 0.578229, test loss: 0.600572, train acc: 0.725904, test acc: 0.728814\n",
      "3101, train loss: 0.578218, test loss: 0.600559, train acc: 0.725904, test acc: 0.728814\n",
      "3102, train loss: 0.578207, test loss: 0.600547, train acc: 0.725904, test acc: 0.728814\n",
      "3103, train loss: 0.578196, test loss: 0.600534, train acc: 0.725904, test acc: 0.728814\n",
      "3104, train loss: 0.578185, test loss: 0.600522, train acc: 0.725904, test acc: 0.728814\n",
      "3105, train loss: 0.578175, test loss: 0.600509, train acc: 0.725904, test acc: 0.728814\n",
      "3106, train loss: 0.578164, test loss: 0.600497, train acc: 0.725904, test acc: 0.728814\n",
      "3107, train loss: 0.578153, test loss: 0.600484, train acc: 0.725904, test acc: 0.728814\n",
      "3108, train loss: 0.578142, test loss: 0.600472, train acc: 0.725904, test acc: 0.728814\n",
      "3109, train loss: 0.578131, test loss: 0.600459, train acc: 0.725904, test acc: 0.728814\n",
      "3110, train loss: 0.57812, test loss: 0.600447, train acc: 0.728916, test acc: 0.728814\n",
      "3111, train loss: 0.578109, test loss: 0.600435, train acc: 0.728916, test acc: 0.728814\n",
      "3112, train loss: 0.578098, test loss: 0.600422, train acc: 0.728916, test acc: 0.728814\n",
      "3113, train loss: 0.578087, test loss: 0.60041, train acc: 0.728916, test acc: 0.728814\n",
      "3114, train loss: 0.578076, test loss: 0.600397, train acc: 0.728916, test acc: 0.728814\n",
      "3115, train loss: 0.578065, test loss: 0.600385, train acc: 0.728916, test acc: 0.728814\n",
      "3116, train loss: 0.578054, test loss: 0.600372, train acc: 0.728916, test acc: 0.728814\n",
      "3117, train loss: 0.578044, test loss: 0.60036, train acc: 0.728916, test acc: 0.728814\n",
      "3118, train loss: 0.578033, test loss: 0.600347, train acc: 0.728916, test acc: 0.728814\n",
      "3119, train loss: 0.578022, test loss: 0.600335, train acc: 0.728916, test acc: 0.728814\n",
      "3120, train loss: 0.578011, test loss: 0.600322, train acc: 0.728916, test acc: 0.728814\n",
      "3121, train loss: 0.578, test loss: 0.60031, train acc: 0.728916, test acc: 0.728814\n",
      "3122, train loss: 0.577989, test loss: 0.600297, train acc: 0.728916, test acc: 0.728814\n",
      "3123, train loss: 0.577978, test loss: 0.600285, train acc: 0.728916, test acc: 0.728814\n",
      "3124, train loss: 0.577967, test loss: 0.600272, train acc: 0.728916, test acc: 0.728814\n",
      "3125, train loss: 0.577956, test loss: 0.60026, train acc: 0.728916, test acc: 0.728814\n",
      "3126, train loss: 0.577946, test loss: 0.600248, train acc: 0.728916, test acc: 0.728814\n",
      "3127, train loss: 0.577935, test loss: 0.600235, train acc: 0.728916, test acc: 0.728814\n",
      "3128, train loss: 0.577924, test loss: 0.600223, train acc: 0.728916, test acc: 0.728814\n",
      "3129, train loss: 0.577913, test loss: 0.60021, train acc: 0.728916, test acc: 0.728814\n",
      "3130, train loss: 0.577902, test loss: 0.600198, train acc: 0.728916, test acc: 0.728814\n",
      "3131, train loss: 0.577891, test loss: 0.600185, train acc: 0.728916, test acc: 0.728814\n",
      "3132, train loss: 0.57788, test loss: 0.600173, train acc: 0.728916, test acc: 0.728814\n",
      "3133, train loss: 0.57787, test loss: 0.60016, train acc: 0.728916, test acc: 0.728814\n",
      "3134, train loss: 0.577859, test loss: 0.600148, train acc: 0.728916, test acc: 0.728814\n",
      "3135, train loss: 0.577848, test loss: 0.600136, train acc: 0.728916, test acc: 0.728814\n",
      "3136, train loss: 0.577837, test loss: 0.600123, train acc: 0.728916, test acc: 0.728814\n",
      "3137, train loss: 0.577826, test loss: 0.600111, train acc: 0.728916, test acc: 0.728814\n",
      "3138, train loss: 0.577815, test loss: 0.600098, train acc: 0.728916, test acc: 0.728814\n",
      "3139, train loss: 0.577804, test loss: 0.600086, train acc: 0.728916, test acc: 0.728814\n",
      "3140, train loss: 0.577794, test loss: 0.600073, train acc: 0.728916, test acc: 0.728814\n",
      "3141, train loss: 0.577783, test loss: 0.600061, train acc: 0.728916, test acc: 0.728814\n",
      "3142, train loss: 0.577772, test loss: 0.600049, train acc: 0.728916, test acc: 0.728814\n",
      "3143, train loss: 0.577761, test loss: 0.600036, train acc: 0.728916, test acc: 0.728814\n",
      "3144, train loss: 0.57775, test loss: 0.600024, train acc: 0.728916, test acc: 0.728814\n",
      "3145, train loss: 0.577739, test loss: 0.600011, train acc: 0.728916, test acc: 0.728814\n",
      "3146, train loss: 0.577729, test loss: 0.599999, train acc: 0.728916, test acc: 0.728814\n",
      "3147, train loss: 0.577718, test loss: 0.599986, train acc: 0.728916, test acc: 0.728814\n",
      "3148, train loss: 0.577707, test loss: 0.599974, train acc: 0.728916, test acc: 0.728814\n",
      "3149, train loss: 0.577696, test loss: 0.599962, train acc: 0.728916, test acc: 0.728814\n",
      "3150, train loss: 0.577685, test loss: 0.599949, train acc: 0.728916, test acc: 0.728814\n",
      "3151, train loss: 0.577674, test loss: 0.599937, train acc: 0.728916, test acc: 0.728814\n",
      "3152, train loss: 0.577664, test loss: 0.599924, train acc: 0.728916, test acc: 0.728814\n",
      "3153, train loss: 0.577653, test loss: 0.599912, train acc: 0.728916, test acc: 0.728814\n",
      "3154, train loss: 0.577642, test loss: 0.5999, train acc: 0.728916, test acc: 0.728814\n",
      "3155, train loss: 0.577631, test loss: 0.599887, train acc: 0.728916, test acc: 0.728814\n",
      "3156, train loss: 0.57762, test loss: 0.599875, train acc: 0.728916, test acc: 0.728814\n",
      "3157, train loss: 0.57761, test loss: 0.599862, train acc: 0.728916, test acc: 0.728814\n",
      "3158, train loss: 0.577599, test loss: 0.59985, train acc: 0.728916, test acc: 0.728814\n",
      "3159, train loss: 0.577588, test loss: 0.599838, train acc: 0.728916, test acc: 0.728814\n",
      "3160, train loss: 0.577577, test loss: 0.599825, train acc: 0.728916, test acc: 0.728814\n",
      "3161, train loss: 0.577566, test loss: 0.599813, train acc: 0.728916, test acc: 0.728814\n",
      "3162, train loss: 0.577556, test loss: 0.5998, train acc: 0.728916, test acc: 0.728814\n",
      "3163, train loss: 0.577545, test loss: 0.599788, train acc: 0.728916, test acc: 0.728814\n",
      "3164, train loss: 0.577534, test loss: 0.599776, train acc: 0.728916, test acc: 0.728814\n",
      "3165, train loss: 0.577523, test loss: 0.599763, train acc: 0.728916, test acc: 0.728814\n",
      "3166, train loss: 0.577512, test loss: 0.599751, train acc: 0.728916, test acc: 0.728814\n",
      "3167, train loss: 0.577502, test loss: 0.599738, train acc: 0.728916, test acc: 0.728814\n",
      "3168, train loss: 0.577491, test loss: 0.599726, train acc: 0.728916, test acc: 0.728814\n",
      "3169, train loss: 0.57748, test loss: 0.599714, train acc: 0.728916, test acc: 0.728814\n",
      "3170, train loss: 0.577469, test loss: 0.599701, train acc: 0.728916, test acc: 0.728814\n",
      "3171, train loss: 0.577459, test loss: 0.599689, train acc: 0.728916, test acc: 0.728814\n",
      "3172, train loss: 0.577448, test loss: 0.599677, train acc: 0.728916, test acc: 0.728814\n",
      "3173, train loss: 0.577437, test loss: 0.599664, train acc: 0.728916, test acc: 0.728814\n",
      "3174, train loss: 0.577426, test loss: 0.599652, train acc: 0.728916, test acc: 0.728814\n",
      "3175, train loss: 0.577416, test loss: 0.599639, train acc: 0.728916, test acc: 0.728814\n",
      "3176, train loss: 0.577405, test loss: 0.599627, train acc: 0.728916, test acc: 0.728814\n",
      "3177, train loss: 0.577394, test loss: 0.599615, train acc: 0.728916, test acc: 0.728814\n",
      "3178, train loss: 0.577383, test loss: 0.599602, train acc: 0.728916, test acc: 0.728814\n",
      "3179, train loss: 0.577372, test loss: 0.59959, train acc: 0.728916, test acc: 0.728814\n",
      "3180, train loss: 0.577362, test loss: 0.599578, train acc: 0.728916, test acc: 0.728814\n",
      "3181, train loss: 0.577351, test loss: 0.599565, train acc: 0.728916, test acc: 0.728814\n",
      "3182, train loss: 0.57734, test loss: 0.599553, train acc: 0.728916, test acc: 0.728814\n",
      "3183, train loss: 0.577329, test loss: 0.599541, train acc: 0.728916, test acc: 0.728814\n",
      "3184, train loss: 0.577319, test loss: 0.599528, train acc: 0.728916, test acc: 0.728814\n",
      "3185, train loss: 0.577308, test loss: 0.599516, train acc: 0.728916, test acc: 0.728814\n",
      "3186, train loss: 0.577297, test loss: 0.599504, train acc: 0.728916, test acc: 0.728814\n",
      "3187, train loss: 0.577286, test loss: 0.599491, train acc: 0.728916, test acc: 0.728814\n",
      "3188, train loss: 0.577276, test loss: 0.599479, train acc: 0.728916, test acc: 0.728814\n",
      "3189, train loss: 0.577265, test loss: 0.599467, train acc: 0.728916, test acc: 0.728814\n",
      "3190, train loss: 0.577254, test loss: 0.599454, train acc: 0.728916, test acc: 0.728814\n",
      "3191, train loss: 0.577244, test loss: 0.599442, train acc: 0.728916, test acc: 0.728814\n",
      "3192, train loss: 0.577233, test loss: 0.599429, train acc: 0.728916, test acc: 0.728814\n",
      "3193, train loss: 0.577222, test loss: 0.599417, train acc: 0.728916, test acc: 0.728814\n",
      "3194, train loss: 0.577211, test loss: 0.599405, train acc: 0.728916, test acc: 0.728814\n",
      "3195, train loss: 0.577201, test loss: 0.599392, train acc: 0.728916, test acc: 0.728814\n",
      "3196, train loss: 0.57719, test loss: 0.59938, train acc: 0.728916, test acc: 0.728814\n",
      "3197, train loss: 0.577179, test loss: 0.599368, train acc: 0.728916, test acc: 0.728814\n",
      "3198, train loss: 0.577169, test loss: 0.599356, train acc: 0.728916, test acc: 0.728814\n",
      "3199, train loss: 0.577158, test loss: 0.599343, train acc: 0.728916, test acc: 0.728814\n",
      "3200, train loss: 0.577147, test loss: 0.599331, train acc: 0.728916, test acc: 0.728814\n",
      "3201, train loss: 0.577136, test loss: 0.599319, train acc: 0.728916, test acc: 0.728814\n",
      "3202, train loss: 0.577126, test loss: 0.599306, train acc: 0.728916, test acc: 0.728814\n",
      "3203, train loss: 0.577115, test loss: 0.599294, train acc: 0.728916, test acc: 0.728814\n",
      "3204, train loss: 0.577104, test loss: 0.599282, train acc: 0.728916, test acc: 0.728814\n",
      "3205, train loss: 0.577094, test loss: 0.599269, train acc: 0.728916, test acc: 0.728814\n",
      "3206, train loss: 0.577083, test loss: 0.599257, train acc: 0.728916, test acc: 0.728814\n",
      "3207, train loss: 0.577072, test loss: 0.599245, train acc: 0.728916, test acc: 0.728814\n",
      "3208, train loss: 0.577062, test loss: 0.599232, train acc: 0.728916, test acc: 0.728814\n",
      "3209, train loss: 0.577051, test loss: 0.59922, train acc: 0.728916, test acc: 0.728814\n",
      "3210, train loss: 0.57704, test loss: 0.599208, train acc: 0.728916, test acc: 0.728814\n",
      "3211, train loss: 0.57703, test loss: 0.599195, train acc: 0.728916, test acc: 0.728814\n",
      "3212, train loss: 0.577019, test loss: 0.599183, train acc: 0.728916, test acc: 0.728814\n",
      "3213, train loss: 0.577008, test loss: 0.599171, train acc: 0.728916, test acc: 0.728814\n",
      "3214, train loss: 0.576998, test loss: 0.599158, train acc: 0.728916, test acc: 0.728814\n",
      "3215, train loss: 0.576987, test loss: 0.599146, train acc: 0.728916, test acc: 0.728814\n",
      "3216, train loss: 0.576976, test loss: 0.599134, train acc: 0.728916, test acc: 0.728814\n",
      "3217, train loss: 0.576965, test loss: 0.599122, train acc: 0.728916, test acc: 0.728814\n",
      "3218, train loss: 0.576955, test loss: 0.599109, train acc: 0.728916, test acc: 0.728814\n",
      "3219, train loss: 0.576944, test loss: 0.599097, train acc: 0.728916, test acc: 0.728814\n",
      "3220, train loss: 0.576934, test loss: 0.599085, train acc: 0.728916, test acc: 0.728814\n",
      "3221, train loss: 0.576923, test loss: 0.599072, train acc: 0.728916, test acc: 0.728814\n",
      "3222, train loss: 0.576912, test loss: 0.59906, train acc: 0.728916, test acc: 0.728814\n",
      "3223, train loss: 0.576902, test loss: 0.599048, train acc: 0.728916, test acc: 0.728814\n",
      "3224, train loss: 0.576891, test loss: 0.599036, train acc: 0.728916, test acc: 0.728814\n",
      "3225, train loss: 0.57688, test loss: 0.599023, train acc: 0.728916, test acc: 0.728814\n",
      "3226, train loss: 0.57687, test loss: 0.599011, train acc: 0.728916, test acc: 0.728814\n",
      "3227, train loss: 0.576859, test loss: 0.598999, train acc: 0.728916, test acc: 0.728814\n",
      "3228, train loss: 0.576848, test loss: 0.598986, train acc: 0.728916, test acc: 0.728814\n",
      "3229, train loss: 0.576838, test loss: 0.598974, train acc: 0.728916, test acc: 0.728814\n",
      "3230, train loss: 0.576827, test loss: 0.598962, train acc: 0.728916, test acc: 0.728814\n",
      "3231, train loss: 0.576816, test loss: 0.59895, train acc: 0.728916, test acc: 0.728814\n",
      "3232, train loss: 0.576806, test loss: 0.598937, train acc: 0.728916, test acc: 0.728814\n",
      "3233, train loss: 0.576795, test loss: 0.598925, train acc: 0.728916, test acc: 0.728814\n",
      "3234, train loss: 0.576784, test loss: 0.598913, train acc: 0.728916, test acc: 0.728814\n",
      "3235, train loss: 0.576774, test loss: 0.5989, train acc: 0.728916, test acc: 0.728814\n",
      "3236, train loss: 0.576763, test loss: 0.598888, train acc: 0.728916, test acc: 0.728814\n",
      "3237, train loss: 0.576753, test loss: 0.598876, train acc: 0.728916, test acc: 0.728814\n",
      "3238, train loss: 0.576742, test loss: 0.598864, train acc: 0.728916, test acc: 0.728814\n",
      "3239, train loss: 0.576731, test loss: 0.598852, train acc: 0.728916, test acc: 0.728814\n",
      "3240, train loss: 0.576721, test loss: 0.598839, train acc: 0.728916, test acc: 0.728814\n",
      "3241, train loss: 0.57671, test loss: 0.598827, train acc: 0.728916, test acc: 0.728814\n",
      "3242, train loss: 0.576699, test loss: 0.598815, train acc: 0.728916, test acc: 0.728814\n",
      "3243, train loss: 0.576689, test loss: 0.598802, train acc: 0.728916, test acc: 0.728814\n",
      "3244, train loss: 0.576678, test loss: 0.59879, train acc: 0.728916, test acc: 0.728814\n",
      "3245, train loss: 0.576668, test loss: 0.598778, train acc: 0.728916, test acc: 0.728814\n",
      "3246, train loss: 0.576657, test loss: 0.598766, train acc: 0.728916, test acc: 0.728814\n",
      "3247, train loss: 0.576647, test loss: 0.598753, train acc: 0.728916, test acc: 0.728814\n",
      "3248, train loss: 0.576636, test loss: 0.598741, train acc: 0.728916, test acc: 0.728814\n",
      "3249, train loss: 0.576625, test loss: 0.598729, train acc: 0.728916, test acc: 0.728814\n",
      "3250, train loss: 0.576615, test loss: 0.598717, train acc: 0.728916, test acc: 0.728814\n",
      "3251, train loss: 0.576604, test loss: 0.598704, train acc: 0.728916, test acc: 0.728814\n",
      "3252, train loss: 0.576594, test loss: 0.598692, train acc: 0.728916, test acc: 0.728814\n",
      "3253, train loss: 0.576583, test loss: 0.59868, train acc: 0.728916, test acc: 0.728814\n",
      "3254, train loss: 0.576572, test loss: 0.598668, train acc: 0.728916, test acc: 0.728814\n",
      "3255, train loss: 0.576562, test loss: 0.598656, train acc: 0.728916, test acc: 0.728814\n",
      "3256, train loss: 0.576551, test loss: 0.598643, train acc: 0.728916, test acc: 0.728814\n",
      "3257, train loss: 0.576541, test loss: 0.598631, train acc: 0.728916, test acc: 0.728814\n",
      "3258, train loss: 0.57653, test loss: 0.598619, train acc: 0.728916, test acc: 0.728814\n",
      "3259, train loss: 0.576519, test loss: 0.598607, train acc: 0.728916, test acc: 0.728814\n",
      "3260, train loss: 0.576509, test loss: 0.598594, train acc: 0.728916, test acc: 0.728814\n",
      "3261, train loss: 0.576498, test loss: 0.598582, train acc: 0.728916, test acc: 0.728814\n",
      "3262, train loss: 0.576488, test loss: 0.59857, train acc: 0.728916, test acc: 0.728814\n",
      "3263, train loss: 0.576477, test loss: 0.598558, train acc: 0.728916, test acc: 0.728814\n",
      "3264, train loss: 0.576467, test loss: 0.598546, train acc: 0.728916, test acc: 0.728814\n",
      "3265, train loss: 0.576456, test loss: 0.598533, train acc: 0.728916, test acc: 0.728814\n",
      "3266, train loss: 0.576446, test loss: 0.598521, train acc: 0.728916, test acc: 0.728814\n",
      "3267, train loss: 0.576435, test loss: 0.598509, train acc: 0.728916, test acc: 0.728814\n",
      "3268, train loss: 0.576424, test loss: 0.598497, train acc: 0.728916, test acc: 0.728814\n",
      "3269, train loss: 0.576414, test loss: 0.598485, train acc: 0.728916, test acc: 0.728814\n",
      "3270, train loss: 0.576403, test loss: 0.598472, train acc: 0.728916, test acc: 0.728814\n",
      "3271, train loss: 0.576393, test loss: 0.59846, train acc: 0.728916, test acc: 0.728814\n",
      "3272, train loss: 0.576382, test loss: 0.598448, train acc: 0.728916, test acc: 0.728814\n",
      "3273, train loss: 0.576372, test loss: 0.598436, train acc: 0.728916, test acc: 0.728814\n",
      "3274, train loss: 0.576361, test loss: 0.598423, train acc: 0.728916, test acc: 0.728814\n",
      "3275, train loss: 0.576351, test loss: 0.598411, train acc: 0.728916, test acc: 0.728814\n",
      "3276, train loss: 0.57634, test loss: 0.598399, train acc: 0.728916, test acc: 0.728814\n",
      "3277, train loss: 0.57633, test loss: 0.598387, train acc: 0.728916, test acc: 0.728814\n",
      "3278, train loss: 0.576319, test loss: 0.598375, train acc: 0.728916, test acc: 0.728814\n",
      "3279, train loss: 0.576308, test loss: 0.598362, train acc: 0.728916, test acc: 0.728814\n",
      "3280, train loss: 0.576298, test loss: 0.59835, train acc: 0.728916, test acc: 0.728814\n",
      "3281, train loss: 0.576287, test loss: 0.598338, train acc: 0.728916, test acc: 0.728814\n",
      "3282, train loss: 0.576277, test loss: 0.598326, train acc: 0.728916, test acc: 0.728814\n",
      "3283, train loss: 0.576266, test loss: 0.598314, train acc: 0.728916, test acc: 0.728814\n",
      "3284, train loss: 0.576256, test loss: 0.598302, train acc: 0.728916, test acc: 0.728814\n",
      "3285, train loss: 0.576245, test loss: 0.598289, train acc: 0.728916, test acc: 0.728814\n",
      "3286, train loss: 0.576235, test loss: 0.598277, train acc: 0.728916, test acc: 0.728814\n",
      "3287, train loss: 0.576224, test loss: 0.598265, train acc: 0.728916, test acc: 0.728814\n",
      "3288, train loss: 0.576214, test loss: 0.598253, train acc: 0.728916, test acc: 0.728814\n",
      "3289, train loss: 0.576203, test loss: 0.598241, train acc: 0.728916, test acc: 0.728814\n",
      "3290, train loss: 0.576193, test loss: 0.598228, train acc: 0.728916, test acc: 0.728814\n",
      "3291, train loss: 0.576182, test loss: 0.598216, train acc: 0.728916, test acc: 0.728814\n",
      "3292, train loss: 0.576172, test loss: 0.598204, train acc: 0.728916, test acc: 0.728814\n",
      "3293, train loss: 0.576161, test loss: 0.598192, train acc: 0.728916, test acc: 0.728814\n",
      "3294, train loss: 0.576151, test loss: 0.59818, train acc: 0.728916, test acc: 0.728814\n",
      "3295, train loss: 0.57614, test loss: 0.598168, train acc: 0.728916, test acc: 0.728814\n",
      "3296, train loss: 0.57613, test loss: 0.598155, train acc: 0.728916, test acc: 0.728814\n",
      "3297, train loss: 0.576119, test loss: 0.598143, train acc: 0.728916, test acc: 0.728814\n",
      "3298, train loss: 0.576109, test loss: 0.598131, train acc: 0.728916, test acc: 0.728814\n",
      "3299, train loss: 0.576098, test loss: 0.598119, train acc: 0.728916, test acc: 0.728814\n",
      "3300, train loss: 0.576088, test loss: 0.598107, train acc: 0.728916, test acc: 0.728814\n",
      "3301, train loss: 0.576077, test loss: 0.598095, train acc: 0.728916, test acc: 0.728814\n",
      "3302, train loss: 0.576067, test loss: 0.598082, train acc: 0.728916, test acc: 0.728814\n",
      "3303, train loss: 0.576056, test loss: 0.59807, train acc: 0.728916, test acc: 0.728814\n",
      "3304, train loss: 0.576046, test loss: 0.598058, train acc: 0.728916, test acc: 0.728814\n",
      "3305, train loss: 0.576035, test loss: 0.598046, train acc: 0.728916, test acc: 0.728814\n",
      "3306, train loss: 0.576025, test loss: 0.598034, train acc: 0.728916, test acc: 0.728814\n",
      "3307, train loss: 0.576014, test loss: 0.598022, train acc: 0.728916, test acc: 0.728814\n",
      "3308, train loss: 0.576004, test loss: 0.59801, train acc: 0.728916, test acc: 0.728814\n",
      "3309, train loss: 0.575993, test loss: 0.597997, train acc: 0.728916, test acc: 0.728814\n",
      "3310, train loss: 0.575983, test loss: 0.597985, train acc: 0.728916, test acc: 0.728814\n",
      "3311, train loss: 0.575973, test loss: 0.597973, train acc: 0.728916, test acc: 0.728814\n",
      "3312, train loss: 0.575962, test loss: 0.597961, train acc: 0.728916, test acc: 0.728814\n",
      "3313, train loss: 0.575952, test loss: 0.597949, train acc: 0.728916, test acc: 0.728814\n",
      "3314, train loss: 0.575941, test loss: 0.597937, train acc: 0.728916, test acc: 0.728814\n",
      "3315, train loss: 0.575931, test loss: 0.597925, train acc: 0.728916, test acc: 0.728814\n",
      "3316, train loss: 0.57592, test loss: 0.597912, train acc: 0.728916, test acc: 0.728814\n",
      "3317, train loss: 0.57591, test loss: 0.5979, train acc: 0.728916, test acc: 0.728814\n",
      "3318, train loss: 0.575899, test loss: 0.597888, train acc: 0.728916, test acc: 0.728814\n",
      "3319, train loss: 0.575889, test loss: 0.597876, train acc: 0.728916, test acc: 0.728814\n",
      "3320, train loss: 0.575878, test loss: 0.597864, train acc: 0.728916, test acc: 0.728814\n",
      "3321, train loss: 0.575868, test loss: 0.597852, train acc: 0.728916, test acc: 0.728814\n",
      "3322, train loss: 0.575858, test loss: 0.59784, train acc: 0.728916, test acc: 0.728814\n",
      "3323, train loss: 0.575847, test loss: 0.597827, train acc: 0.728916, test acc: 0.728814\n",
      "3324, train loss: 0.575837, test loss: 0.597815, train acc: 0.728916, test acc: 0.728814\n",
      "3325, train loss: 0.575826, test loss: 0.597803, train acc: 0.728916, test acc: 0.728814\n",
      "3326, train loss: 0.575816, test loss: 0.597791, train acc: 0.728916, test acc: 0.728814\n",
      "3327, train loss: 0.575805, test loss: 0.597779, train acc: 0.728916, test acc: 0.728814\n",
      "3328, train loss: 0.575795, test loss: 0.597767, train acc: 0.728916, test acc: 0.728814\n",
      "3329, train loss: 0.575785, test loss: 0.597755, train acc: 0.728916, test acc: 0.728814\n",
      "3330, train loss: 0.575774, test loss: 0.597743, train acc: 0.728916, test acc: 0.728814\n",
      "3331, train loss: 0.575764, test loss: 0.597731, train acc: 0.728916, test acc: 0.728814\n",
      "3332, train loss: 0.575753, test loss: 0.597718, train acc: 0.728916, test acc: 0.728814\n",
      "3333, train loss: 0.575743, test loss: 0.597706, train acc: 0.728916, test acc: 0.728814\n",
      "3334, train loss: 0.575732, test loss: 0.597694, train acc: 0.728916, test acc: 0.728814\n",
      "3335, train loss: 0.575722, test loss: 0.597682, train acc: 0.728916, test acc: 0.728814\n",
      "3336, train loss: 0.575712, test loss: 0.59767, train acc: 0.728916, test acc: 0.728814\n",
      "3337, train loss: 0.575701, test loss: 0.597658, train acc: 0.728916, test acc: 0.728814\n",
      "3338, train loss: 0.575691, test loss: 0.597646, train acc: 0.728916, test acc: 0.728814\n",
      "3339, train loss: 0.57568, test loss: 0.597634, train acc: 0.728916, test acc: 0.728814\n",
      "3340, train loss: 0.57567, test loss: 0.597622, train acc: 0.728916, test acc: 0.728814\n",
      "3341, train loss: 0.57566, test loss: 0.59761, train acc: 0.728916, test acc: 0.728814\n",
      "3342, train loss: 0.575649, test loss: 0.597597, train acc: 0.728916, test acc: 0.728814\n",
      "3343, train loss: 0.575639, test loss: 0.597585, train acc: 0.728916, test acc: 0.728814\n",
      "3344, train loss: 0.575628, test loss: 0.597573, train acc: 0.728916, test acc: 0.728814\n",
      "3345, train loss: 0.575618, test loss: 0.597561, train acc: 0.728916, test acc: 0.728814\n",
      "3346, train loss: 0.575608, test loss: 0.597549, train acc: 0.728916, test acc: 0.728814\n",
      "3347, train loss: 0.575597, test loss: 0.597537, train acc: 0.728916, test acc: 0.728814\n",
      "3348, train loss: 0.575587, test loss: 0.597525, train acc: 0.728916, test acc: 0.728814\n",
      "3349, train loss: 0.575576, test loss: 0.597513, train acc: 0.728916, test acc: 0.728814\n",
      "3350, train loss: 0.575566, test loss: 0.597501, train acc: 0.728916, test acc: 0.728814\n",
      "3351, train loss: 0.575556, test loss: 0.597489, train acc: 0.728916, test acc: 0.728814\n",
      "3352, train loss: 0.575545, test loss: 0.597476, train acc: 0.728916, test acc: 0.728814\n",
      "3353, train loss: 0.575535, test loss: 0.597465, train acc: 0.728916, test acc: 0.728814\n",
      "3354, train loss: 0.575525, test loss: 0.597452, train acc: 0.728916, test acc: 0.728814\n",
      "3355, train loss: 0.575514, test loss: 0.59744, train acc: 0.728916, test acc: 0.728814\n",
      "3356, train loss: 0.575504, test loss: 0.597428, train acc: 0.728916, test acc: 0.728814\n",
      "3357, train loss: 0.575493, test loss: 0.597416, train acc: 0.728916, test acc: 0.728814\n",
      "3358, train loss: 0.575483, test loss: 0.597404, train acc: 0.728916, test acc: 0.728814\n",
      "3359, train loss: 0.575473, test loss: 0.597392, train acc: 0.728916, test acc: 0.728814\n",
      "3360, train loss: 0.575462, test loss: 0.59738, train acc: 0.728916, test acc: 0.728814\n",
      "3361, train loss: 0.575452, test loss: 0.597368, train acc: 0.728916, test acc: 0.728814\n",
      "3362, train loss: 0.575442, test loss: 0.597356, train acc: 0.728916, test acc: 0.728814\n",
      "3363, train loss: 0.575431, test loss: 0.597344, train acc: 0.728916, test acc: 0.728814\n",
      "3364, train loss: 0.575421, test loss: 0.597332, train acc: 0.728916, test acc: 0.728814\n",
      "3365, train loss: 0.575411, test loss: 0.59732, train acc: 0.728916, test acc: 0.728814\n",
      "3366, train loss: 0.5754, test loss: 0.597308, train acc: 0.728916, test acc: 0.728814\n",
      "3367, train loss: 0.57539, test loss: 0.597296, train acc: 0.728916, test acc: 0.728814\n",
      "3368, train loss: 0.57538, test loss: 0.597283, train acc: 0.728916, test acc: 0.728814\n",
      "3369, train loss: 0.575369, test loss: 0.597271, train acc: 0.728916, test acc: 0.728814\n",
      "3370, train loss: 0.575359, test loss: 0.597259, train acc: 0.728916, test acc: 0.728814\n",
      "3371, train loss: 0.575348, test loss: 0.597247, train acc: 0.728916, test acc: 0.728814\n",
      "3372, train loss: 0.575338, test loss: 0.597235, train acc: 0.731928, test acc: 0.728814\n",
      "3373, train loss: 0.575328, test loss: 0.597223, train acc: 0.731928, test acc: 0.728814\n",
      "3374, train loss: 0.575318, test loss: 0.597211, train acc: 0.731928, test acc: 0.728814\n",
      "3375, train loss: 0.575307, test loss: 0.597199, train acc: 0.731928, test acc: 0.728814\n",
      "3376, train loss: 0.575297, test loss: 0.597187, train acc: 0.731928, test acc: 0.728814\n",
      "3377, train loss: 0.575287, test loss: 0.597175, train acc: 0.731928, test acc: 0.728814\n",
      "3378, train loss: 0.575276, test loss: 0.597163, train acc: 0.731928, test acc: 0.728814\n",
      "3379, train loss: 0.575266, test loss: 0.597151, train acc: 0.731928, test acc: 0.728814\n",
      "3380, train loss: 0.575256, test loss: 0.597139, train acc: 0.731928, test acc: 0.728814\n",
      "3381, train loss: 0.575245, test loss: 0.597127, train acc: 0.731928, test acc: 0.728814\n",
      "3382, train loss: 0.575235, test loss: 0.597115, train acc: 0.731928, test acc: 0.728814\n",
      "3383, train loss: 0.575225, test loss: 0.597103, train acc: 0.731928, test acc: 0.728814\n",
      "3384, train loss: 0.575214, test loss: 0.597091, train acc: 0.731928, test acc: 0.728814\n",
      "3385, train loss: 0.575204, test loss: 0.597079, train acc: 0.731928, test acc: 0.728814\n",
      "3386, train loss: 0.575194, test loss: 0.597067, train acc: 0.731928, test acc: 0.728814\n",
      "3387, train loss: 0.575183, test loss: 0.597055, train acc: 0.731928, test acc: 0.728814\n",
      "3388, train loss: 0.575173, test loss: 0.597043, train acc: 0.731928, test acc: 0.728814\n",
      "3389, train loss: 0.575163, test loss: 0.597031, train acc: 0.731928, test acc: 0.728814\n",
      "3390, train loss: 0.575152, test loss: 0.597019, train acc: 0.731928, test acc: 0.728814\n",
      "3391, train loss: 0.575142, test loss: 0.597007, train acc: 0.731928, test acc: 0.728814\n",
      "3392, train loss: 0.575132, test loss: 0.596995, train acc: 0.731928, test acc: 0.728814\n",
      "3393, train loss: 0.575122, test loss: 0.596983, train acc: 0.731928, test acc: 0.728814\n",
      "3394, train loss: 0.575111, test loss: 0.596971, train acc: 0.731928, test acc: 0.728814\n",
      "3395, train loss: 0.575101, test loss: 0.596959, train acc: 0.731928, test acc: 0.728814\n",
      "3396, train loss: 0.575091, test loss: 0.596947, train acc: 0.731928, test acc: 0.728814\n",
      "3397, train loss: 0.57508, test loss: 0.596935, train acc: 0.731928, test acc: 0.728814\n",
      "3398, train loss: 0.57507, test loss: 0.596923, train acc: 0.731928, test acc: 0.728814\n",
      "3399, train loss: 0.57506, test loss: 0.596911, train acc: 0.731928, test acc: 0.728814\n",
      "3400, train loss: 0.575049, test loss: 0.596899, train acc: 0.731928, test acc: 0.728814\n",
      "3401, train loss: 0.575039, test loss: 0.596887, train acc: 0.731928, test acc: 0.728814\n",
      "3402, train loss: 0.575029, test loss: 0.596875, train acc: 0.731928, test acc: 0.728814\n",
      "3403, train loss: 0.575019, test loss: 0.596863, train acc: 0.731928, test acc: 0.728814\n",
      "3404, train loss: 0.575008, test loss: 0.596851, train acc: 0.731928, test acc: 0.728814\n",
      "3405, train loss: 0.574998, test loss: 0.596839, train acc: 0.731928, test acc: 0.728814\n",
      "3406, train loss: 0.574988, test loss: 0.596827, train acc: 0.731928, test acc: 0.728814\n",
      "3407, train loss: 0.574978, test loss: 0.596815, train acc: 0.731928, test acc: 0.728814\n",
      "3408, train loss: 0.574967, test loss: 0.596803, train acc: 0.731928, test acc: 0.728814\n",
      "3409, train loss: 0.574957, test loss: 0.596791, train acc: 0.731928, test acc: 0.728814\n",
      "3410, train loss: 0.574947, test loss: 0.596779, train acc: 0.731928, test acc: 0.728814\n",
      "3411, train loss: 0.574937, test loss: 0.596767, train acc: 0.731928, test acc: 0.728814\n",
      "3412, train loss: 0.574926, test loss: 0.596755, train acc: 0.731928, test acc: 0.728814\n",
      "3413, train loss: 0.574916, test loss: 0.596743, train acc: 0.731928, test acc: 0.728814\n",
      "3414, train loss: 0.574906, test loss: 0.596731, train acc: 0.731928, test acc: 0.728814\n",
      "3415, train loss: 0.574896, test loss: 0.596719, train acc: 0.731928, test acc: 0.728814\n",
      "3416, train loss: 0.574885, test loss: 0.596707, train acc: 0.731928, test acc: 0.728814\n",
      "3417, train loss: 0.574875, test loss: 0.596695, train acc: 0.731928, test acc: 0.728814\n",
      "3418, train loss: 0.574865, test loss: 0.596683, train acc: 0.731928, test acc: 0.728814\n",
      "3419, train loss: 0.574855, test loss: 0.596671, train acc: 0.731928, test acc: 0.728814\n",
      "3420, train loss: 0.574844, test loss: 0.596659, train acc: 0.731928, test acc: 0.728814\n",
      "3421, train loss: 0.574834, test loss: 0.596647, train acc: 0.731928, test acc: 0.728814\n",
      "3422, train loss: 0.574824, test loss: 0.596635, train acc: 0.731928, test acc: 0.728814\n",
      "3423, train loss: 0.574814, test loss: 0.596623, train acc: 0.731928, test acc: 0.728814\n",
      "3424, train loss: 0.574803, test loss: 0.596611, train acc: 0.731928, test acc: 0.728814\n",
      "3425, train loss: 0.574793, test loss: 0.596599, train acc: 0.731928, test acc: 0.728814\n",
      "3426, train loss: 0.574783, test loss: 0.596587, train acc: 0.731928, test acc: 0.728814\n",
      "3427, train loss: 0.574773, test loss: 0.596575, train acc: 0.731928, test acc: 0.728814\n",
      "3428, train loss: 0.574762, test loss: 0.596563, train acc: 0.731928, test acc: 0.728814\n",
      "3429, train loss: 0.574752, test loss: 0.596551, train acc: 0.731928, test acc: 0.728814\n",
      "3430, train loss: 0.574742, test loss: 0.596539, train acc: 0.731928, test acc: 0.728814\n",
      "3431, train loss: 0.574732, test loss: 0.596527, train acc: 0.731928, test acc: 0.728814\n",
      "3432, train loss: 0.574722, test loss: 0.596515, train acc: 0.731928, test acc: 0.728814\n",
      "3433, train loss: 0.574711, test loss: 0.596503, train acc: 0.731928, test acc: 0.728814\n",
      "3434, train loss: 0.574701, test loss: 0.596491, train acc: 0.731928, test acc: 0.728814\n",
      "3435, train loss: 0.574691, test loss: 0.59648, train acc: 0.731928, test acc: 0.728814\n",
      "3436, train loss: 0.574681, test loss: 0.596468, train acc: 0.731928, test acc: 0.728814\n",
      "3437, train loss: 0.57467, test loss: 0.596456, train acc: 0.731928, test acc: 0.728814\n",
      "3438, train loss: 0.57466, test loss: 0.596444, train acc: 0.731928, test acc: 0.728814\n",
      "3439, train loss: 0.57465, test loss: 0.596432, train acc: 0.731928, test acc: 0.728814\n",
      "3440, train loss: 0.57464, test loss: 0.59642, train acc: 0.731928, test acc: 0.728814\n",
      "3441, train loss: 0.57463, test loss: 0.596408, train acc: 0.731928, test acc: 0.728814\n",
      "3442, train loss: 0.57462, test loss: 0.596396, train acc: 0.731928, test acc: 0.728814\n",
      "3443, train loss: 0.574609, test loss: 0.596384, train acc: 0.731928, test acc: 0.728814\n",
      "3444, train loss: 0.574599, test loss: 0.596372, train acc: 0.731928, test acc: 0.728814\n",
      "3445, train loss: 0.574589, test loss: 0.59636, train acc: 0.731928, test acc: 0.728814\n",
      "3446, train loss: 0.574579, test loss: 0.596348, train acc: 0.731928, test acc: 0.728814\n",
      "3447, train loss: 0.574569, test loss: 0.596336, train acc: 0.731928, test acc: 0.728814\n",
      "3448, train loss: 0.574558, test loss: 0.596324, train acc: 0.731928, test acc: 0.728814\n",
      "3449, train loss: 0.574548, test loss: 0.596312, train acc: 0.731928, test acc: 0.728814\n",
      "3450, train loss: 0.574538, test loss: 0.5963, train acc: 0.731928, test acc: 0.728814\n",
      "3451, train loss: 0.574528, test loss: 0.596288, train acc: 0.73494, test acc: 0.728814\n",
      "3452, train loss: 0.574518, test loss: 0.596277, train acc: 0.73494, test acc: 0.728814\n",
      "3453, train loss: 0.574507, test loss: 0.596265, train acc: 0.73494, test acc: 0.728814\n",
      "3454, train loss: 0.574497, test loss: 0.596253, train acc: 0.73494, test acc: 0.728814\n",
      "3455, train loss: 0.574487, test loss: 0.596241, train acc: 0.73494, test acc: 0.728814\n",
      "3456, train loss: 0.574477, test loss: 0.596229, train acc: 0.73494, test acc: 0.728814\n",
      "3457, train loss: 0.574467, test loss: 0.596217, train acc: 0.73494, test acc: 0.728814\n",
      "3458, train loss: 0.574457, test loss: 0.596205, train acc: 0.73494, test acc: 0.728814\n",
      "3459, train loss: 0.574446, test loss: 0.596193, train acc: 0.73494, test acc: 0.728814\n",
      "3460, train loss: 0.574436, test loss: 0.596181, train acc: 0.73494, test acc: 0.728814\n",
      "3461, train loss: 0.574426, test loss: 0.596169, train acc: 0.73494, test acc: 0.728814\n",
      "3462, train loss: 0.574416, test loss: 0.596157, train acc: 0.73494, test acc: 0.728814\n",
      "3463, train loss: 0.574406, test loss: 0.596145, train acc: 0.73494, test acc: 0.728814\n",
      "3464, train loss: 0.574396, test loss: 0.596134, train acc: 0.73494, test acc: 0.728814\n",
      "3465, train loss: 0.574385, test loss: 0.596122, train acc: 0.73494, test acc: 0.728814\n",
      "3466, train loss: 0.574375, test loss: 0.59611, train acc: 0.73494, test acc: 0.728814\n",
      "3467, train loss: 0.574365, test loss: 0.596098, train acc: 0.73494, test acc: 0.728814\n",
      "3468, train loss: 0.574355, test loss: 0.596086, train acc: 0.73494, test acc: 0.728814\n",
      "3469, train loss: 0.574345, test loss: 0.596074, train acc: 0.73494, test acc: 0.728814\n",
      "3470, train loss: 0.574335, test loss: 0.596062, train acc: 0.73494, test acc: 0.728814\n",
      "3471, train loss: 0.574325, test loss: 0.59605, train acc: 0.73494, test acc: 0.728814\n",
      "3472, train loss: 0.574314, test loss: 0.596038, train acc: 0.73494, test acc: 0.728814\n",
      "3473, train loss: 0.574304, test loss: 0.596026, train acc: 0.73494, test acc: 0.728814\n",
      "3474, train loss: 0.574294, test loss: 0.596015, train acc: 0.73494, test acc: 0.728814\n",
      "3475, train loss: 0.574284, test loss: 0.596003, train acc: 0.73494, test acc: 0.728814\n",
      "3476, train loss: 0.574274, test loss: 0.595991, train acc: 0.73494, test acc: 0.728814\n",
      "3477, train loss: 0.574264, test loss: 0.595979, train acc: 0.73494, test acc: 0.728814\n",
      "3478, train loss: 0.574254, test loss: 0.595967, train acc: 0.73494, test acc: 0.728814\n",
      "3479, train loss: 0.574243, test loss: 0.595955, train acc: 0.73494, test acc: 0.728814\n",
      "3480, train loss: 0.574233, test loss: 0.595943, train acc: 0.73494, test acc: 0.728814\n",
      "3481, train loss: 0.574223, test loss: 0.595931, train acc: 0.73494, test acc: 0.728814\n",
      "3482, train loss: 0.574213, test loss: 0.59592, train acc: 0.73494, test acc: 0.728814\n",
      "3483, train loss: 0.574203, test loss: 0.595908, train acc: 0.73494, test acc: 0.728814\n",
      "3484, train loss: 0.574193, test loss: 0.595896, train acc: 0.73494, test acc: 0.728814\n",
      "3485, train loss: 0.574183, test loss: 0.595884, train acc: 0.73494, test acc: 0.728814\n",
      "3486, train loss: 0.574173, test loss: 0.595872, train acc: 0.73494, test acc: 0.728814\n",
      "3487, train loss: 0.574162, test loss: 0.59586, train acc: 0.73494, test acc: 0.728814\n",
      "3488, train loss: 0.574152, test loss: 0.595848, train acc: 0.73494, test acc: 0.728814\n",
      "3489, train loss: 0.574142, test loss: 0.595836, train acc: 0.73494, test acc: 0.728814\n",
      "3490, train loss: 0.574132, test loss: 0.595825, train acc: 0.73494, test acc: 0.728814\n",
      "3491, train loss: 0.574122, test loss: 0.595813, train acc: 0.73494, test acc: 0.728814\n",
      "3492, train loss: 0.574112, test loss: 0.595801, train acc: 0.73494, test acc: 0.728814\n",
      "3493, train loss: 0.574102, test loss: 0.595789, train acc: 0.73494, test acc: 0.728814\n",
      "3494, train loss: 0.574092, test loss: 0.595777, train acc: 0.73494, test acc: 0.728814\n",
      "3495, train loss: 0.574082, test loss: 0.595765, train acc: 0.73494, test acc: 0.728814\n",
      "3496, train loss: 0.574072, test loss: 0.595753, train acc: 0.73494, test acc: 0.728814\n",
      "3497, train loss: 0.574061, test loss: 0.595741, train acc: 0.73494, test acc: 0.728814\n",
      "3498, train loss: 0.574051, test loss: 0.59573, train acc: 0.73494, test acc: 0.728814\n",
      "3499, train loss: 0.574041, test loss: 0.595718, train acc: 0.73494, test acc: 0.728814\n",
      "3500, train loss: 0.574031, test loss: 0.595706, train acc: 0.73494, test acc: 0.728814\n",
      "3501, train loss: 0.574021, test loss: 0.595694, train acc: 0.73494, test acc: 0.728814\n",
      "3502, train loss: 0.574011, test loss: 0.595682, train acc: 0.73494, test acc: 0.728814\n",
      "3503, train loss: 0.574001, test loss: 0.59567, train acc: 0.73494, test acc: 0.728814\n",
      "3504, train loss: 0.573991, test loss: 0.595659, train acc: 0.73494, test acc: 0.728814\n",
      "3505, train loss: 0.573981, test loss: 0.595647, train acc: 0.73494, test acc: 0.728814\n",
      "3506, train loss: 0.573971, test loss: 0.595635, train acc: 0.73494, test acc: 0.728814\n",
      "3507, train loss: 0.573961, test loss: 0.595623, train acc: 0.73494, test acc: 0.728814\n",
      "3508, train loss: 0.573951, test loss: 0.595611, train acc: 0.73494, test acc: 0.728814\n",
      "3509, train loss: 0.573941, test loss: 0.595599, train acc: 0.73494, test acc: 0.728814\n",
      "3510, train loss: 0.57393, test loss: 0.595587, train acc: 0.73494, test acc: 0.728814\n",
      "3511, train loss: 0.57392, test loss: 0.595576, train acc: 0.73494, test acc: 0.728814\n",
      "3512, train loss: 0.57391, test loss: 0.595564, train acc: 0.73494, test acc: 0.728814\n",
      "3513, train loss: 0.5739, test loss: 0.595552, train acc: 0.73494, test acc: 0.728814\n",
      "3514, train loss: 0.57389, test loss: 0.59554, train acc: 0.73494, test acc: 0.728814\n",
      "3515, train loss: 0.57388, test loss: 0.595528, train acc: 0.73494, test acc: 0.728814\n",
      "3516, train loss: 0.57387, test loss: 0.595516, train acc: 0.73494, test acc: 0.728814\n",
      "3517, train loss: 0.57386, test loss: 0.595505, train acc: 0.73494, test acc: 0.728814\n",
      "3518, train loss: 0.57385, test loss: 0.595493, train acc: 0.73494, test acc: 0.728814\n",
      "3519, train loss: 0.57384, test loss: 0.595481, train acc: 0.73494, test acc: 0.728814\n",
      "3520, train loss: 0.57383, test loss: 0.595469, train acc: 0.73494, test acc: 0.728814\n",
      "3521, train loss: 0.57382, test loss: 0.595457, train acc: 0.73494, test acc: 0.728814\n",
      "3522, train loss: 0.57381, test loss: 0.595446, train acc: 0.73494, test acc: 0.728814\n",
      "3523, train loss: 0.5738, test loss: 0.595434, train acc: 0.73494, test acc: 0.728814\n",
      "3524, train loss: 0.57379, test loss: 0.595422, train acc: 0.73494, test acc: 0.728814\n",
      "3525, train loss: 0.57378, test loss: 0.59541, train acc: 0.73494, test acc: 0.728814\n",
      "3526, train loss: 0.57377, test loss: 0.595398, train acc: 0.73494, test acc: 0.728814\n",
      "3527, train loss: 0.573759, test loss: 0.595387, train acc: 0.73494, test acc: 0.728814\n",
      "3528, train loss: 0.573749, test loss: 0.595375, train acc: 0.73494, test acc: 0.728814\n",
      "3529, train loss: 0.573739, test loss: 0.595363, train acc: 0.73494, test acc: 0.728814\n",
      "3530, train loss: 0.573729, test loss: 0.595351, train acc: 0.73494, test acc: 0.728814\n",
      "3531, train loss: 0.573719, test loss: 0.595339, train acc: 0.73494, test acc: 0.728814\n",
      "3532, train loss: 0.573709, test loss: 0.595327, train acc: 0.73494, test acc: 0.728814\n",
      "3533, train loss: 0.573699, test loss: 0.595316, train acc: 0.73494, test acc: 0.728814\n",
      "3534, train loss: 0.573689, test loss: 0.595304, train acc: 0.73494, test acc: 0.728814\n",
      "3535, train loss: 0.573679, test loss: 0.595292, train acc: 0.73494, test acc: 0.728814\n",
      "3536, train loss: 0.573669, test loss: 0.59528, train acc: 0.73494, test acc: 0.728814\n",
      "3537, train loss: 0.573659, test loss: 0.595268, train acc: 0.73494, test acc: 0.728814\n",
      "3538, train loss: 0.573649, test loss: 0.595257, train acc: 0.73494, test acc: 0.728814\n",
      "3539, train loss: 0.573639, test loss: 0.595245, train acc: 0.73494, test acc: 0.728814\n",
      "3540, train loss: 0.573629, test loss: 0.595233, train acc: 0.73494, test acc: 0.728814\n",
      "3541, train loss: 0.573619, test loss: 0.595221, train acc: 0.73494, test acc: 0.728814\n",
      "3542, train loss: 0.573609, test loss: 0.595209, train acc: 0.73494, test acc: 0.728814\n",
      "3543, train loss: 0.573599, test loss: 0.595198, train acc: 0.73494, test acc: 0.728814\n",
      "3544, train loss: 0.573589, test loss: 0.595186, train acc: 0.73494, test acc: 0.728814\n",
      "3545, train loss: 0.573579, test loss: 0.595174, train acc: 0.73494, test acc: 0.728814\n",
      "3546, train loss: 0.573569, test loss: 0.595162, train acc: 0.73494, test acc: 0.728814\n",
      "3547, train loss: 0.573559, test loss: 0.59515, train acc: 0.73494, test acc: 0.728814\n",
      "3548, train loss: 0.573549, test loss: 0.595139, train acc: 0.73494, test acc: 0.728814\n",
      "3549, train loss: 0.573539, test loss: 0.595127, train acc: 0.73494, test acc: 0.728814\n",
      "3550, train loss: 0.573529, test loss: 0.595115, train acc: 0.73494, test acc: 0.728814\n",
      "3551, train loss: 0.573519, test loss: 0.595103, train acc: 0.73494, test acc: 0.728814\n",
      "3552, train loss: 0.573509, test loss: 0.595092, train acc: 0.73494, test acc: 0.728814\n",
      "3553, train loss: 0.573499, test loss: 0.59508, train acc: 0.73494, test acc: 0.728814\n",
      "3554, train loss: 0.573489, test loss: 0.595068, train acc: 0.73494, test acc: 0.728814\n",
      "3555, train loss: 0.573479, test loss: 0.595056, train acc: 0.73494, test acc: 0.728814\n",
      "3556, train loss: 0.573469, test loss: 0.595044, train acc: 0.73494, test acc: 0.728814\n",
      "3557, train loss: 0.573459, test loss: 0.595033, train acc: 0.73494, test acc: 0.728814\n",
      "3558, train loss: 0.573449, test loss: 0.595021, train acc: 0.73494, test acc: 0.728814\n",
      "3559, train loss: 0.573439, test loss: 0.595009, train acc: 0.73494, test acc: 0.728814\n",
      "3560, train loss: 0.573429, test loss: 0.594997, train acc: 0.73494, test acc: 0.728814\n",
      "3561, train loss: 0.573419, test loss: 0.594986, train acc: 0.73494, test acc: 0.728814\n",
      "3562, train loss: 0.573409, test loss: 0.594974, train acc: 0.73494, test acc: 0.728814\n",
      "3563, train loss: 0.573399, test loss: 0.594962, train acc: 0.73494, test acc: 0.728814\n",
      "3564, train loss: 0.573389, test loss: 0.59495, train acc: 0.73494, test acc: 0.728814\n",
      "3565, train loss: 0.573379, test loss: 0.594939, train acc: 0.73494, test acc: 0.728814\n",
      "3566, train loss: 0.573369, test loss: 0.594927, train acc: 0.73494, test acc: 0.728814\n",
      "3567, train loss: 0.573359, test loss: 0.594915, train acc: 0.73494, test acc: 0.728814\n",
      "3568, train loss: 0.57335, test loss: 0.594903, train acc: 0.73494, test acc: 0.728814\n",
      "3569, train loss: 0.57334, test loss: 0.594892, train acc: 0.73494, test acc: 0.728814\n",
      "3570, train loss: 0.57333, test loss: 0.59488, train acc: 0.73494, test acc: 0.728814\n",
      "3571, train loss: 0.57332, test loss: 0.594868, train acc: 0.73494, test acc: 0.728814\n",
      "3572, train loss: 0.57331, test loss: 0.594856, train acc: 0.73494, test acc: 0.728814\n",
      "3573, train loss: 0.5733, test loss: 0.594845, train acc: 0.73494, test acc: 0.728814\n",
      "3574, train loss: 0.57329, test loss: 0.594833, train acc: 0.73494, test acc: 0.728814\n",
      "3575, train loss: 0.57328, test loss: 0.594821, train acc: 0.73494, test acc: 0.728814\n",
      "3576, train loss: 0.57327, test loss: 0.594809, train acc: 0.73494, test acc: 0.728814\n",
      "3577, train loss: 0.57326, test loss: 0.594798, train acc: 0.73494, test acc: 0.728814\n",
      "3578, train loss: 0.57325, test loss: 0.594786, train acc: 0.73494, test acc: 0.728814\n",
      "3579, train loss: 0.57324, test loss: 0.594774, train acc: 0.73494, test acc: 0.728814\n",
      "3580, train loss: 0.57323, test loss: 0.594762, train acc: 0.73494, test acc: 0.728814\n",
      "3581, train loss: 0.57322, test loss: 0.594751, train acc: 0.73494, test acc: 0.728814\n",
      "3582, train loss: 0.57321, test loss: 0.594739, train acc: 0.73494, test acc: 0.728814\n",
      "3583, train loss: 0.5732, test loss: 0.594727, train acc: 0.73494, test acc: 0.728814\n",
      "3584, train loss: 0.57319, test loss: 0.594715, train acc: 0.73494, test acc: 0.728814\n",
      "3585, train loss: 0.57318, test loss: 0.594704, train acc: 0.73494, test acc: 0.728814\n",
      "3586, train loss: 0.57317, test loss: 0.594692, train acc: 0.73494, test acc: 0.728814\n",
      "3587, train loss: 0.573161, test loss: 0.59468, train acc: 0.73494, test acc: 0.728814\n",
      "3588, train loss: 0.573151, test loss: 0.594669, train acc: 0.73494, test acc: 0.728814\n",
      "3589, train loss: 0.573141, test loss: 0.594657, train acc: 0.73494, test acc: 0.728814\n",
      "3590, train loss: 0.573131, test loss: 0.594645, train acc: 0.73494, test acc: 0.728814\n",
      "3591, train loss: 0.573121, test loss: 0.594633, train acc: 0.73494, test acc: 0.728814\n",
      "3592, train loss: 0.573111, test loss: 0.594622, train acc: 0.73494, test acc: 0.728814\n",
      "3593, train loss: 0.573101, test loss: 0.59461, train acc: 0.73494, test acc: 0.728814\n",
      "3594, train loss: 0.573091, test loss: 0.594598, train acc: 0.73494, test acc: 0.728814\n",
      "3595, train loss: 0.573081, test loss: 0.594586, train acc: 0.73494, test acc: 0.728814\n",
      "3596, train loss: 0.573071, test loss: 0.594575, train acc: 0.73494, test acc: 0.728814\n",
      "3597, train loss: 0.573061, test loss: 0.594563, train acc: 0.73494, test acc: 0.728814\n",
      "3598, train loss: 0.573051, test loss: 0.594551, train acc: 0.73494, test acc: 0.728814\n",
      "3599, train loss: 0.573042, test loss: 0.59454, train acc: 0.73494, test acc: 0.728814\n",
      "3600, train loss: 0.573032, test loss: 0.594528, train acc: 0.73494, test acc: 0.728814\n",
      "3601, train loss: 0.573022, test loss: 0.594516, train acc: 0.73494, test acc: 0.728814\n",
      "3602, train loss: 0.573012, test loss: 0.594505, train acc: 0.73494, test acc: 0.728814\n",
      "3603, train loss: 0.573002, test loss: 0.594493, train acc: 0.73494, test acc: 0.728814\n",
      "3604, train loss: 0.572992, test loss: 0.594481, train acc: 0.73494, test acc: 0.728814\n",
      "3605, train loss: 0.572982, test loss: 0.594469, train acc: 0.73494, test acc: 0.728814\n",
      "3606, train loss: 0.572972, test loss: 0.594458, train acc: 0.73494, test acc: 0.728814\n",
      "3607, train loss: 0.572962, test loss: 0.594446, train acc: 0.73494, test acc: 0.728814\n",
      "3608, train loss: 0.572952, test loss: 0.594434, train acc: 0.73494, test acc: 0.728814\n",
      "3609, train loss: 0.572943, test loss: 0.594423, train acc: 0.73494, test acc: 0.728814\n",
      "3610, train loss: 0.572933, test loss: 0.594411, train acc: 0.73494, test acc: 0.728814\n",
      "3611, train loss: 0.572923, test loss: 0.594399, train acc: 0.73494, test acc: 0.728814\n",
      "3612, train loss: 0.572913, test loss: 0.594388, train acc: 0.73494, test acc: 0.728814\n",
      "3613, train loss: 0.572903, test loss: 0.594376, train acc: 0.73494, test acc: 0.728814\n",
      "3614, train loss: 0.572893, test loss: 0.594364, train acc: 0.73494, test acc: 0.728814\n",
      "3615, train loss: 0.572883, test loss: 0.594352, train acc: 0.73494, test acc: 0.728814\n",
      "3616, train loss: 0.572873, test loss: 0.594341, train acc: 0.73494, test acc: 0.728814\n",
      "3617, train loss: 0.572864, test loss: 0.594329, train acc: 0.73494, test acc: 0.728814\n",
      "3618, train loss: 0.572854, test loss: 0.594317, train acc: 0.73494, test acc: 0.728814\n",
      "3619, train loss: 0.572844, test loss: 0.594306, train acc: 0.73494, test acc: 0.728814\n",
      "3620, train loss: 0.572834, test loss: 0.594294, train acc: 0.73494, test acc: 0.728814\n",
      "3621, train loss: 0.572824, test loss: 0.594282, train acc: 0.73494, test acc: 0.728814\n",
      "3622, train loss: 0.572814, test loss: 0.594271, train acc: 0.73494, test acc: 0.728814\n",
      "3623, train loss: 0.572804, test loss: 0.594259, train acc: 0.73494, test acc: 0.728814\n",
      "3624, train loss: 0.572794, test loss: 0.594247, train acc: 0.73494, test acc: 0.728814\n",
      "3625, train loss: 0.572785, test loss: 0.594236, train acc: 0.73494, test acc: 0.728814\n",
      "3626, train loss: 0.572775, test loss: 0.594224, train acc: 0.73494, test acc: 0.728814\n",
      "3627, train loss: 0.572765, test loss: 0.594212, train acc: 0.73494, test acc: 0.728814\n",
      "3628, train loss: 0.572755, test loss: 0.594201, train acc: 0.73494, test acc: 0.728814\n",
      "3629, train loss: 0.572745, test loss: 0.594189, train acc: 0.73494, test acc: 0.728814\n",
      "3630, train loss: 0.572735, test loss: 0.594177, train acc: 0.73494, test acc: 0.728814\n",
      "3631, train loss: 0.572725, test loss: 0.594166, train acc: 0.73494, test acc: 0.728814\n",
      "3632, train loss: 0.572716, test loss: 0.594154, train acc: 0.73494, test acc: 0.728814\n",
      "3633, train loss: 0.572706, test loss: 0.594142, train acc: 0.73494, test acc: 0.728814\n",
      "3634, train loss: 0.572696, test loss: 0.594131, train acc: 0.73494, test acc: 0.728814\n",
      "3635, train loss: 0.572686, test loss: 0.594119, train acc: 0.73494, test acc: 0.728814\n",
      "3636, train loss: 0.572676, test loss: 0.594107, train acc: 0.73494, test acc: 0.728814\n",
      "3637, train loss: 0.572666, test loss: 0.594096, train acc: 0.73494, test acc: 0.728814\n",
      "3638, train loss: 0.572656, test loss: 0.594084, train acc: 0.73494, test acc: 0.728814\n",
      "3639, train loss: 0.572647, test loss: 0.594072, train acc: 0.73494, test acc: 0.728814\n",
      "3640, train loss: 0.572637, test loss: 0.594061, train acc: 0.73494, test acc: 0.728814\n",
      "3641, train loss: 0.572627, test loss: 0.594049, train acc: 0.73494, test acc: 0.728814\n",
      "3642, train loss: 0.572617, test loss: 0.594037, train acc: 0.73494, test acc: 0.728814\n",
      "3643, train loss: 0.572607, test loss: 0.594026, train acc: 0.73494, test acc: 0.728814\n",
      "3644, train loss: 0.572597, test loss: 0.594014, train acc: 0.73494, test acc: 0.728814\n",
      "3645, train loss: 0.572588, test loss: 0.594003, train acc: 0.73494, test acc: 0.728814\n",
      "3646, train loss: 0.572578, test loss: 0.593991, train acc: 0.73494, test acc: 0.728814\n",
      "3647, train loss: 0.572568, test loss: 0.593979, train acc: 0.73494, test acc: 0.728814\n",
      "3648, train loss: 0.572558, test loss: 0.593968, train acc: 0.73494, test acc: 0.728814\n",
      "3649, train loss: 0.572548, test loss: 0.593956, train acc: 0.73494, test acc: 0.728814\n",
      "3650, train loss: 0.572539, test loss: 0.593944, train acc: 0.73494, test acc: 0.728814\n",
      "3651, train loss: 0.572529, test loss: 0.593933, train acc: 0.73494, test acc: 0.728814\n",
      "3652, train loss: 0.572519, test loss: 0.593921, train acc: 0.73494, test acc: 0.728814\n",
      "3653, train loss: 0.572509, test loss: 0.593909, train acc: 0.73494, test acc: 0.728814\n",
      "3654, train loss: 0.572499, test loss: 0.593898, train acc: 0.737952, test acc: 0.728814\n",
      "3655, train loss: 0.57249, test loss: 0.593886, train acc: 0.737952, test acc: 0.728814\n",
      "3656, train loss: 0.57248, test loss: 0.593875, train acc: 0.737952, test acc: 0.728814\n",
      "3657, train loss: 0.57247, test loss: 0.593863, train acc: 0.737952, test acc: 0.728814\n",
      "3658, train loss: 0.57246, test loss: 0.593851, train acc: 0.737952, test acc: 0.728814\n",
      "3659, train loss: 0.57245, test loss: 0.59384, train acc: 0.737952, test acc: 0.728814\n",
      "3660, train loss: 0.572441, test loss: 0.593828, train acc: 0.737952, test acc: 0.728814\n",
      "3661, train loss: 0.572431, test loss: 0.593816, train acc: 0.737952, test acc: 0.728814\n",
      "3662, train loss: 0.572421, test loss: 0.593805, train acc: 0.737952, test acc: 0.728814\n",
      "3663, train loss: 0.572411, test loss: 0.593793, train acc: 0.737952, test acc: 0.728814\n",
      "3664, train loss: 0.572401, test loss: 0.593782, train acc: 0.737952, test acc: 0.728814\n",
      "3665, train loss: 0.572392, test loss: 0.59377, train acc: 0.737952, test acc: 0.728814\n",
      "3666, train loss: 0.572382, test loss: 0.593758, train acc: 0.737952, test acc: 0.728814\n",
      "3667, train loss: 0.572372, test loss: 0.593747, train acc: 0.737952, test acc: 0.728814\n",
      "3668, train loss: 0.572362, test loss: 0.593735, train acc: 0.737952, test acc: 0.728814\n",
      "3669, train loss: 0.572352, test loss: 0.593723, train acc: 0.737952, test acc: 0.728814\n",
      "3670, train loss: 0.572343, test loss: 0.593712, train acc: 0.737952, test acc: 0.728814\n",
      "3671, train loss: 0.572333, test loss: 0.5937, train acc: 0.737952, test acc: 0.728814\n",
      "3672, train loss: 0.572323, test loss: 0.593689, train acc: 0.737952, test acc: 0.728814\n",
      "3673, train loss: 0.572313, test loss: 0.593677, train acc: 0.737952, test acc: 0.728814\n",
      "3674, train loss: 0.572303, test loss: 0.593665, train acc: 0.737952, test acc: 0.728814\n",
      "3675, train loss: 0.572294, test loss: 0.593654, train acc: 0.737952, test acc: 0.728814\n",
      "3676, train loss: 0.572284, test loss: 0.593642, train acc: 0.737952, test acc: 0.728814\n",
      "3677, train loss: 0.572274, test loss: 0.593631, train acc: 0.737952, test acc: 0.728814\n",
      "3678, train loss: 0.572264, test loss: 0.593619, train acc: 0.737952, test acc: 0.728814\n",
      "3679, train loss: 0.572255, test loss: 0.593607, train acc: 0.737952, test acc: 0.728814\n",
      "3680, train loss: 0.572245, test loss: 0.593596, train acc: 0.737952, test acc: 0.728814\n",
      "3681, train loss: 0.572235, test loss: 0.593584, train acc: 0.737952, test acc: 0.728814\n",
      "3682, train loss: 0.572225, test loss: 0.593573, train acc: 0.737952, test acc: 0.728814\n",
      "3683, train loss: 0.572216, test loss: 0.593561, train acc: 0.737952, test acc: 0.728814\n",
      "3684, train loss: 0.572206, test loss: 0.593549, train acc: 0.737952, test acc: 0.728814\n",
      "3685, train loss: 0.572196, test loss: 0.593538, train acc: 0.737952, test acc: 0.728814\n",
      "3686, train loss: 0.572186, test loss: 0.593526, train acc: 0.737952, test acc: 0.728814\n",
      "3687, train loss: 0.572177, test loss: 0.593515, train acc: 0.737952, test acc: 0.728814\n",
      "3688, train loss: 0.572167, test loss: 0.593503, train acc: 0.737952, test acc: 0.728814\n",
      "3689, train loss: 0.572157, test loss: 0.593492, train acc: 0.737952, test acc: 0.728814\n",
      "3690, train loss: 0.572147, test loss: 0.59348, train acc: 0.737952, test acc: 0.728814\n",
      "3691, train loss: 0.572137, test loss: 0.593468, train acc: 0.737952, test acc: 0.728814\n",
      "3692, train loss: 0.572128, test loss: 0.593457, train acc: 0.737952, test acc: 0.728814\n",
      "3693, train loss: 0.572118, test loss: 0.593445, train acc: 0.737952, test acc: 0.728814\n",
      "3694, train loss: 0.572108, test loss: 0.593434, train acc: 0.737952, test acc: 0.728814\n",
      "3695, train loss: 0.572098, test loss: 0.593422, train acc: 0.737952, test acc: 0.728814\n",
      "3696, train loss: 0.572089, test loss: 0.59341, train acc: 0.737952, test acc: 0.728814\n",
      "3697, train loss: 0.572079, test loss: 0.593399, train acc: 0.737952, test acc: 0.728814\n",
      "3698, train loss: 0.572069, test loss: 0.593387, train acc: 0.737952, test acc: 0.728814\n",
      "3699, train loss: 0.57206, test loss: 0.593376, train acc: 0.737952, test acc: 0.728814\n",
      "3700, train loss: 0.57205, test loss: 0.593364, train acc: 0.737952, test acc: 0.728814\n",
      "3701, train loss: 0.57204, test loss: 0.593353, train acc: 0.737952, test acc: 0.728814\n",
      "3702, train loss: 0.57203, test loss: 0.593341, train acc: 0.737952, test acc: 0.728814\n",
      "3703, train loss: 0.572021, test loss: 0.593329, train acc: 0.737952, test acc: 0.728814\n",
      "3704, train loss: 0.572011, test loss: 0.593318, train acc: 0.737952, test acc: 0.728814\n",
      "3705, train loss: 0.572001, test loss: 0.593306, train acc: 0.737952, test acc: 0.728814\n",
      "3706, train loss: 0.571991, test loss: 0.593295, train acc: 0.737952, test acc: 0.728814\n",
      "3707, train loss: 0.571982, test loss: 0.593283, train acc: 0.737952, test acc: 0.728814\n",
      "3708, train loss: 0.571972, test loss: 0.593272, train acc: 0.737952, test acc: 0.728814\n",
      "3709, train loss: 0.571962, test loss: 0.59326, train acc: 0.737952, test acc: 0.728814\n",
      "3710, train loss: 0.571953, test loss: 0.593249, train acc: 0.737952, test acc: 0.728814\n",
      "3711, train loss: 0.571943, test loss: 0.593237, train acc: 0.737952, test acc: 0.728814\n",
      "3712, train loss: 0.571933, test loss: 0.593225, train acc: 0.737952, test acc: 0.728814\n",
      "3713, train loss: 0.571923, test loss: 0.593214, train acc: 0.737952, test acc: 0.728814\n",
      "3714, train loss: 0.571914, test loss: 0.593202, train acc: 0.737952, test acc: 0.728814\n",
      "3715, train loss: 0.571904, test loss: 0.593191, train acc: 0.737952, test acc: 0.728814\n",
      "3716, train loss: 0.571894, test loss: 0.593179, train acc: 0.737952, test acc: 0.728814\n",
      "3717, train loss: 0.571885, test loss: 0.593168, train acc: 0.737952, test acc: 0.728814\n",
      "3718, train loss: 0.571875, test loss: 0.593156, train acc: 0.737952, test acc: 0.728814\n",
      "3719, train loss: 0.571865, test loss: 0.593145, train acc: 0.737952, test acc: 0.728814\n",
      "3720, train loss: 0.571856, test loss: 0.593133, train acc: 0.737952, test acc: 0.728814\n",
      "3721, train loss: 0.571846, test loss: 0.593122, train acc: 0.737952, test acc: 0.728814\n",
      "3722, train loss: 0.571836, test loss: 0.59311, train acc: 0.737952, test acc: 0.728814\n",
      "3723, train loss: 0.571826, test loss: 0.593099, train acc: 0.737952, test acc: 0.728814\n",
      "3724, train loss: 0.571817, test loss: 0.593087, train acc: 0.737952, test acc: 0.728814\n",
      "3725, train loss: 0.571807, test loss: 0.593075, train acc: 0.737952, test acc: 0.728814\n",
      "3726, train loss: 0.571797, test loss: 0.593064, train acc: 0.737952, test acc: 0.728814\n",
      "3727, train loss: 0.571788, test loss: 0.593052, train acc: 0.737952, test acc: 0.728814\n",
      "3728, train loss: 0.571778, test loss: 0.593041, train acc: 0.737952, test acc: 0.728814\n",
      "3729, train loss: 0.571768, test loss: 0.593029, train acc: 0.737952, test acc: 0.728814\n",
      "3730, train loss: 0.571759, test loss: 0.593018, train acc: 0.737952, test acc: 0.728814\n",
      "3731, train loss: 0.571749, test loss: 0.593006, train acc: 0.737952, test acc: 0.728814\n",
      "3732, train loss: 0.571739, test loss: 0.592995, train acc: 0.737952, test acc: 0.728814\n",
      "3733, train loss: 0.57173, test loss: 0.592983, train acc: 0.737952, test acc: 0.728814\n",
      "3734, train loss: 0.57172, test loss: 0.592972, train acc: 0.737952, test acc: 0.728814\n",
      "3735, train loss: 0.57171, test loss: 0.59296, train acc: 0.737952, test acc: 0.728814\n",
      "3736, train loss: 0.571701, test loss: 0.592949, train acc: 0.737952, test acc: 0.728814\n",
      "3737, train loss: 0.571691, test loss: 0.592937, train acc: 0.737952, test acc: 0.728814\n",
      "3738, train loss: 0.571681, test loss: 0.592926, train acc: 0.737952, test acc: 0.728814\n",
      "3739, train loss: 0.571671, test loss: 0.592914, train acc: 0.737952, test acc: 0.728814\n",
      "3740, train loss: 0.571662, test loss: 0.592903, train acc: 0.737952, test acc: 0.728814\n",
      "3741, train loss: 0.571652, test loss: 0.592891, train acc: 0.737952, test acc: 0.728814\n",
      "3742, train loss: 0.571643, test loss: 0.59288, train acc: 0.737952, test acc: 0.728814\n",
      "3743, train loss: 0.571633, test loss: 0.592868, train acc: 0.737952, test acc: 0.728814\n",
      "3744, train loss: 0.571623, test loss: 0.592857, train acc: 0.737952, test acc: 0.728814\n",
      "3745, train loss: 0.571613, test loss: 0.592845, train acc: 0.737952, test acc: 0.728814\n",
      "3746, train loss: 0.571604, test loss: 0.592834, train acc: 0.737952, test acc: 0.728814\n",
      "3747, train loss: 0.571594, test loss: 0.592822, train acc: 0.737952, test acc: 0.728814\n",
      "3748, train loss: 0.571585, test loss: 0.592811, train acc: 0.737952, test acc: 0.728814\n",
      "3749, train loss: 0.571575, test loss: 0.592799, train acc: 0.737952, test acc: 0.728814\n",
      "3750, train loss: 0.571565, test loss: 0.592788, train acc: 0.737952, test acc: 0.728814\n",
      "3751, train loss: 0.571556, test loss: 0.592776, train acc: 0.737952, test acc: 0.728814\n",
      "3752, train loss: 0.571546, test loss: 0.592764, train acc: 0.737952, test acc: 0.728814\n",
      "3753, train loss: 0.571536, test loss: 0.592753, train acc: 0.737952, test acc: 0.728814\n",
      "3754, train loss: 0.571527, test loss: 0.592741, train acc: 0.737952, test acc: 0.728814\n",
      "3755, train loss: 0.571517, test loss: 0.59273, train acc: 0.737952, test acc: 0.728814\n",
      "3756, train loss: 0.571507, test loss: 0.592718, train acc: 0.737952, test acc: 0.728814\n",
      "3757, train loss: 0.571498, test loss: 0.592707, train acc: 0.737952, test acc: 0.728814\n",
      "3758, train loss: 0.571488, test loss: 0.592696, train acc: 0.737952, test acc: 0.728814\n",
      "3759, train loss: 0.571478, test loss: 0.592684, train acc: 0.737952, test acc: 0.728814\n",
      "3760, train loss: 0.571469, test loss: 0.592673, train acc: 0.737952, test acc: 0.728814\n",
      "3761, train loss: 0.571459, test loss: 0.592661, train acc: 0.737952, test acc: 0.728814\n",
      "3762, train loss: 0.57145, test loss: 0.59265, train acc: 0.737952, test acc: 0.728814\n",
      "3763, train loss: 0.57144, test loss: 0.592638, train acc: 0.737952, test acc: 0.728814\n",
      "3764, train loss: 0.57143, test loss: 0.592627, train acc: 0.737952, test acc: 0.728814\n",
      "3765, train loss: 0.571421, test loss: 0.592615, train acc: 0.737952, test acc: 0.728814\n",
      "3766, train loss: 0.571411, test loss: 0.592604, train acc: 0.737952, test acc: 0.728814\n",
      "3767, train loss: 0.571401, test loss: 0.592592, train acc: 0.737952, test acc: 0.728814\n",
      "3768, train loss: 0.571392, test loss: 0.592581, train acc: 0.737952, test acc: 0.728814\n",
      "3769, train loss: 0.571382, test loss: 0.592569, train acc: 0.737952, test acc: 0.728814\n",
      "3770, train loss: 0.571373, test loss: 0.592558, train acc: 0.737952, test acc: 0.728814\n",
      "3771, train loss: 0.571363, test loss: 0.592546, train acc: 0.737952, test acc: 0.728814\n",
      "3772, train loss: 0.571353, test loss: 0.592535, train acc: 0.737952, test acc: 0.728814\n",
      "3773, train loss: 0.571344, test loss: 0.592523, train acc: 0.737952, test acc: 0.728814\n",
      "3774, train loss: 0.571334, test loss: 0.592512, train acc: 0.737952, test acc: 0.728814\n",
      "3775, train loss: 0.571324, test loss: 0.5925, train acc: 0.737952, test acc: 0.728814\n",
      "3776, train loss: 0.571315, test loss: 0.592489, train acc: 0.737952, test acc: 0.728814\n",
      "3777, train loss: 0.571305, test loss: 0.592478, train acc: 0.737952, test acc: 0.728814\n",
      "3778, train loss: 0.571296, test loss: 0.592466, train acc: 0.737952, test acc: 0.728814\n",
      "3779, train loss: 0.571286, test loss: 0.592455, train acc: 0.737952, test acc: 0.728814\n",
      "3780, train loss: 0.571276, test loss: 0.592443, train acc: 0.737952, test acc: 0.728814\n",
      "3781, train loss: 0.571267, test loss: 0.592432, train acc: 0.737952, test acc: 0.728814\n",
      "3782, train loss: 0.571257, test loss: 0.59242, train acc: 0.737952, test acc: 0.728814\n",
      "3783, train loss: 0.571248, test loss: 0.592409, train acc: 0.737952, test acc: 0.728814\n",
      "3784, train loss: 0.571238, test loss: 0.592397, train acc: 0.737952, test acc: 0.728814\n",
      "3785, train loss: 0.571229, test loss: 0.592386, train acc: 0.737952, test acc: 0.728814\n",
      "3786, train loss: 0.571219, test loss: 0.592374, train acc: 0.737952, test acc: 0.728814\n",
      "3787, train loss: 0.571209, test loss: 0.592363, train acc: 0.737952, test acc: 0.728814\n",
      "3788, train loss: 0.5712, test loss: 0.592351, train acc: 0.737952, test acc: 0.728814\n",
      "3789, train loss: 0.57119, test loss: 0.59234, train acc: 0.737952, test acc: 0.728814\n",
      "3790, train loss: 0.571181, test loss: 0.592329, train acc: 0.737952, test acc: 0.728814\n",
      "3791, train loss: 0.571171, test loss: 0.592317, train acc: 0.737952, test acc: 0.728814\n",
      "3792, train loss: 0.571161, test loss: 0.592306, train acc: 0.737952, test acc: 0.728814\n",
      "3793, train loss: 0.571152, test loss: 0.592294, train acc: 0.737952, test acc: 0.728814\n",
      "3794, train loss: 0.571142, test loss: 0.592283, train acc: 0.737952, test acc: 0.728814\n",
      "3795, train loss: 0.571133, test loss: 0.592271, train acc: 0.737952, test acc: 0.728814\n",
      "3796, train loss: 0.571123, test loss: 0.59226, train acc: 0.737952, test acc: 0.728814\n",
      "3797, train loss: 0.571113, test loss: 0.592249, train acc: 0.737952, test acc: 0.728814\n",
      "3798, train loss: 0.571104, test loss: 0.592237, train acc: 0.737952, test acc: 0.728814\n",
      "3799, train loss: 0.571094, test loss: 0.592226, train acc: 0.737952, test acc: 0.728814\n",
      "3800, train loss: 0.571085, test loss: 0.592214, train acc: 0.737952, test acc: 0.728814\n",
      "3801, train loss: 0.571075, test loss: 0.592203, train acc: 0.737952, test acc: 0.728814\n",
      "3802, train loss: 0.571066, test loss: 0.592191, train acc: 0.737952, test acc: 0.728814\n",
      "3803, train loss: 0.571056, test loss: 0.59218, train acc: 0.737952, test acc: 0.728814\n",
      "3804, train loss: 0.571046, test loss: 0.592169, train acc: 0.737952, test acc: 0.728814\n",
      "3805, train loss: 0.571037, test loss: 0.592157, train acc: 0.737952, test acc: 0.728814\n",
      "3806, train loss: 0.571027, test loss: 0.592146, train acc: 0.737952, test acc: 0.728814\n",
      "3807, train loss: 0.571018, test loss: 0.592134, train acc: 0.737952, test acc: 0.728814\n",
      "3808, train loss: 0.571008, test loss: 0.592123, train acc: 0.737952, test acc: 0.728814\n",
      "3809, train loss: 0.570999, test loss: 0.592111, train acc: 0.737952, test acc: 0.728814\n",
      "3810, train loss: 0.570989, test loss: 0.5921, train acc: 0.737952, test acc: 0.728814\n",
      "3811, train loss: 0.57098, test loss: 0.592089, train acc: 0.737952, test acc: 0.728814\n",
      "3812, train loss: 0.57097, test loss: 0.592077, train acc: 0.737952, test acc: 0.728814\n",
      "3813, train loss: 0.57096, test loss: 0.592066, train acc: 0.737952, test acc: 0.728814\n",
      "3814, train loss: 0.570951, test loss: 0.592054, train acc: 0.737952, test acc: 0.728814\n",
      "3815, train loss: 0.570941, test loss: 0.592043, train acc: 0.737952, test acc: 0.728814\n",
      "3816, train loss: 0.570932, test loss: 0.592031, train acc: 0.737952, test acc: 0.728814\n",
      "3817, train loss: 0.570922, test loss: 0.59202, train acc: 0.737952, test acc: 0.728814\n",
      "3818, train loss: 0.570913, test loss: 0.592009, train acc: 0.737952, test acc: 0.728814\n",
      "3819, train loss: 0.570903, test loss: 0.591997, train acc: 0.737952, test acc: 0.728814\n",
      "3820, train loss: 0.570894, test loss: 0.591986, train acc: 0.737952, test acc: 0.728814\n",
      "3821, train loss: 0.570884, test loss: 0.591974, train acc: 0.737952, test acc: 0.728814\n",
      "3822, train loss: 0.570875, test loss: 0.591963, train acc: 0.737952, test acc: 0.728814\n",
      "3823, train loss: 0.570865, test loss: 0.591952, train acc: 0.737952, test acc: 0.728814\n",
      "3824, train loss: 0.570856, test loss: 0.59194, train acc: 0.737952, test acc: 0.728814\n",
      "3825, train loss: 0.570846, test loss: 0.591929, train acc: 0.737952, test acc: 0.728814\n",
      "3826, train loss: 0.570836, test loss: 0.591917, train acc: 0.737952, test acc: 0.728814\n",
      "3827, train loss: 0.570827, test loss: 0.591906, train acc: 0.737952, test acc: 0.728814\n",
      "3828, train loss: 0.570817, test loss: 0.591895, train acc: 0.737952, test acc: 0.728814\n",
      "3829, train loss: 0.570808, test loss: 0.591883, train acc: 0.737952, test acc: 0.728814\n",
      "3830, train loss: 0.570798, test loss: 0.591872, train acc: 0.737952, test acc: 0.728814\n",
      "3831, train loss: 0.570789, test loss: 0.59186, train acc: 0.737952, test acc: 0.728814\n",
      "3832, train loss: 0.570779, test loss: 0.591849, train acc: 0.737952, test acc: 0.728814\n",
      "3833, train loss: 0.57077, test loss: 0.591838, train acc: 0.737952, test acc: 0.728814\n",
      "3834, train loss: 0.57076, test loss: 0.591826, train acc: 0.737952, test acc: 0.728814\n",
      "3835, train loss: 0.570751, test loss: 0.591815, train acc: 0.737952, test acc: 0.728814\n",
      "3836, train loss: 0.570741, test loss: 0.591803, train acc: 0.737952, test acc: 0.728814\n",
      "3837, train loss: 0.570732, test loss: 0.591792, train acc: 0.737952, test acc: 0.728814\n",
      "3838, train loss: 0.570722, test loss: 0.591781, train acc: 0.737952, test acc: 0.728814\n",
      "3839, train loss: 0.570713, test loss: 0.591769, train acc: 0.737952, test acc: 0.728814\n",
      "3840, train loss: 0.570703, test loss: 0.591758, train acc: 0.737952, test acc: 0.728814\n",
      "3841, train loss: 0.570694, test loss: 0.591747, train acc: 0.737952, test acc: 0.728814\n",
      "3842, train loss: 0.570684, test loss: 0.591735, train acc: 0.737952, test acc: 0.728814\n",
      "3843, train loss: 0.570675, test loss: 0.591724, train acc: 0.737952, test acc: 0.728814\n",
      "3844, train loss: 0.570665, test loss: 0.591712, train acc: 0.737952, test acc: 0.728814\n",
      "3845, train loss: 0.570656, test loss: 0.591701, train acc: 0.737952, test acc: 0.728814\n",
      "3846, train loss: 0.570646, test loss: 0.59169, train acc: 0.737952, test acc: 0.728814\n",
      "3847, train loss: 0.570637, test loss: 0.591678, train acc: 0.737952, test acc: 0.728814\n",
      "3848, train loss: 0.570627, test loss: 0.591667, train acc: 0.737952, test acc: 0.728814\n",
      "3849, train loss: 0.570618, test loss: 0.591656, train acc: 0.737952, test acc: 0.728814\n",
      "3850, train loss: 0.570608, test loss: 0.591644, train acc: 0.737952, test acc: 0.728814\n",
      "3851, train loss: 0.570599, test loss: 0.591633, train acc: 0.737952, test acc: 0.728814\n",
      "3852, train loss: 0.570589, test loss: 0.591622, train acc: 0.737952, test acc: 0.728814\n",
      "3853, train loss: 0.57058, test loss: 0.59161, train acc: 0.737952, test acc: 0.728814\n",
      "3854, train loss: 0.57057, test loss: 0.591599, train acc: 0.737952, test acc: 0.728814\n",
      "3855, train loss: 0.570561, test loss: 0.591587, train acc: 0.737952, test acc: 0.728814\n",
      "3856, train loss: 0.570551, test loss: 0.591576, train acc: 0.737952, test acc: 0.728814\n",
      "3857, train loss: 0.570542, test loss: 0.591565, train acc: 0.737952, test acc: 0.728814\n",
      "3858, train loss: 0.570532, test loss: 0.591553, train acc: 0.737952, test acc: 0.728814\n",
      "3859, train loss: 0.570523, test loss: 0.591542, train acc: 0.737952, test acc: 0.728814\n",
      "3860, train loss: 0.570513, test loss: 0.591531, train acc: 0.737952, test acc: 0.728814\n",
      "3861, train loss: 0.570504, test loss: 0.591519, train acc: 0.737952, test acc: 0.728814\n",
      "3862, train loss: 0.570494, test loss: 0.591508, train acc: 0.737952, test acc: 0.728814\n",
      "3863, train loss: 0.570485, test loss: 0.591497, train acc: 0.737952, test acc: 0.728814\n",
      "3864, train loss: 0.570476, test loss: 0.591485, train acc: 0.737952, test acc: 0.728814\n",
      "3865, train loss: 0.570466, test loss: 0.591474, train acc: 0.737952, test acc: 0.728814\n",
      "3866, train loss: 0.570457, test loss: 0.591463, train acc: 0.737952, test acc: 0.728814\n",
      "3867, train loss: 0.570447, test loss: 0.591451, train acc: 0.737952, test acc: 0.728814\n",
      "3868, train loss: 0.570438, test loss: 0.59144, train acc: 0.737952, test acc: 0.728814\n",
      "3869, train loss: 0.570428, test loss: 0.591428, train acc: 0.737952, test acc: 0.728814\n",
      "3870, train loss: 0.570419, test loss: 0.591417, train acc: 0.737952, test acc: 0.728814\n",
      "3871, train loss: 0.570409, test loss: 0.591406, train acc: 0.737952, test acc: 0.728814\n",
      "3872, train loss: 0.5704, test loss: 0.591395, train acc: 0.737952, test acc: 0.728814\n",
      "3873, train loss: 0.57039, test loss: 0.591383, train acc: 0.737952, test acc: 0.728814\n",
      "3874, train loss: 0.570381, test loss: 0.591372, train acc: 0.737952, test acc: 0.728814\n",
      "3875, train loss: 0.570372, test loss: 0.59136, train acc: 0.737952, test acc: 0.728814\n",
      "3876, train loss: 0.570362, test loss: 0.591349, train acc: 0.737952, test acc: 0.728814\n",
      "3877, train loss: 0.570353, test loss: 0.591338, train acc: 0.737952, test acc: 0.728814\n",
      "3878, train loss: 0.570343, test loss: 0.591326, train acc: 0.737952, test acc: 0.728814\n",
      "3879, train loss: 0.570334, test loss: 0.591315, train acc: 0.737952, test acc: 0.728814\n",
      "3880, train loss: 0.570324, test loss: 0.591304, train acc: 0.737952, test acc: 0.728814\n",
      "3881, train loss: 0.570315, test loss: 0.591293, train acc: 0.737952, test acc: 0.728814\n",
      "3882, train loss: 0.570305, test loss: 0.591281, train acc: 0.737952, test acc: 0.728814\n",
      "3883, train loss: 0.570296, test loss: 0.59127, train acc: 0.737952, test acc: 0.728814\n",
      "3884, train loss: 0.570287, test loss: 0.591259, train acc: 0.737952, test acc: 0.728814\n",
      "3885, train loss: 0.570277, test loss: 0.591247, train acc: 0.737952, test acc: 0.728814\n",
      "3886, train loss: 0.570268, test loss: 0.591236, train acc: 0.737952, test acc: 0.728814\n",
      "3887, train loss: 0.570258, test loss: 0.591225, train acc: 0.737952, test acc: 0.728814\n",
      "3888, train loss: 0.570249, test loss: 0.591213, train acc: 0.737952, test acc: 0.728814\n",
      "3889, train loss: 0.570239, test loss: 0.591202, train acc: 0.737952, test acc: 0.728814\n",
      "3890, train loss: 0.57023, test loss: 0.591191, train acc: 0.737952, test acc: 0.728814\n",
      "3891, train loss: 0.57022, test loss: 0.591179, train acc: 0.737952, test acc: 0.728814\n",
      "3892, train loss: 0.570211, test loss: 0.591168, train acc: 0.737952, test acc: 0.728814\n",
      "3893, train loss: 0.570202, test loss: 0.591157, train acc: 0.737952, test acc: 0.728814\n",
      "3894, train loss: 0.570192, test loss: 0.591145, train acc: 0.737952, test acc: 0.728814\n",
      "3895, train loss: 0.570183, test loss: 0.591134, train acc: 0.737952, test acc: 0.728814\n",
      "3896, train loss: 0.570173, test loss: 0.591123, train acc: 0.737952, test acc: 0.728814\n",
      "3897, train loss: 0.570164, test loss: 0.591112, train acc: 0.737952, test acc: 0.728814\n",
      "3898, train loss: 0.570155, test loss: 0.5911, train acc: 0.737952, test acc: 0.728814\n",
      "3899, train loss: 0.570145, test loss: 0.591089, train acc: 0.737952, test acc: 0.728814\n",
      "3900, train loss: 0.570136, test loss: 0.591078, train acc: 0.737952, test acc: 0.728814\n",
      "3901, train loss: 0.570126, test loss: 0.591066, train acc: 0.737952, test acc: 0.728814\n",
      "3902, train loss: 0.570117, test loss: 0.591055, train acc: 0.737952, test acc: 0.728814\n",
      "3903, train loss: 0.570107, test loss: 0.591044, train acc: 0.737952, test acc: 0.728814\n",
      "3904, train loss: 0.570098, test loss: 0.591032, train acc: 0.737952, test acc: 0.728814\n",
      "3905, train loss: 0.570089, test loss: 0.591021, train acc: 0.737952, test acc: 0.728814\n",
      "3906, train loss: 0.570079, test loss: 0.59101, train acc: 0.737952, test acc: 0.728814\n",
      "3907, train loss: 0.57007, test loss: 0.590998, train acc: 0.737952, test acc: 0.728814\n",
      "3908, train loss: 0.57006, test loss: 0.590987, train acc: 0.737952, test acc: 0.728814\n",
      "3909, train loss: 0.570051, test loss: 0.590976, train acc: 0.737952, test acc: 0.728814\n",
      "3910, train loss: 0.570042, test loss: 0.590965, train acc: 0.737952, test acc: 0.728814\n",
      "3911, train loss: 0.570032, test loss: 0.590953, train acc: 0.737952, test acc: 0.728814\n",
      "3912, train loss: 0.570023, test loss: 0.590942, train acc: 0.737952, test acc: 0.728814\n",
      "3913, train loss: 0.570013, test loss: 0.590931, train acc: 0.737952, test acc: 0.728814\n",
      "3914, train loss: 0.570004, test loss: 0.590919, train acc: 0.737952, test acc: 0.728814\n",
      "3915, train loss: 0.569995, test loss: 0.590908, train acc: 0.737952, test acc: 0.728814\n",
      "3916, train loss: 0.569985, test loss: 0.590897, train acc: 0.737952, test acc: 0.728814\n",
      "3917, train loss: 0.569976, test loss: 0.590886, train acc: 0.737952, test acc: 0.728814\n",
      "3918, train loss: 0.569967, test loss: 0.590874, train acc: 0.737952, test acc: 0.728814\n",
      "3919, train loss: 0.569957, test loss: 0.590863, train acc: 0.737952, test acc: 0.728814\n",
      "3920, train loss: 0.569948, test loss: 0.590852, train acc: 0.737952, test acc: 0.728814\n",
      "3921, train loss: 0.569938, test loss: 0.590841, train acc: 0.737952, test acc: 0.728814\n",
      "3922, train loss: 0.569929, test loss: 0.590829, train acc: 0.737952, test acc: 0.728814\n",
      "3923, train loss: 0.56992, test loss: 0.590818, train acc: 0.737952, test acc: 0.728814\n",
      "3924, train loss: 0.56991, test loss: 0.590807, train acc: 0.737952, test acc: 0.728814\n",
      "3925, train loss: 0.569901, test loss: 0.590795, train acc: 0.737952, test acc: 0.728814\n",
      "3926, train loss: 0.569891, test loss: 0.590784, train acc: 0.737952, test acc: 0.728814\n",
      "3927, train loss: 0.569882, test loss: 0.590773, train acc: 0.737952, test acc: 0.728814\n",
      "3928, train loss: 0.569873, test loss: 0.590762, train acc: 0.737952, test acc: 0.728814\n",
      "3929, train loss: 0.569863, test loss: 0.59075, train acc: 0.737952, test acc: 0.728814\n",
      "3930, train loss: 0.569854, test loss: 0.590739, train acc: 0.737952, test acc: 0.728814\n",
      "3931, train loss: 0.569845, test loss: 0.590728, train acc: 0.737952, test acc: 0.728814\n",
      "3932, train loss: 0.569835, test loss: 0.590717, train acc: 0.737952, test acc: 0.728814\n",
      "3933, train loss: 0.569826, test loss: 0.590705, train acc: 0.737952, test acc: 0.728814\n",
      "3934, train loss: 0.569816, test loss: 0.590694, train acc: 0.737952, test acc: 0.728814\n",
      "3935, train loss: 0.569807, test loss: 0.590683, train acc: 0.737952, test acc: 0.728814\n",
      "3936, train loss: 0.569798, test loss: 0.590672, train acc: 0.737952, test acc: 0.728814\n",
      "3937, train loss: 0.569788, test loss: 0.59066, train acc: 0.737952, test acc: 0.728814\n",
      "3938, train loss: 0.569779, test loss: 0.590649, train acc: 0.737952, test acc: 0.728814\n",
      "3939, train loss: 0.56977, test loss: 0.590638, train acc: 0.737952, test acc: 0.728814\n",
      "3940, train loss: 0.56976, test loss: 0.590627, train acc: 0.737952, test acc: 0.745763\n",
      "3941, train loss: 0.569751, test loss: 0.590615, train acc: 0.737952, test acc: 0.745763\n",
      "3942, train loss: 0.569742, test loss: 0.590604, train acc: 0.737952, test acc: 0.745763\n",
      "3943, train loss: 0.569732, test loss: 0.590593, train acc: 0.737952, test acc: 0.745763\n",
      "3944, train loss: 0.569723, test loss: 0.590582, train acc: 0.737952, test acc: 0.745763\n",
      "3945, train loss: 0.569714, test loss: 0.59057, train acc: 0.737952, test acc: 0.745763\n",
      "3946, train loss: 0.569704, test loss: 0.590559, train acc: 0.737952, test acc: 0.745763\n",
      "3947, train loss: 0.569695, test loss: 0.590548, train acc: 0.737952, test acc: 0.745763\n",
      "3948, train loss: 0.569686, test loss: 0.590537, train acc: 0.737952, test acc: 0.745763\n",
      "3949, train loss: 0.569676, test loss: 0.590525, train acc: 0.737952, test acc: 0.745763\n",
      "3950, train loss: 0.569667, test loss: 0.590514, train acc: 0.737952, test acc: 0.745763\n",
      "3951, train loss: 0.569658, test loss: 0.590503, train acc: 0.737952, test acc: 0.745763\n",
      "3952, train loss: 0.569648, test loss: 0.590492, train acc: 0.737952, test acc: 0.745763\n",
      "3953, train loss: 0.569639, test loss: 0.59048, train acc: 0.737952, test acc: 0.745763\n",
      "3954, train loss: 0.56963, test loss: 0.590469, train acc: 0.737952, test acc: 0.745763\n",
      "3955, train loss: 0.56962, test loss: 0.590458, train acc: 0.737952, test acc: 0.745763\n",
      "3956, train loss: 0.569611, test loss: 0.590447, train acc: 0.737952, test acc: 0.745763\n",
      "3957, train loss: 0.569602, test loss: 0.590436, train acc: 0.737952, test acc: 0.745763\n",
      "3958, train loss: 0.569592, test loss: 0.590424, train acc: 0.737952, test acc: 0.745763\n",
      "3959, train loss: 0.569583, test loss: 0.590413, train acc: 0.737952, test acc: 0.745763\n",
      "3960, train loss: 0.569574, test loss: 0.590402, train acc: 0.737952, test acc: 0.745763\n",
      "3961, train loss: 0.569564, test loss: 0.590391, train acc: 0.737952, test acc: 0.745763\n",
      "3962, train loss: 0.569555, test loss: 0.590379, train acc: 0.737952, test acc: 0.745763\n",
      "3963, train loss: 0.569546, test loss: 0.590368, train acc: 0.737952, test acc: 0.745763\n",
      "3964, train loss: 0.569536, test loss: 0.590357, train acc: 0.737952, test acc: 0.745763\n",
      "3965, train loss: 0.569527, test loss: 0.590346, train acc: 0.737952, test acc: 0.745763\n",
      "3966, train loss: 0.569518, test loss: 0.590334, train acc: 0.737952, test acc: 0.745763\n",
      "3967, train loss: 0.569508, test loss: 0.590323, train acc: 0.737952, test acc: 0.745763\n",
      "3968, train loss: 0.569499, test loss: 0.590312, train acc: 0.737952, test acc: 0.745763\n",
      "3969, train loss: 0.56949, test loss: 0.590301, train acc: 0.737952, test acc: 0.745763\n",
      "3970, train loss: 0.56948, test loss: 0.59029, train acc: 0.737952, test acc: 0.745763\n",
      "3971, train loss: 0.569471, test loss: 0.590278, train acc: 0.737952, test acc: 0.745763\n",
      "3972, train loss: 0.569462, test loss: 0.590267, train acc: 0.737952, test acc: 0.745763\n",
      "3973, train loss: 0.569453, test loss: 0.590256, train acc: 0.737952, test acc: 0.745763\n",
      "3974, train loss: 0.569443, test loss: 0.590245, train acc: 0.737952, test acc: 0.745763\n",
      "3975, train loss: 0.569434, test loss: 0.590234, train acc: 0.737952, test acc: 0.745763\n",
      "3976, train loss: 0.569425, test loss: 0.590222, train acc: 0.737952, test acc: 0.745763\n",
      "3977, train loss: 0.569415, test loss: 0.590211, train acc: 0.737952, test acc: 0.745763\n",
      "3978, train loss: 0.569406, test loss: 0.5902, train acc: 0.737952, test acc: 0.745763\n",
      "3979, train loss: 0.569397, test loss: 0.590189, train acc: 0.737952, test acc: 0.745763\n",
      "3980, train loss: 0.569387, test loss: 0.590178, train acc: 0.737952, test acc: 0.745763\n",
      "3981, train loss: 0.569378, test loss: 0.590166, train acc: 0.737952, test acc: 0.745763\n",
      "3982, train loss: 0.569369, test loss: 0.590155, train acc: 0.737952, test acc: 0.745763\n",
      "3983, train loss: 0.56936, test loss: 0.590144, train acc: 0.737952, test acc: 0.745763\n",
      "3984, train loss: 0.56935, test loss: 0.590133, train acc: 0.737952, test acc: 0.745763\n",
      "3985, train loss: 0.569341, test loss: 0.590122, train acc: 0.737952, test acc: 0.745763\n",
      "3986, train loss: 0.569332, test loss: 0.59011, train acc: 0.737952, test acc: 0.745763\n",
      "3987, train loss: 0.569322, test loss: 0.590099, train acc: 0.737952, test acc: 0.745763\n",
      "3988, train loss: 0.569313, test loss: 0.590088, train acc: 0.737952, test acc: 0.745763\n",
      "3989, train loss: 0.569304, test loss: 0.590077, train acc: 0.737952, test acc: 0.745763\n",
      "3990, train loss: 0.569295, test loss: 0.590066, train acc: 0.737952, test acc: 0.745763\n",
      "3991, train loss: 0.569285, test loss: 0.590055, train acc: 0.737952, test acc: 0.745763\n",
      "3992, train loss: 0.569276, test loss: 0.590043, train acc: 0.737952, test acc: 0.745763\n",
      "3993, train loss: 0.569267, test loss: 0.590032, train acc: 0.737952, test acc: 0.745763\n",
      "3994, train loss: 0.569257, test loss: 0.590021, train acc: 0.737952, test acc: 0.745763\n",
      "3995, train loss: 0.569248, test loss: 0.59001, train acc: 0.737952, test acc: 0.745763\n",
      "3996, train loss: 0.569239, test loss: 0.589999, train acc: 0.737952, test acc: 0.745763\n",
      "3997, train loss: 0.56923, test loss: 0.589987, train acc: 0.737952, test acc: 0.745763\n",
      "3998, train loss: 0.56922, test loss: 0.589976, train acc: 0.737952, test acc: 0.745763\n",
      "3999, train loss: 0.569211, test loss: 0.589965, train acc: 0.737952, test acc: 0.745763\n",
      "4000, train loss: 0.569202, test loss: 0.589954, train acc: 0.737952, test acc: 0.745763\n",
      "4001, train loss: 0.569193, test loss: 0.589943, train acc: 0.737952, test acc: 0.745763\n",
      "4002, train loss: 0.569183, test loss: 0.589932, train acc: 0.737952, test acc: 0.745763\n",
      "4003, train loss: 0.569174, test loss: 0.58992, train acc: 0.737952, test acc: 0.745763\n",
      "4004, train loss: 0.569165, test loss: 0.589909, train acc: 0.737952, test acc: 0.745763\n",
      "4005, train loss: 0.569156, test loss: 0.589898, train acc: 0.737952, test acc: 0.745763\n",
      "4006, train loss: 0.569146, test loss: 0.589887, train acc: 0.737952, test acc: 0.745763\n",
      "4007, train loss: 0.569137, test loss: 0.589876, train acc: 0.737952, test acc: 0.745763\n",
      "4008, train loss: 0.569128, test loss: 0.589865, train acc: 0.737952, test acc: 0.745763\n",
      "4009, train loss: 0.569119, test loss: 0.589853, train acc: 0.737952, test acc: 0.745763\n",
      "4010, train loss: 0.569109, test loss: 0.589842, train acc: 0.737952, test acc: 0.745763\n",
      "4011, train loss: 0.5691, test loss: 0.589831, train acc: 0.737952, test acc: 0.745763\n",
      "4012, train loss: 0.569091, test loss: 0.58982, train acc: 0.737952, test acc: 0.745763\n",
      "4013, train loss: 0.569082, test loss: 0.589809, train acc: 0.737952, test acc: 0.745763\n",
      "4014, train loss: 0.569072, test loss: 0.589798, train acc: 0.737952, test acc: 0.745763\n",
      "4015, train loss: 0.569063, test loss: 0.589786, train acc: 0.737952, test acc: 0.745763\n",
      "4016, train loss: 0.569054, test loss: 0.589775, train acc: 0.737952, test acc: 0.745763\n",
      "4017, train loss: 0.569045, test loss: 0.589764, train acc: 0.737952, test acc: 0.745763\n",
      "4018, train loss: 0.569035, test loss: 0.589753, train acc: 0.737952, test acc: 0.745763\n",
      "4019, train loss: 0.569026, test loss: 0.589742, train acc: 0.737952, test acc: 0.745763\n",
      "4020, train loss: 0.569017, test loss: 0.589731, train acc: 0.737952, test acc: 0.745763\n",
      "4021, train loss: 0.569008, test loss: 0.58972, train acc: 0.737952, test acc: 0.745763\n",
      "4022, train loss: 0.568998, test loss: 0.589708, train acc: 0.737952, test acc: 0.745763\n",
      "4023, train loss: 0.568989, test loss: 0.589697, train acc: 0.737952, test acc: 0.745763\n",
      "4024, train loss: 0.56898, test loss: 0.589686, train acc: 0.737952, test acc: 0.745763\n",
      "4025, train loss: 0.568971, test loss: 0.589675, train acc: 0.737952, test acc: 0.745763\n",
      "4026, train loss: 0.568962, test loss: 0.589664, train acc: 0.737952, test acc: 0.745763\n",
      "4027, train loss: 0.568952, test loss: 0.589653, train acc: 0.737952, test acc: 0.745763\n",
      "4028, train loss: 0.568943, test loss: 0.589642, train acc: 0.737952, test acc: 0.745763\n",
      "4029, train loss: 0.568934, test loss: 0.58963, train acc: 0.737952, test acc: 0.745763\n",
      "4030, train loss: 0.568925, test loss: 0.589619, train acc: 0.737952, test acc: 0.745763\n",
      "4031, train loss: 0.568915, test loss: 0.589608, train acc: 0.737952, test acc: 0.745763\n",
      "4032, train loss: 0.568906, test loss: 0.589597, train acc: 0.737952, test acc: 0.745763\n",
      "4033, train loss: 0.568897, test loss: 0.589586, train acc: 0.737952, test acc: 0.745763\n",
      "4034, train loss: 0.568888, test loss: 0.589575, train acc: 0.737952, test acc: 0.745763\n",
      "4035, train loss: 0.568879, test loss: 0.589564, train acc: 0.737952, test acc: 0.745763\n",
      "4036, train loss: 0.568869, test loss: 0.589553, train acc: 0.737952, test acc: 0.745763\n",
      "4037, train loss: 0.56886, test loss: 0.589541, train acc: 0.737952, test acc: 0.745763\n",
      "4038, train loss: 0.568851, test loss: 0.58953, train acc: 0.737952, test acc: 0.745763\n",
      "4039, train loss: 0.568842, test loss: 0.589519, train acc: 0.737952, test acc: 0.745763\n",
      "4040, train loss: 0.568833, test loss: 0.589508, train acc: 0.737952, test acc: 0.745763\n",
      "4041, train loss: 0.568823, test loss: 0.589497, train acc: 0.737952, test acc: 0.745763\n",
      "4042, train loss: 0.568814, test loss: 0.589486, train acc: 0.737952, test acc: 0.745763\n",
      "4043, train loss: 0.568805, test loss: 0.589475, train acc: 0.737952, test acc: 0.745763\n",
      "4044, train loss: 0.568796, test loss: 0.589464, train acc: 0.737952, test acc: 0.745763\n",
      "4045, train loss: 0.568787, test loss: 0.589452, train acc: 0.737952, test acc: 0.745763\n",
      "4046, train loss: 0.568777, test loss: 0.589441, train acc: 0.737952, test acc: 0.745763\n",
      "4047, train loss: 0.568768, test loss: 0.58943, train acc: 0.737952, test acc: 0.745763\n",
      "4048, train loss: 0.568759, test loss: 0.589419, train acc: 0.737952, test acc: 0.745763\n",
      "4049, train loss: 0.56875, test loss: 0.589408, train acc: 0.737952, test acc: 0.745763\n",
      "4050, train loss: 0.568741, test loss: 0.589397, train acc: 0.737952, test acc: 0.745763\n",
      "4051, train loss: 0.568731, test loss: 0.589386, train acc: 0.737952, test acc: 0.745763\n",
      "4052, train loss: 0.568722, test loss: 0.589375, train acc: 0.737952, test acc: 0.745763\n",
      "4053, train loss: 0.568713, test loss: 0.589363, train acc: 0.737952, test acc: 0.745763\n",
      "4054, train loss: 0.568704, test loss: 0.589352, train acc: 0.737952, test acc: 0.745763\n",
      "4055, train loss: 0.568695, test loss: 0.589341, train acc: 0.737952, test acc: 0.745763\n",
      "4056, train loss: 0.568685, test loss: 0.58933, train acc: 0.737952, test acc: 0.745763\n",
      "4057, train loss: 0.568676, test loss: 0.589319, train acc: 0.737952, test acc: 0.745763\n",
      "4058, train loss: 0.568667, test loss: 0.589308, train acc: 0.737952, test acc: 0.745763\n",
      "4059, train loss: 0.568658, test loss: 0.589297, train acc: 0.737952, test acc: 0.745763\n",
      "4060, train loss: 0.568649, test loss: 0.589286, train acc: 0.737952, test acc: 0.745763\n",
      "4061, train loss: 0.568639, test loss: 0.589275, train acc: 0.737952, test acc: 0.745763\n",
      "4062, train loss: 0.56863, test loss: 0.589264, train acc: 0.737952, test acc: 0.745763\n",
      "4063, train loss: 0.568621, test loss: 0.589252, train acc: 0.737952, test acc: 0.745763\n",
      "4064, train loss: 0.568612, test loss: 0.589241, train acc: 0.737952, test acc: 0.745763\n",
      "4065, train loss: 0.568603, test loss: 0.58923, train acc: 0.737952, test acc: 0.745763\n",
      "4066, train loss: 0.568594, test loss: 0.589219, train acc: 0.737952, test acc: 0.745763\n",
      "4067, train loss: 0.568585, test loss: 0.589208, train acc: 0.737952, test acc: 0.745763\n",
      "4068, train loss: 0.568575, test loss: 0.589197, train acc: 0.737952, test acc: 0.745763\n",
      "4069, train loss: 0.568566, test loss: 0.589186, train acc: 0.737952, test acc: 0.745763\n",
      "4070, train loss: 0.568557, test loss: 0.589175, train acc: 0.737952, test acc: 0.745763\n",
      "4071, train loss: 0.568548, test loss: 0.589164, train acc: 0.737952, test acc: 0.745763\n",
      "4072, train loss: 0.568539, test loss: 0.589153, train acc: 0.737952, test acc: 0.745763\n",
      "4073, train loss: 0.568529, test loss: 0.589142, train acc: 0.737952, test acc: 0.745763\n",
      "4074, train loss: 0.56852, test loss: 0.589131, train acc: 0.737952, test acc: 0.745763\n",
      "4075, train loss: 0.568511, test loss: 0.589119, train acc: 0.737952, test acc: 0.745763\n",
      "4076, train loss: 0.568502, test loss: 0.589108, train acc: 0.737952, test acc: 0.745763\n",
      "4077, train loss: 0.568493, test loss: 0.589097, train acc: 0.737952, test acc: 0.745763\n",
      "4078, train loss: 0.568484, test loss: 0.589086, train acc: 0.737952, test acc: 0.745763\n",
      "4079, train loss: 0.568475, test loss: 0.589075, train acc: 0.737952, test acc: 0.745763\n",
      "4080, train loss: 0.568465, test loss: 0.589064, train acc: 0.737952, test acc: 0.745763\n",
      "4081, train loss: 0.568456, test loss: 0.589053, train acc: 0.737952, test acc: 0.745763\n",
      "4082, train loss: 0.568447, test loss: 0.589042, train acc: 0.737952, test acc: 0.745763\n",
      "4083, train loss: 0.568438, test loss: 0.589031, train acc: 0.737952, test acc: 0.745763\n",
      "4084, train loss: 0.568429, test loss: 0.58902, train acc: 0.737952, test acc: 0.745763\n",
      "4085, train loss: 0.56842, test loss: 0.589009, train acc: 0.737952, test acc: 0.745763\n",
      "4086, train loss: 0.568411, test loss: 0.588998, train acc: 0.737952, test acc: 0.745763\n",
      "4087, train loss: 0.568401, test loss: 0.588987, train acc: 0.737952, test acc: 0.745763\n",
      "4088, train loss: 0.568392, test loss: 0.588976, train acc: 0.737952, test acc: 0.745763\n",
      "4089, train loss: 0.568383, test loss: 0.588964, train acc: 0.737952, test acc: 0.745763\n",
      "4090, train loss: 0.568374, test loss: 0.588953, train acc: 0.737952, test acc: 0.745763\n",
      "4091, train loss: 0.568365, test loss: 0.588942, train acc: 0.737952, test acc: 0.745763\n",
      "4092, train loss: 0.568356, test loss: 0.588931, train acc: 0.737952, test acc: 0.745763\n",
      "4093, train loss: 0.568347, test loss: 0.58892, train acc: 0.737952, test acc: 0.745763\n",
      "4094, train loss: 0.568337, test loss: 0.588909, train acc: 0.737952, test acc: 0.745763\n",
      "4095, train loss: 0.568328, test loss: 0.588898, train acc: 0.737952, test acc: 0.745763\n",
      "4096, train loss: 0.568319, test loss: 0.588887, train acc: 0.737952, test acc: 0.745763\n",
      "4097, train loss: 0.56831, test loss: 0.588876, train acc: 0.737952, test acc: 0.745763\n",
      "4098, train loss: 0.568301, test loss: 0.588865, train acc: 0.737952, test acc: 0.745763\n",
      "4099, train loss: 0.568292, test loss: 0.588854, train acc: 0.737952, test acc: 0.745763\n",
      "4100, train loss: 0.568283, test loss: 0.588843, train acc: 0.737952, test acc: 0.745763\n",
      "4101, train loss: 0.568274, test loss: 0.588832, train acc: 0.737952, test acc: 0.745763\n",
      "4102, train loss: 0.568264, test loss: 0.588821, train acc: 0.737952, test acc: 0.745763\n",
      "4103, train loss: 0.568255, test loss: 0.58881, train acc: 0.737952, test acc: 0.745763\n",
      "4104, train loss: 0.568246, test loss: 0.588799, train acc: 0.737952, test acc: 0.745763\n",
      "4105, train loss: 0.568237, test loss: 0.588788, train acc: 0.737952, test acc: 0.745763\n",
      "4106, train loss: 0.568228, test loss: 0.588777, train acc: 0.737952, test acc: 0.745763\n",
      "4107, train loss: 0.568219, test loss: 0.588766, train acc: 0.737952, test acc: 0.745763\n",
      "4108, train loss: 0.56821, test loss: 0.588754, train acc: 0.737952, test acc: 0.745763\n",
      "4109, train loss: 0.568201, test loss: 0.588743, train acc: 0.737952, test acc: 0.745763\n",
      "4110, train loss: 0.568192, test loss: 0.588732, train acc: 0.737952, test acc: 0.745763\n",
      "4111, train loss: 0.568182, test loss: 0.588721, train acc: 0.737952, test acc: 0.745763\n",
      "4112, train loss: 0.568173, test loss: 0.58871, train acc: 0.737952, test acc: 0.745763\n",
      "4113, train loss: 0.568164, test loss: 0.588699, train acc: 0.737952, test acc: 0.745763\n",
      "4114, train loss: 0.568155, test loss: 0.588688, train acc: 0.737952, test acc: 0.745763\n",
      "4115, train loss: 0.568146, test loss: 0.588677, train acc: 0.737952, test acc: 0.745763\n",
      "4116, train loss: 0.568137, test loss: 0.588666, train acc: 0.737952, test acc: 0.745763\n",
      "4117, train loss: 0.568128, test loss: 0.588655, train acc: 0.737952, test acc: 0.745763\n",
      "4118, train loss: 0.568119, test loss: 0.588644, train acc: 0.737952, test acc: 0.745763\n",
      "4119, train loss: 0.56811, test loss: 0.588633, train acc: 0.737952, test acc: 0.745763\n",
      "4120, train loss: 0.568101, test loss: 0.588622, train acc: 0.737952, test acc: 0.745763\n",
      "4121, train loss: 0.568091, test loss: 0.588611, train acc: 0.737952, test acc: 0.745763\n",
      "4122, train loss: 0.568082, test loss: 0.5886, train acc: 0.737952, test acc: 0.745763\n",
      "4123, train loss: 0.568073, test loss: 0.588589, train acc: 0.737952, test acc: 0.745763\n",
      "4124, train loss: 0.568064, test loss: 0.588578, train acc: 0.737952, test acc: 0.745763\n",
      "4125, train loss: 0.568055, test loss: 0.588567, train acc: 0.737952, test acc: 0.745763\n",
      "4126, train loss: 0.568046, test loss: 0.588556, train acc: 0.737952, test acc: 0.745763\n",
      "4127, train loss: 0.568037, test loss: 0.588545, train acc: 0.737952, test acc: 0.745763\n",
      "4128, train loss: 0.568028, test loss: 0.588534, train acc: 0.737952, test acc: 0.745763\n",
      "4129, train loss: 0.568019, test loss: 0.588523, train acc: 0.737952, test acc: 0.745763\n",
      "4130, train loss: 0.56801, test loss: 0.588512, train acc: 0.737952, test acc: 0.745763\n",
      "4131, train loss: 0.568001, test loss: 0.588501, train acc: 0.737952, test acc: 0.745763\n",
      "4132, train loss: 0.567991, test loss: 0.58849, train acc: 0.737952, test acc: 0.745763\n",
      "4133, train loss: 0.567982, test loss: 0.588479, train acc: 0.737952, test acc: 0.745763\n",
      "4134, train loss: 0.567973, test loss: 0.588468, train acc: 0.737952, test acc: 0.745763\n",
      "4135, train loss: 0.567964, test loss: 0.588457, train acc: 0.737952, test acc: 0.745763\n",
      "4136, train loss: 0.567955, test loss: 0.588446, train acc: 0.737952, test acc: 0.745763\n",
      "4137, train loss: 0.567946, test loss: 0.588435, train acc: 0.737952, test acc: 0.745763\n",
      "4138, train loss: 0.567937, test loss: 0.588424, train acc: 0.737952, test acc: 0.745763\n",
      "4139, train loss: 0.567928, test loss: 0.588413, train acc: 0.737952, test acc: 0.745763\n",
      "4140, train loss: 0.567919, test loss: 0.588402, train acc: 0.737952, test acc: 0.745763\n",
      "4141, train loss: 0.56791, test loss: 0.588391, train acc: 0.737952, test acc: 0.745763\n",
      "4142, train loss: 0.567901, test loss: 0.58838, train acc: 0.737952, test acc: 0.745763\n",
      "4143, train loss: 0.567892, test loss: 0.588369, train acc: 0.737952, test acc: 0.745763\n",
      "4144, train loss: 0.567883, test loss: 0.588358, train acc: 0.737952, test acc: 0.745763\n",
      "4145, train loss: 0.567874, test loss: 0.588347, train acc: 0.737952, test acc: 0.745763\n",
      "4146, train loss: 0.567865, test loss: 0.588336, train acc: 0.737952, test acc: 0.745763\n",
      "4147, train loss: 0.567856, test loss: 0.588325, train acc: 0.737952, test acc: 0.745763\n",
      "4148, train loss: 0.567846, test loss: 0.588314, train acc: 0.737952, test acc: 0.745763\n",
      "4149, train loss: 0.567837, test loss: 0.588303, train acc: 0.737952, test acc: 0.745763\n",
      "4150, train loss: 0.567828, test loss: 0.588292, train acc: 0.737952, test acc: 0.745763\n",
      "4151, train loss: 0.567819, test loss: 0.588281, train acc: 0.737952, test acc: 0.745763\n",
      "4152, train loss: 0.56781, test loss: 0.58827, train acc: 0.737952, test acc: 0.745763\n",
      "4153, train loss: 0.567801, test loss: 0.588259, train acc: 0.737952, test acc: 0.745763\n",
      "4154, train loss: 0.567792, test loss: 0.588248, train acc: 0.737952, test acc: 0.745763\n",
      "4155, train loss: 0.567783, test loss: 0.588237, train acc: 0.737952, test acc: 0.745763\n",
      "4156, train loss: 0.567774, test loss: 0.588226, train acc: 0.737952, test acc: 0.745763\n",
      "4157, train loss: 0.567765, test loss: 0.588215, train acc: 0.737952, test acc: 0.745763\n",
      "4158, train loss: 0.567756, test loss: 0.588204, train acc: 0.737952, test acc: 0.745763\n",
      "4159, train loss: 0.567747, test loss: 0.588193, train acc: 0.737952, test acc: 0.745763\n",
      "4160, train loss: 0.567738, test loss: 0.588182, train acc: 0.737952, test acc: 0.745763\n",
      "4161, train loss: 0.567729, test loss: 0.588171, train acc: 0.737952, test acc: 0.745763\n",
      "4162, train loss: 0.56772, test loss: 0.58816, train acc: 0.737952, test acc: 0.745763\n",
      "4163, train loss: 0.567711, test loss: 0.588149, train acc: 0.737952, test acc: 0.745763\n",
      "4164, train loss: 0.567702, test loss: 0.588138, train acc: 0.737952, test acc: 0.745763\n",
      "4165, train loss: 0.567693, test loss: 0.588127, train acc: 0.737952, test acc: 0.745763\n",
      "4166, train loss: 0.567684, test loss: 0.588116, train acc: 0.737952, test acc: 0.745763\n",
      "4167, train loss: 0.567675, test loss: 0.588105, train acc: 0.737952, test acc: 0.745763\n",
      "4168, train loss: 0.567666, test loss: 0.588094, train acc: 0.737952, test acc: 0.745763\n",
      "4169, train loss: 0.567657, test loss: 0.588083, train acc: 0.737952, test acc: 0.745763\n",
      "4170, train loss: 0.567648, test loss: 0.588072, train acc: 0.737952, test acc: 0.745763\n",
      "4171, train loss: 0.567639, test loss: 0.588062, train acc: 0.737952, test acc: 0.745763\n",
      "4172, train loss: 0.56763, test loss: 0.588051, train acc: 0.737952, test acc: 0.745763\n",
      "4173, train loss: 0.567621, test loss: 0.58804, train acc: 0.737952, test acc: 0.745763\n",
      "4174, train loss: 0.567612, test loss: 0.588029, train acc: 0.737952, test acc: 0.745763\n",
      "4175, train loss: 0.567603, test loss: 0.588018, train acc: 0.737952, test acc: 0.745763\n",
      "4176, train loss: 0.567594, test loss: 0.588007, train acc: 0.737952, test acc: 0.745763\n",
      "4177, train loss: 0.567585, test loss: 0.587996, train acc: 0.737952, test acc: 0.745763\n",
      "4178, train loss: 0.567575, test loss: 0.587985, train acc: 0.737952, test acc: 0.745763\n",
      "4179, train loss: 0.567566, test loss: 0.587974, train acc: 0.737952, test acc: 0.745763\n",
      "4180, train loss: 0.567557, test loss: 0.587963, train acc: 0.737952, test acc: 0.745763\n",
      "4181, train loss: 0.567548, test loss: 0.587952, train acc: 0.737952, test acc: 0.745763\n",
      "4182, train loss: 0.567539, test loss: 0.587941, train acc: 0.737952, test acc: 0.745763\n",
      "4183, train loss: 0.56753, test loss: 0.58793, train acc: 0.737952, test acc: 0.745763\n",
      "4184, train loss: 0.567521, test loss: 0.587919, train acc: 0.737952, test acc: 0.745763\n",
      "4185, train loss: 0.567512, test loss: 0.587908, train acc: 0.737952, test acc: 0.745763\n",
      "4186, train loss: 0.567503, test loss: 0.587897, train acc: 0.737952, test acc: 0.745763\n",
      "4187, train loss: 0.567494, test loss: 0.587886, train acc: 0.737952, test acc: 0.745763\n",
      "4188, train loss: 0.567485, test loss: 0.587875, train acc: 0.737952, test acc: 0.745763\n",
      "4189, train loss: 0.567476, test loss: 0.587864, train acc: 0.737952, test acc: 0.745763\n",
      "4190, train loss: 0.567467, test loss: 0.587853, train acc: 0.737952, test acc: 0.745763\n",
      "4191, train loss: 0.567458, test loss: 0.587843, train acc: 0.737952, test acc: 0.745763\n",
      "4192, train loss: 0.567449, test loss: 0.587832, train acc: 0.737952, test acc: 0.745763\n",
      "4193, train loss: 0.56744, test loss: 0.587821, train acc: 0.737952, test acc: 0.745763\n",
      "4194, train loss: 0.567431, test loss: 0.58781, train acc: 0.737952, test acc: 0.745763\n",
      "4195, train loss: 0.567422, test loss: 0.587799, train acc: 0.737952, test acc: 0.745763\n",
      "4196, train loss: 0.567413, test loss: 0.587788, train acc: 0.737952, test acc: 0.745763\n",
      "4197, train loss: 0.567404, test loss: 0.587777, train acc: 0.737952, test acc: 0.745763\n",
      "4198, train loss: 0.567395, test loss: 0.587766, train acc: 0.737952, test acc: 0.745763\n",
      "4199, train loss: 0.567386, test loss: 0.587755, train acc: 0.737952, test acc: 0.745763\n",
      "4200, train loss: 0.567378, test loss: 0.587744, train acc: 0.737952, test acc: 0.745763\n",
      "4201, train loss: 0.567369, test loss: 0.587733, train acc: 0.737952, test acc: 0.745763\n",
      "4202, train loss: 0.56736, test loss: 0.587722, train acc: 0.737952, test acc: 0.745763\n",
      "4203, train loss: 0.567351, test loss: 0.587711, train acc: 0.737952, test acc: 0.745763\n",
      "4204, train loss: 0.567342, test loss: 0.5877, train acc: 0.737952, test acc: 0.745763\n",
      "4205, train loss: 0.567333, test loss: 0.587689, train acc: 0.737952, test acc: 0.745763\n",
      "4206, train loss: 0.567324, test loss: 0.587679, train acc: 0.737952, test acc: 0.745763\n",
      "4207, train loss: 0.567315, test loss: 0.587668, train acc: 0.737952, test acc: 0.745763\n",
      "4208, train loss: 0.567306, test loss: 0.587657, train acc: 0.737952, test acc: 0.745763\n",
      "4209, train loss: 0.567297, test loss: 0.587646, train acc: 0.737952, test acc: 0.745763\n",
      "4210, train loss: 0.567288, test loss: 0.587635, train acc: 0.737952, test acc: 0.745763\n",
      "4211, train loss: 0.567279, test loss: 0.587624, train acc: 0.737952, test acc: 0.745763\n",
      "4212, train loss: 0.56727, test loss: 0.587613, train acc: 0.737952, test acc: 0.745763\n",
      "4213, train loss: 0.567261, test loss: 0.587602, train acc: 0.737952, test acc: 0.745763\n",
      "4214, train loss: 0.567252, test loss: 0.587591, train acc: 0.737952, test acc: 0.745763\n",
      "4215, train loss: 0.567243, test loss: 0.58758, train acc: 0.737952, test acc: 0.745763\n",
      "4216, train loss: 0.567234, test loss: 0.587569, train acc: 0.737952, test acc: 0.745763\n",
      "4217, train loss: 0.567225, test loss: 0.587559, train acc: 0.737952, test acc: 0.745763\n",
      "4218, train loss: 0.567216, test loss: 0.587548, train acc: 0.737952, test acc: 0.745763\n",
      "4219, train loss: 0.567207, test loss: 0.587537, train acc: 0.737952, test acc: 0.745763\n",
      "4220, train loss: 0.567198, test loss: 0.587526, train acc: 0.737952, test acc: 0.745763\n",
      "4221, train loss: 0.567189, test loss: 0.587515, train acc: 0.737952, test acc: 0.745763\n",
      "4222, train loss: 0.56718, test loss: 0.587504, train acc: 0.737952, test acc: 0.745763\n",
      "4223, train loss: 0.567171, test loss: 0.587493, train acc: 0.737952, test acc: 0.745763\n",
      "4224, train loss: 0.567162, test loss: 0.587482, train acc: 0.737952, test acc: 0.745763\n",
      "4225, train loss: 0.567153, test loss: 0.587471, train acc: 0.737952, test acc: 0.745763\n",
      "4226, train loss: 0.567144, test loss: 0.58746, train acc: 0.737952, test acc: 0.745763\n",
      "4227, train loss: 0.567135, test loss: 0.58745, train acc: 0.737952, test acc: 0.745763\n",
      "4228, train loss: 0.567126, test loss: 0.587439, train acc: 0.737952, test acc: 0.745763\n",
      "4229, train loss: 0.567118, test loss: 0.587428, train acc: 0.737952, test acc: 0.745763\n",
      "4230, train loss: 0.567109, test loss: 0.587417, train acc: 0.737952, test acc: 0.745763\n",
      "4231, train loss: 0.5671, test loss: 0.587406, train acc: 0.737952, test acc: 0.745763\n",
      "4232, train loss: 0.567091, test loss: 0.587395, train acc: 0.737952, test acc: 0.745763\n",
      "4233, train loss: 0.567082, test loss: 0.587384, train acc: 0.737952, test acc: 0.745763\n",
      "4234, train loss: 0.567073, test loss: 0.587373, train acc: 0.737952, test acc: 0.745763\n",
      "4235, train loss: 0.567064, test loss: 0.587362, train acc: 0.737952, test acc: 0.745763\n",
      "4236, train loss: 0.567055, test loss: 0.587352, train acc: 0.737952, test acc: 0.745763\n",
      "4237, train loss: 0.567046, test loss: 0.587341, train acc: 0.737952, test acc: 0.745763\n",
      "4238, train loss: 0.567037, test loss: 0.58733, train acc: 0.737952, test acc: 0.745763\n",
      "4239, train loss: 0.567028, test loss: 0.587319, train acc: 0.737952, test acc: 0.745763\n",
      "4240, train loss: 0.567019, test loss: 0.587308, train acc: 0.737952, test acc: 0.745763\n",
      "4241, train loss: 0.56701, test loss: 0.587297, train acc: 0.737952, test acc: 0.745763\n",
      "4242, train loss: 0.567001, test loss: 0.587286, train acc: 0.737952, test acc: 0.745763\n",
      "4243, train loss: 0.566992, test loss: 0.587275, train acc: 0.737952, test acc: 0.745763\n",
      "4244, train loss: 0.566983, test loss: 0.587264, train acc: 0.737952, test acc: 0.745763\n",
      "4245, train loss: 0.566975, test loss: 0.587254, train acc: 0.737952, test acc: 0.745763\n",
      "4246, train loss: 0.566966, test loss: 0.587243, train acc: 0.737952, test acc: 0.745763\n",
      "4247, train loss: 0.566957, test loss: 0.587232, train acc: 0.737952, test acc: 0.745763\n",
      "4248, train loss: 0.566948, test loss: 0.587221, train acc: 0.737952, test acc: 0.745763\n",
      "4249, train loss: 0.566939, test loss: 0.58721, train acc: 0.737952, test acc: 0.745763\n",
      "4250, train loss: 0.56693, test loss: 0.587199, train acc: 0.737952, test acc: 0.745763\n",
      "4251, train loss: 0.566921, test loss: 0.587188, train acc: 0.737952, test acc: 0.745763\n",
      "4252, train loss: 0.566912, test loss: 0.587178, train acc: 0.737952, test acc: 0.745763\n",
      "4253, train loss: 0.566903, test loss: 0.587167, train acc: 0.737952, test acc: 0.745763\n",
      "4254, train loss: 0.566894, test loss: 0.587156, train acc: 0.737952, test acc: 0.745763\n",
      "4255, train loss: 0.566885, test loss: 0.587145, train acc: 0.737952, test acc: 0.745763\n",
      "4256, train loss: 0.566876, test loss: 0.587134, train acc: 0.737952, test acc: 0.745763\n",
      "4257, train loss: 0.566868, test loss: 0.587123, train acc: 0.737952, test acc: 0.745763\n",
      "4258, train loss: 0.566859, test loss: 0.587112, train acc: 0.737952, test acc: 0.745763\n",
      "4259, train loss: 0.56685, test loss: 0.587102, train acc: 0.737952, test acc: 0.745763\n",
      "4260, train loss: 0.566841, test loss: 0.587091, train acc: 0.737952, test acc: 0.745763\n",
      "4261, train loss: 0.566832, test loss: 0.58708, train acc: 0.737952, test acc: 0.745763\n",
      "4262, train loss: 0.566823, test loss: 0.587069, train acc: 0.737952, test acc: 0.745763\n",
      "4263, train loss: 0.566814, test loss: 0.587058, train acc: 0.737952, test acc: 0.745763\n",
      "4264, train loss: 0.566805, test loss: 0.587047, train acc: 0.737952, test acc: 0.745763\n",
      "4265, train loss: 0.566796, test loss: 0.587036, train acc: 0.737952, test acc: 0.745763\n",
      "4266, train loss: 0.566787, test loss: 0.587026, train acc: 0.737952, test acc: 0.745763\n",
      "4267, train loss: 0.566779, test loss: 0.587015, train acc: 0.737952, test acc: 0.745763\n",
      "4268, train loss: 0.56677, test loss: 0.587004, train acc: 0.737952, test acc: 0.745763\n",
      "4269, train loss: 0.566761, test loss: 0.586993, train acc: 0.737952, test acc: 0.745763\n",
      "4270, train loss: 0.566752, test loss: 0.586982, train acc: 0.737952, test acc: 0.745763\n",
      "4271, train loss: 0.566743, test loss: 0.586971, train acc: 0.737952, test acc: 0.745763\n",
      "4272, train loss: 0.566734, test loss: 0.586961, train acc: 0.737952, test acc: 0.745763\n",
      "4273, train loss: 0.566725, test loss: 0.58695, train acc: 0.737952, test acc: 0.745763\n",
      "4274, train loss: 0.566716, test loss: 0.586939, train acc: 0.737952, test acc: 0.745763\n",
      "4275, train loss: 0.566707, test loss: 0.586928, train acc: 0.737952, test acc: 0.745763\n",
      "4276, train loss: 0.566699, test loss: 0.586917, train acc: 0.737952, test acc: 0.745763\n",
      "4277, train loss: 0.56669, test loss: 0.586906, train acc: 0.737952, test acc: 0.745763\n",
      "4278, train loss: 0.566681, test loss: 0.586896, train acc: 0.737952, test acc: 0.745763\n",
      "4279, train loss: 0.566672, test loss: 0.586885, train acc: 0.737952, test acc: 0.745763\n",
      "4280, train loss: 0.566663, test loss: 0.586874, train acc: 0.737952, test acc: 0.745763\n",
      "4281, train loss: 0.566654, test loss: 0.586863, train acc: 0.737952, test acc: 0.745763\n",
      "4282, train loss: 0.566645, test loss: 0.586852, train acc: 0.737952, test acc: 0.745763\n",
      "4283, train loss: 0.566636, test loss: 0.586841, train acc: 0.737952, test acc: 0.745763\n",
      "4284, train loss: 0.566628, test loss: 0.586831, train acc: 0.737952, test acc: 0.745763\n",
      "4285, train loss: 0.566619, test loss: 0.58682, train acc: 0.737952, test acc: 0.745763\n",
      "4286, train loss: 0.56661, test loss: 0.586809, train acc: 0.737952, test acc: 0.745763\n",
      "4287, train loss: 0.566601, test loss: 0.586798, train acc: 0.737952, test acc: 0.745763\n",
      "4288, train loss: 0.566592, test loss: 0.586787, train acc: 0.737952, test acc: 0.745763\n",
      "4289, train loss: 0.566583, test loss: 0.586776, train acc: 0.737952, test acc: 0.745763\n",
      "4290, train loss: 0.566574, test loss: 0.586766, train acc: 0.737952, test acc: 0.745763\n",
      "4291, train loss: 0.566565, test loss: 0.586755, train acc: 0.737952, test acc: 0.745763\n",
      "4292, train loss: 0.566557, test loss: 0.586744, train acc: 0.737952, test acc: 0.745763\n",
      "4293, train loss: 0.566548, test loss: 0.586733, train acc: 0.737952, test acc: 0.745763\n",
      "4294, train loss: 0.566539, test loss: 0.586722, train acc: 0.737952, test acc: 0.745763\n",
      "4295, train loss: 0.56653, test loss: 0.586712, train acc: 0.737952, test acc: 0.745763\n",
      "4296, train loss: 0.566521, test loss: 0.586701, train acc: 0.737952, test acc: 0.745763\n",
      "4297, train loss: 0.566512, test loss: 0.58669, train acc: 0.737952, test acc: 0.745763\n",
      "4298, train loss: 0.566503, test loss: 0.586679, train acc: 0.737952, test acc: 0.745763\n",
      "4299, train loss: 0.566495, test loss: 0.586668, train acc: 0.737952, test acc: 0.745763\n",
      "4300, train loss: 0.566486, test loss: 0.586658, train acc: 0.737952, test acc: 0.745763\n",
      "4301, train loss: 0.566477, test loss: 0.586647, train acc: 0.737952, test acc: 0.745763\n",
      "4302, train loss: 0.566468, test loss: 0.586636, train acc: 0.737952, test acc: 0.745763\n",
      "4303, train loss: 0.566459, test loss: 0.586625, train acc: 0.737952, test acc: 0.745763\n",
      "4304, train loss: 0.56645, test loss: 0.586614, train acc: 0.737952, test acc: 0.745763\n",
      "4305, train loss: 0.566441, test loss: 0.586603, train acc: 0.737952, test acc: 0.745763\n",
      "4306, train loss: 0.566433, test loss: 0.586593, train acc: 0.737952, test acc: 0.745763\n",
      "4307, train loss: 0.566424, test loss: 0.586582, train acc: 0.737952, test acc: 0.745763\n",
      "4308, train loss: 0.566415, test loss: 0.586571, train acc: 0.737952, test acc: 0.745763\n",
      "4309, train loss: 0.566406, test loss: 0.58656, train acc: 0.737952, test acc: 0.745763\n",
      "4310, train loss: 0.566397, test loss: 0.586549, train acc: 0.737952, test acc: 0.745763\n",
      "4311, train loss: 0.566388, test loss: 0.586539, train acc: 0.737952, test acc: 0.745763\n",
      "4312, train loss: 0.56638, test loss: 0.586528, train acc: 0.737952, test acc: 0.745763\n",
      "4313, train loss: 0.566371, test loss: 0.586517, train acc: 0.737952, test acc: 0.745763\n",
      "4314, train loss: 0.566362, test loss: 0.586506, train acc: 0.737952, test acc: 0.745763\n",
      "4315, train loss: 0.566353, test loss: 0.586495, train acc: 0.737952, test acc: 0.745763\n",
      "4316, train loss: 0.566344, test loss: 0.586485, train acc: 0.737952, test acc: 0.745763\n",
      "4317, train loss: 0.566335, test loss: 0.586474, train acc: 0.737952, test acc: 0.745763\n",
      "4318, train loss: 0.566327, test loss: 0.586463, train acc: 0.737952, test acc: 0.745763\n",
      "4319, train loss: 0.566318, test loss: 0.586452, train acc: 0.737952, test acc: 0.745763\n",
      "4320, train loss: 0.566309, test loss: 0.586442, train acc: 0.737952, test acc: 0.745763\n",
      "4321, train loss: 0.5663, test loss: 0.586431, train acc: 0.737952, test acc: 0.745763\n",
      "4322, train loss: 0.566291, test loss: 0.58642, train acc: 0.737952, test acc: 0.745763\n",
      "4323, train loss: 0.566282, test loss: 0.586409, train acc: 0.737952, test acc: 0.745763\n",
      "4324, train loss: 0.566274, test loss: 0.586398, train acc: 0.737952, test acc: 0.745763\n",
      "4325, train loss: 0.566265, test loss: 0.586388, train acc: 0.737952, test acc: 0.745763\n",
      "4326, train loss: 0.566256, test loss: 0.586377, train acc: 0.737952, test acc: 0.745763\n",
      "4327, train loss: 0.566247, test loss: 0.586366, train acc: 0.737952, test acc: 0.745763\n",
      "4328, train loss: 0.566238, test loss: 0.586355, train acc: 0.737952, test acc: 0.745763\n",
      "4329, train loss: 0.566229, test loss: 0.586344, train acc: 0.737952, test acc: 0.745763\n",
      "4330, train loss: 0.566221, test loss: 0.586334, train acc: 0.737952, test acc: 0.745763\n",
      "4331, train loss: 0.566212, test loss: 0.586323, train acc: 0.737952, test acc: 0.745763\n",
      "4332, train loss: 0.566203, test loss: 0.586312, train acc: 0.737952, test acc: 0.745763\n",
      "4333, train loss: 0.566194, test loss: 0.586301, train acc: 0.737952, test acc: 0.745763\n",
      "4334, train loss: 0.566185, test loss: 0.586291, train acc: 0.737952, test acc: 0.745763\n",
      "4335, train loss: 0.566177, test loss: 0.58628, train acc: 0.737952, test acc: 0.745763\n",
      "4336, train loss: 0.566168, test loss: 0.586269, train acc: 0.737952, test acc: 0.745763\n",
      "4337, train loss: 0.566159, test loss: 0.586258, train acc: 0.737952, test acc: 0.745763\n",
      "4338, train loss: 0.56615, test loss: 0.586248, train acc: 0.737952, test acc: 0.745763\n",
      "4339, train loss: 0.566141, test loss: 0.586237, train acc: 0.737952, test acc: 0.745763\n",
      "4340, train loss: 0.566133, test loss: 0.586226, train acc: 0.737952, test acc: 0.745763\n",
      "4341, train loss: 0.566124, test loss: 0.586215, train acc: 0.737952, test acc: 0.745763\n",
      "4342, train loss: 0.566115, test loss: 0.586205, train acc: 0.737952, test acc: 0.745763\n",
      "4343, train loss: 0.566106, test loss: 0.586194, train acc: 0.737952, test acc: 0.745763\n",
      "4344, train loss: 0.566097, test loss: 0.586183, train acc: 0.737952, test acc: 0.745763\n",
      "4345, train loss: 0.566089, test loss: 0.586172, train acc: 0.737952, test acc: 0.745763\n",
      "4346, train loss: 0.56608, test loss: 0.586162, train acc: 0.737952, test acc: 0.745763\n",
      "4347, train loss: 0.566071, test loss: 0.586151, train acc: 0.737952, test acc: 0.745763\n",
      "4348, train loss: 0.566062, test loss: 0.58614, train acc: 0.737952, test acc: 0.745763\n",
      "4349, train loss: 0.566054, test loss: 0.586129, train acc: 0.737952, test acc: 0.745763\n",
      "4350, train loss: 0.566045, test loss: 0.586119, train acc: 0.737952, test acc: 0.745763\n",
      "4351, train loss: 0.566036, test loss: 0.586108, train acc: 0.737952, test acc: 0.745763\n",
      "4352, train loss: 0.566027, test loss: 0.586097, train acc: 0.737952, test acc: 0.745763\n",
      "4353, train loss: 0.566018, test loss: 0.586086, train acc: 0.737952, test acc: 0.745763\n",
      "4354, train loss: 0.56601, test loss: 0.586076, train acc: 0.737952, test acc: 0.745763\n",
      "4355, train loss: 0.566001, test loss: 0.586065, train acc: 0.737952, test acc: 0.745763\n",
      "4356, train loss: 0.565992, test loss: 0.586054, train acc: 0.737952, test acc: 0.745763\n",
      "4357, train loss: 0.565983, test loss: 0.586043, train acc: 0.737952, test acc: 0.745763\n",
      "4358, train loss: 0.565974, test loss: 0.586033, train acc: 0.737952, test acc: 0.745763\n",
      "4359, train loss: 0.565966, test loss: 0.586022, train acc: 0.737952, test acc: 0.745763\n",
      "4360, train loss: 0.565957, test loss: 0.586011, train acc: 0.737952, test acc: 0.745763\n",
      "4361, train loss: 0.565948, test loss: 0.586, train acc: 0.737952, test acc: 0.745763\n",
      "4362, train loss: 0.565939, test loss: 0.58599, train acc: 0.737952, test acc: 0.745763\n",
      "4363, train loss: 0.565931, test loss: 0.585979, train acc: 0.737952, test acc: 0.745763\n",
      "4364, train loss: 0.565922, test loss: 0.585968, train acc: 0.737952, test acc: 0.745763\n",
      "4365, train loss: 0.565913, test loss: 0.585957, train acc: 0.737952, test acc: 0.745763\n",
      "4366, train loss: 0.565904, test loss: 0.585947, train acc: 0.737952, test acc: 0.745763\n",
      "4367, train loss: 0.565896, test loss: 0.585936, train acc: 0.737952, test acc: 0.745763\n",
      "4368, train loss: 0.565887, test loss: 0.585925, train acc: 0.737952, test acc: 0.745763\n",
      "4369, train loss: 0.565878, test loss: 0.585915, train acc: 0.737952, test acc: 0.745763\n",
      "4370, train loss: 0.565869, test loss: 0.585904, train acc: 0.737952, test acc: 0.745763\n",
      "4371, train loss: 0.565861, test loss: 0.585893, train acc: 0.737952, test acc: 0.745763\n",
      "4372, train loss: 0.565852, test loss: 0.585882, train acc: 0.737952, test acc: 0.745763\n",
      "4373, train loss: 0.565843, test loss: 0.585872, train acc: 0.737952, test acc: 0.745763\n",
      "4374, train loss: 0.565834, test loss: 0.585861, train acc: 0.737952, test acc: 0.745763\n",
      "4375, train loss: 0.565825, test loss: 0.58585, train acc: 0.737952, test acc: 0.745763\n",
      "4376, train loss: 0.565817, test loss: 0.58584, train acc: 0.737952, test acc: 0.745763\n",
      "4377, train loss: 0.565808, test loss: 0.585829, train acc: 0.737952, test acc: 0.745763\n",
      "4378, train loss: 0.565799, test loss: 0.585818, train acc: 0.737952, test acc: 0.745763\n",
      "4379, train loss: 0.56579, test loss: 0.585807, train acc: 0.737952, test acc: 0.745763\n",
      "4380, train loss: 0.565782, test loss: 0.585797, train acc: 0.737952, test acc: 0.745763\n",
      "4381, train loss: 0.565773, test loss: 0.585786, train acc: 0.737952, test acc: 0.745763\n",
      "4382, train loss: 0.565764, test loss: 0.585775, train acc: 0.737952, test acc: 0.745763\n",
      "4383, train loss: 0.565755, test loss: 0.585764, train acc: 0.737952, test acc: 0.745763\n",
      "4384, train loss: 0.565747, test loss: 0.585754, train acc: 0.737952, test acc: 0.745763\n",
      "4385, train loss: 0.565738, test loss: 0.585743, train acc: 0.737952, test acc: 0.745763\n",
      "4386, train loss: 0.565729, test loss: 0.585732, train acc: 0.737952, test acc: 0.745763\n",
      "4387, train loss: 0.56572, test loss: 0.585722, train acc: 0.737952, test acc: 0.745763\n",
      "4388, train loss: 0.565712, test loss: 0.585711, train acc: 0.737952, test acc: 0.745763\n",
      "4389, train loss: 0.565703, test loss: 0.5857, train acc: 0.737952, test acc: 0.745763\n",
      "4390, train loss: 0.565694, test loss: 0.58569, train acc: 0.737952, test acc: 0.745763\n",
      "4391, train loss: 0.565685, test loss: 0.585679, train acc: 0.737952, test acc: 0.745763\n",
      "4392, train loss: 0.565677, test loss: 0.585668, train acc: 0.737952, test acc: 0.745763\n",
      "4393, train loss: 0.565668, test loss: 0.585657, train acc: 0.737952, test acc: 0.745763\n",
      "4394, train loss: 0.565659, test loss: 0.585647, train acc: 0.737952, test acc: 0.745763\n",
      "4395, train loss: 0.565651, test loss: 0.585636, train acc: 0.737952, test acc: 0.745763\n",
      "4396, train loss: 0.565642, test loss: 0.585625, train acc: 0.737952, test acc: 0.745763\n",
      "4397, train loss: 0.565633, test loss: 0.585615, train acc: 0.737952, test acc: 0.745763\n",
      "4398, train loss: 0.565624, test loss: 0.585604, train acc: 0.737952, test acc: 0.745763\n",
      "4399, train loss: 0.565616, test loss: 0.585593, train acc: 0.737952, test acc: 0.745763\n",
      "4400, train loss: 0.565607, test loss: 0.585583, train acc: 0.737952, test acc: 0.745763\n",
      "4401, train loss: 0.565598, test loss: 0.585572, train acc: 0.737952, test acc: 0.745763\n",
      "4402, train loss: 0.565589, test loss: 0.585561, train acc: 0.737952, test acc: 0.745763\n",
      "4403, train loss: 0.565581, test loss: 0.585551, train acc: 0.737952, test acc: 0.745763\n",
      "4404, train loss: 0.565572, test loss: 0.58554, train acc: 0.737952, test acc: 0.745763\n",
      "4405, train loss: 0.565563, test loss: 0.585529, train acc: 0.737952, test acc: 0.745763\n",
      "4406, train loss: 0.565555, test loss: 0.585518, train acc: 0.737952, test acc: 0.745763\n",
      "4407, train loss: 0.565546, test loss: 0.585508, train acc: 0.737952, test acc: 0.745763\n",
      "4408, train loss: 0.565537, test loss: 0.585497, train acc: 0.737952, test acc: 0.745763\n",
      "4409, train loss: 0.565528, test loss: 0.585486, train acc: 0.737952, test acc: 0.745763\n",
      "4410, train loss: 0.56552, test loss: 0.585476, train acc: 0.737952, test acc: 0.745763\n",
      "4411, train loss: 0.565511, test loss: 0.585465, train acc: 0.737952, test acc: 0.745763\n",
      "4412, train loss: 0.565502, test loss: 0.585454, train acc: 0.737952, test acc: 0.745763\n",
      "4413, train loss: 0.565494, test loss: 0.585444, train acc: 0.737952, test acc: 0.745763\n",
      "4414, train loss: 0.565485, test loss: 0.585433, train acc: 0.737952, test acc: 0.745763\n",
      "4415, train loss: 0.565476, test loss: 0.585422, train acc: 0.737952, test acc: 0.745763\n",
      "4416, train loss: 0.565468, test loss: 0.585412, train acc: 0.737952, test acc: 0.745763\n",
      "4417, train loss: 0.565459, test loss: 0.585401, train acc: 0.737952, test acc: 0.745763\n",
      "4418, train loss: 0.56545, test loss: 0.58539, train acc: 0.737952, test acc: 0.745763\n",
      "4419, train loss: 0.565441, test loss: 0.58538, train acc: 0.737952, test acc: 0.745763\n",
      "4420, train loss: 0.565433, test loss: 0.585369, train acc: 0.737952, test acc: 0.745763\n",
      "4421, train loss: 0.565424, test loss: 0.585358, train acc: 0.737952, test acc: 0.745763\n",
      "4422, train loss: 0.565415, test loss: 0.585348, train acc: 0.737952, test acc: 0.745763\n",
      "4423, train loss: 0.565407, test loss: 0.585337, train acc: 0.737952, test acc: 0.745763\n",
      "4424, train loss: 0.565398, test loss: 0.585326, train acc: 0.737952, test acc: 0.745763\n",
      "4425, train loss: 0.565389, test loss: 0.585316, train acc: 0.737952, test acc: 0.745763\n",
      "4426, train loss: 0.565381, test loss: 0.585305, train acc: 0.737952, test acc: 0.745763\n",
      "4427, train loss: 0.565372, test loss: 0.585294, train acc: 0.737952, test acc: 0.745763\n",
      "4428, train loss: 0.565363, test loss: 0.585284, train acc: 0.737952, test acc: 0.745763\n",
      "4429, train loss: 0.565354, test loss: 0.585273, train acc: 0.737952, test acc: 0.745763\n",
      "4430, train loss: 0.565346, test loss: 0.585262, train acc: 0.737952, test acc: 0.745763\n",
      "4431, train loss: 0.565337, test loss: 0.585252, train acc: 0.737952, test acc: 0.745763\n",
      "4432, train loss: 0.565328, test loss: 0.585241, train acc: 0.737952, test acc: 0.745763\n",
      "4433, train loss: 0.56532, test loss: 0.58523, train acc: 0.737952, test acc: 0.745763\n",
      "4434, train loss: 0.565311, test loss: 0.58522, train acc: 0.737952, test acc: 0.745763\n",
      "4435, train loss: 0.565302, test loss: 0.585209, train acc: 0.737952, test acc: 0.745763\n",
      "4436, train loss: 0.565294, test loss: 0.585198, train acc: 0.737952, test acc: 0.745763\n",
      "4437, train loss: 0.565285, test loss: 0.585188, train acc: 0.737952, test acc: 0.745763\n",
      "4438, train loss: 0.565276, test loss: 0.585177, train acc: 0.737952, test acc: 0.745763\n",
      "4439, train loss: 0.565268, test loss: 0.585167, train acc: 0.737952, test acc: 0.745763\n",
      "4440, train loss: 0.565259, test loss: 0.585156, train acc: 0.737952, test acc: 0.745763\n",
      "4441, train loss: 0.56525, test loss: 0.585145, train acc: 0.737952, test acc: 0.745763\n",
      "4442, train loss: 0.565242, test loss: 0.585135, train acc: 0.737952, test acc: 0.745763\n",
      "4443, train loss: 0.565233, test loss: 0.585124, train acc: 0.737952, test acc: 0.745763\n",
      "4444, train loss: 0.565224, test loss: 0.585113, train acc: 0.737952, test acc: 0.745763\n",
      "4445, train loss: 0.565216, test loss: 0.585103, train acc: 0.737952, test acc: 0.745763\n",
      "4446, train loss: 0.565207, test loss: 0.585092, train acc: 0.737952, test acc: 0.745763\n",
      "4447, train loss: 0.565198, test loss: 0.585081, train acc: 0.737952, test acc: 0.745763\n",
      "4448, train loss: 0.565189, test loss: 0.585071, train acc: 0.737952, test acc: 0.745763\n",
      "4449, train loss: 0.565181, test loss: 0.58506, train acc: 0.737952, test acc: 0.745763\n",
      "4450, train loss: 0.565172, test loss: 0.58505, train acc: 0.737952, test acc: 0.745763\n",
      "4451, train loss: 0.565164, test loss: 0.585039, train acc: 0.737952, test acc: 0.745763\n",
      "4452, train loss: 0.565155, test loss: 0.585028, train acc: 0.737952, test acc: 0.745763\n",
      "4453, train loss: 0.565146, test loss: 0.585018, train acc: 0.737952, test acc: 0.745763\n",
      "4454, train loss: 0.565138, test loss: 0.585007, train acc: 0.737952, test acc: 0.745763\n",
      "4455, train loss: 0.565129, test loss: 0.584996, train acc: 0.737952, test acc: 0.745763\n",
      "4456, train loss: 0.56512, test loss: 0.584986, train acc: 0.737952, test acc: 0.745763\n",
      "4457, train loss: 0.565112, test loss: 0.584975, train acc: 0.737952, test acc: 0.745763\n",
      "4458, train loss: 0.565103, test loss: 0.584964, train acc: 0.737952, test acc: 0.745763\n",
      "4459, train loss: 0.565094, test loss: 0.584954, train acc: 0.737952, test acc: 0.745763\n",
      "4460, train loss: 0.565086, test loss: 0.584943, train acc: 0.737952, test acc: 0.745763\n",
      "4461, train loss: 0.565077, test loss: 0.584933, train acc: 0.737952, test acc: 0.745763\n",
      "4462, train loss: 0.565068, test loss: 0.584922, train acc: 0.737952, test acc: 0.745763\n",
      "4463, train loss: 0.56506, test loss: 0.584911, train acc: 0.737952, test acc: 0.745763\n",
      "4464, train loss: 0.565051, test loss: 0.584901, train acc: 0.737952, test acc: 0.745763\n",
      "4465, train loss: 0.565042, test loss: 0.58489, train acc: 0.737952, test acc: 0.745763\n",
      "4466, train loss: 0.565034, test loss: 0.584879, train acc: 0.737952, test acc: 0.745763\n",
      "4467, train loss: 0.565025, test loss: 0.584869, train acc: 0.737952, test acc: 0.745763\n",
      "4468, train loss: 0.565016, test loss: 0.584858, train acc: 0.737952, test acc: 0.745763\n",
      "4469, train loss: 0.565008, test loss: 0.584848, train acc: 0.737952, test acc: 0.745763\n",
      "4470, train loss: 0.564999, test loss: 0.584837, train acc: 0.737952, test acc: 0.745763\n",
      "4471, train loss: 0.564991, test loss: 0.584826, train acc: 0.737952, test acc: 0.745763\n",
      "4472, train loss: 0.564982, test loss: 0.584816, train acc: 0.737952, test acc: 0.745763\n",
      "4473, train loss: 0.564973, test loss: 0.584805, train acc: 0.737952, test acc: 0.745763\n",
      "4474, train loss: 0.564965, test loss: 0.584795, train acc: 0.737952, test acc: 0.745763\n",
      "4475, train loss: 0.564956, test loss: 0.584784, train acc: 0.737952, test acc: 0.745763\n",
      "4476, train loss: 0.564947, test loss: 0.584773, train acc: 0.737952, test acc: 0.745763\n",
      "4477, train loss: 0.564939, test loss: 0.584763, train acc: 0.737952, test acc: 0.745763\n",
      "4478, train loss: 0.56493, test loss: 0.584752, train acc: 0.737952, test acc: 0.745763\n",
      "4479, train loss: 0.564921, test loss: 0.584742, train acc: 0.737952, test acc: 0.745763\n",
      "4480, train loss: 0.564913, test loss: 0.584731, train acc: 0.737952, test acc: 0.745763\n",
      "4481, train loss: 0.564904, test loss: 0.58472, train acc: 0.737952, test acc: 0.745763\n",
      "4482, train loss: 0.564896, test loss: 0.58471, train acc: 0.737952, test acc: 0.745763\n",
      "4483, train loss: 0.564887, test loss: 0.584699, train acc: 0.737952, test acc: 0.745763\n",
      "4484, train loss: 0.564878, test loss: 0.584689, train acc: 0.737952, test acc: 0.745763\n",
      "4485, train loss: 0.56487, test loss: 0.584678, train acc: 0.737952, test acc: 0.745763\n",
      "4486, train loss: 0.564861, test loss: 0.584667, train acc: 0.737952, test acc: 0.745763\n",
      "4487, train loss: 0.564853, test loss: 0.584657, train acc: 0.737952, test acc: 0.745763\n",
      "4488, train loss: 0.564844, test loss: 0.584646, train acc: 0.737952, test acc: 0.745763\n",
      "4489, train loss: 0.564835, test loss: 0.584636, train acc: 0.737952, test acc: 0.745763\n",
      "4490, train loss: 0.564827, test loss: 0.584625, train acc: 0.737952, test acc: 0.745763\n",
      "4491, train loss: 0.564818, test loss: 0.584615, train acc: 0.737952, test acc: 0.745763\n",
      "4492, train loss: 0.564809, test loss: 0.584604, train acc: 0.737952, test acc: 0.745763\n",
      "4493, train loss: 0.564801, test loss: 0.584593, train acc: 0.737952, test acc: 0.745763\n",
      "4494, train loss: 0.564792, test loss: 0.584583, train acc: 0.737952, test acc: 0.745763\n",
      "4495, train loss: 0.564784, test loss: 0.584572, train acc: 0.737952, test acc: 0.745763\n",
      "4496, train loss: 0.564775, test loss: 0.584561, train acc: 0.737952, test acc: 0.745763\n",
      "4497, train loss: 0.564766, test loss: 0.584551, train acc: 0.737952, test acc: 0.745763\n",
      "4498, train loss: 0.564758, test loss: 0.58454, train acc: 0.737952, test acc: 0.745763\n",
      "4499, train loss: 0.564749, test loss: 0.58453, train acc: 0.737952, test acc: 0.745763\n",
      "4500, train loss: 0.564741, test loss: 0.584519, train acc: 0.737952, test acc: 0.745763\n",
      "4501, train loss: 0.564732, test loss: 0.584509, train acc: 0.737952, test acc: 0.745763\n",
      "4502, train loss: 0.564723, test loss: 0.584498, train acc: 0.737952, test acc: 0.745763\n",
      "4503, train loss: 0.564715, test loss: 0.584487, train acc: 0.737952, test acc: 0.745763\n",
      "4504, train loss: 0.564706, test loss: 0.584477, train acc: 0.737952, test acc: 0.745763\n",
      "4505, train loss: 0.564698, test loss: 0.584466, train acc: 0.737952, test acc: 0.745763\n",
      "4506, train loss: 0.564689, test loss: 0.584456, train acc: 0.737952, test acc: 0.745763\n",
      "4507, train loss: 0.56468, test loss: 0.584445, train acc: 0.737952, test acc: 0.745763\n",
      "4508, train loss: 0.564672, test loss: 0.584435, train acc: 0.737952, test acc: 0.745763\n",
      "4509, train loss: 0.564663, test loss: 0.584424, train acc: 0.737952, test acc: 0.745763\n",
      "4510, train loss: 0.564655, test loss: 0.584414, train acc: 0.737952, test acc: 0.745763\n",
      "4511, train loss: 0.564646, test loss: 0.584403, train acc: 0.737952, test acc: 0.745763\n",
      "4512, train loss: 0.564637, test loss: 0.584392, train acc: 0.737952, test acc: 0.745763\n",
      "4513, train loss: 0.564629, test loss: 0.584382, train acc: 0.737952, test acc: 0.745763\n",
      "4514, train loss: 0.56462, test loss: 0.584371, train acc: 0.737952, test acc: 0.745763\n",
      "4515, train loss: 0.564612, test loss: 0.584361, train acc: 0.737952, test acc: 0.745763\n",
      "4516, train loss: 0.564603, test loss: 0.58435, train acc: 0.737952, test acc: 0.745763\n",
      "4517, train loss: 0.564595, test loss: 0.58434, train acc: 0.737952, test acc: 0.745763\n",
      "4518, train loss: 0.564586, test loss: 0.584329, train acc: 0.737952, test acc: 0.745763\n",
      "4519, train loss: 0.564577, test loss: 0.584319, train acc: 0.737952, test acc: 0.745763\n",
      "4520, train loss: 0.564569, test loss: 0.584308, train acc: 0.737952, test acc: 0.745763\n",
      "4521, train loss: 0.56456, test loss: 0.584297, train acc: 0.737952, test acc: 0.745763\n",
      "4522, train loss: 0.564552, test loss: 0.584287, train acc: 0.737952, test acc: 0.745763\n",
      "4523, train loss: 0.564543, test loss: 0.584276, train acc: 0.737952, test acc: 0.745763\n",
      "4524, train loss: 0.564534, test loss: 0.584266, train acc: 0.737952, test acc: 0.745763\n",
      "4525, train loss: 0.564526, test loss: 0.584255, train acc: 0.737952, test acc: 0.745763\n",
      "4526, train loss: 0.564517, test loss: 0.584245, train acc: 0.737952, test acc: 0.745763\n",
      "4527, train loss: 0.564509, test loss: 0.584234, train acc: 0.737952, test acc: 0.745763\n",
      "4528, train loss: 0.5645, test loss: 0.584224, train acc: 0.737952, test acc: 0.745763\n",
      "4529, train loss: 0.564492, test loss: 0.584213, train acc: 0.737952, test acc: 0.745763\n",
      "4530, train loss: 0.564483, test loss: 0.584202, train acc: 0.737952, test acc: 0.745763\n",
      "4531, train loss: 0.564475, test loss: 0.584192, train acc: 0.737952, test acc: 0.745763\n",
      "4532, train loss: 0.564466, test loss: 0.584181, train acc: 0.737952, test acc: 0.745763\n",
      "4533, train loss: 0.564457, test loss: 0.584171, train acc: 0.737952, test acc: 0.745763\n",
      "4534, train loss: 0.564449, test loss: 0.58416, train acc: 0.737952, test acc: 0.745763\n",
      "4535, train loss: 0.56444, test loss: 0.58415, train acc: 0.737952, test acc: 0.745763\n",
      "4536, train loss: 0.564432, test loss: 0.584139, train acc: 0.737952, test acc: 0.745763\n",
      "4537, train loss: 0.564423, test loss: 0.584129, train acc: 0.737952, test acc: 0.745763\n",
      "4538, train loss: 0.564415, test loss: 0.584118, train acc: 0.737952, test acc: 0.745763\n",
      "4539, train loss: 0.564406, test loss: 0.584108, train acc: 0.737952, test acc: 0.745763\n",
      "4540, train loss: 0.564398, test loss: 0.584097, train acc: 0.737952, test acc: 0.745763\n",
      "4541, train loss: 0.564389, test loss: 0.584087, train acc: 0.737952, test acc: 0.745763\n",
      "4542, train loss: 0.56438, test loss: 0.584076, train acc: 0.737952, test acc: 0.745763\n",
      "4543, train loss: 0.564372, test loss: 0.584065, train acc: 0.737952, test acc: 0.745763\n",
      "4544, train loss: 0.564363, test loss: 0.584055, train acc: 0.737952, test acc: 0.745763\n",
      "4545, train loss: 0.564355, test loss: 0.584045, train acc: 0.737952, test acc: 0.745763\n",
      "4546, train loss: 0.564346, test loss: 0.584034, train acc: 0.737952, test acc: 0.745763\n",
      "4547, train loss: 0.564338, test loss: 0.584023, train acc: 0.737952, test acc: 0.745763\n",
      "4548, train loss: 0.564329, test loss: 0.584013, train acc: 0.737952, test acc: 0.745763\n",
      "4549, train loss: 0.564321, test loss: 0.584002, train acc: 0.737952, test acc: 0.745763\n",
      "4550, train loss: 0.564312, test loss: 0.583992, train acc: 0.737952, test acc: 0.745763\n",
      "4551, train loss: 0.564303, test loss: 0.583981, train acc: 0.737952, test acc: 0.745763\n",
      "4552, train loss: 0.564295, test loss: 0.583971, train acc: 0.737952, test acc: 0.745763\n",
      "4553, train loss: 0.564286, test loss: 0.58396, train acc: 0.737952, test acc: 0.745763\n",
      "4554, train loss: 0.564278, test loss: 0.58395, train acc: 0.737952, test acc: 0.745763\n",
      "4555, train loss: 0.564269, test loss: 0.583939, train acc: 0.737952, test acc: 0.745763\n",
      "4556, train loss: 0.564261, test loss: 0.583929, train acc: 0.737952, test acc: 0.745763\n",
      "4557, train loss: 0.564252, test loss: 0.583918, train acc: 0.737952, test acc: 0.745763\n",
      "4558, train loss: 0.564244, test loss: 0.583908, train acc: 0.737952, test acc: 0.745763\n",
      "4559, train loss: 0.564235, test loss: 0.583897, train acc: 0.737952, test acc: 0.745763\n",
      "4560, train loss: 0.564227, test loss: 0.583887, train acc: 0.737952, test acc: 0.745763\n",
      "4561, train loss: 0.564218, test loss: 0.583876, train acc: 0.737952, test acc: 0.745763\n",
      "4562, train loss: 0.56421, test loss: 0.583866, train acc: 0.737952, test acc: 0.745763\n",
      "4563, train loss: 0.564201, test loss: 0.583855, train acc: 0.737952, test acc: 0.745763\n",
      "4564, train loss: 0.564192, test loss: 0.583845, train acc: 0.737952, test acc: 0.745763\n",
      "4565, train loss: 0.564184, test loss: 0.583834, train acc: 0.737952, test acc: 0.745763\n",
      "4566, train loss: 0.564175, test loss: 0.583824, train acc: 0.737952, test acc: 0.745763\n",
      "4567, train loss: 0.564167, test loss: 0.583813, train acc: 0.737952, test acc: 0.745763\n",
      "4568, train loss: 0.564158, test loss: 0.583803, train acc: 0.737952, test acc: 0.745763\n",
      "4569, train loss: 0.56415, test loss: 0.583792, train acc: 0.737952, test acc: 0.745763\n",
      "4570, train loss: 0.564141, test loss: 0.583782, train acc: 0.737952, test acc: 0.745763\n",
      "4571, train loss: 0.564133, test loss: 0.583771, train acc: 0.737952, test acc: 0.745763\n",
      "4572, train loss: 0.564124, test loss: 0.583761, train acc: 0.737952, test acc: 0.745763\n",
      "4573, train loss: 0.564116, test loss: 0.58375, train acc: 0.737952, test acc: 0.745763\n",
      "4574, train loss: 0.564107, test loss: 0.58374, train acc: 0.737952, test acc: 0.745763\n",
      "4575, train loss: 0.564099, test loss: 0.583729, train acc: 0.737952, test acc: 0.745763\n",
      "4576, train loss: 0.56409, test loss: 0.583719, train acc: 0.737952, test acc: 0.745763\n",
      "4577, train loss: 0.564082, test loss: 0.583708, train acc: 0.737952, test acc: 0.745763\n",
      "4578, train loss: 0.564073, test loss: 0.583698, train acc: 0.737952, test acc: 0.745763\n",
      "4579, train loss: 0.564065, test loss: 0.583687, train acc: 0.737952, test acc: 0.745763\n",
      "4580, train loss: 0.564056, test loss: 0.583677, train acc: 0.737952, test acc: 0.745763\n",
      "4581, train loss: 0.564048, test loss: 0.583666, train acc: 0.737952, test acc: 0.745763\n",
      "4582, train loss: 0.564039, test loss: 0.583656, train acc: 0.737952, test acc: 0.745763\n",
      "4583, train loss: 0.564031, test loss: 0.583645, train acc: 0.737952, test acc: 0.745763\n",
      "4584, train loss: 0.564022, test loss: 0.583635, train acc: 0.737952, test acc: 0.745763\n",
      "4585, train loss: 0.564014, test loss: 0.583624, train acc: 0.737952, test acc: 0.745763\n",
      "4586, train loss: 0.564005, test loss: 0.583614, train acc: 0.737952, test acc: 0.745763\n",
      "4587, train loss: 0.563997, test loss: 0.583603, train acc: 0.737952, test acc: 0.745763\n",
      "4588, train loss: 0.563988, test loss: 0.583593, train acc: 0.737952, test acc: 0.745763\n",
      "4589, train loss: 0.56398, test loss: 0.583582, train acc: 0.737952, test acc: 0.745763\n",
      "4590, train loss: 0.563971, test loss: 0.583572, train acc: 0.737952, test acc: 0.745763\n",
      "4591, train loss: 0.563963, test loss: 0.583562, train acc: 0.737952, test acc: 0.745763\n",
      "4592, train loss: 0.563954, test loss: 0.583551, train acc: 0.737952, test acc: 0.745763\n",
      "4593, train loss: 0.563946, test loss: 0.58354, train acc: 0.737952, test acc: 0.745763\n",
      "4594, train loss: 0.563937, test loss: 0.58353, train acc: 0.737952, test acc: 0.745763\n",
      "4595, train loss: 0.563929, test loss: 0.58352, train acc: 0.737952, test acc: 0.745763\n",
      "4596, train loss: 0.56392, test loss: 0.583509, train acc: 0.737952, test acc: 0.745763\n",
      "4597, train loss: 0.563912, test loss: 0.583499, train acc: 0.737952, test acc: 0.745763\n",
      "4598, train loss: 0.563903, test loss: 0.583488, train acc: 0.737952, test acc: 0.745763\n",
      "4599, train loss: 0.563895, test loss: 0.583478, train acc: 0.737952, test acc: 0.745763\n",
      "4600, train loss: 0.563886, test loss: 0.583467, train acc: 0.737952, test acc: 0.745763\n",
      "4601, train loss: 0.563878, test loss: 0.583457, train acc: 0.737952, test acc: 0.745763\n",
      "4602, train loss: 0.563869, test loss: 0.583446, train acc: 0.737952, test acc: 0.745763\n",
      "4603, train loss: 0.563861, test loss: 0.583436, train acc: 0.737952, test acc: 0.745763\n",
      "4604, train loss: 0.563852, test loss: 0.583425, train acc: 0.737952, test acc: 0.745763\n",
      "4605, train loss: 0.563844, test loss: 0.583415, train acc: 0.737952, test acc: 0.745763\n",
      "4606, train loss: 0.563835, test loss: 0.583405, train acc: 0.737952, test acc: 0.745763\n",
      "4607, train loss: 0.563827, test loss: 0.583394, train acc: 0.737952, test acc: 0.745763\n",
      "4608, train loss: 0.563819, test loss: 0.583384, train acc: 0.737952, test acc: 0.745763\n",
      "4609, train loss: 0.56381, test loss: 0.583373, train acc: 0.737952, test acc: 0.745763\n",
      "4610, train loss: 0.563802, test loss: 0.583363, train acc: 0.737952, test acc: 0.745763\n",
      "4611, train loss: 0.563793, test loss: 0.583352, train acc: 0.737952, test acc: 0.745763\n",
      "4612, train loss: 0.563785, test loss: 0.583342, train acc: 0.737952, test acc: 0.745763\n",
      "4613, train loss: 0.563776, test loss: 0.583331, train acc: 0.740964, test acc: 0.745763\n",
      "4614, train loss: 0.563768, test loss: 0.583321, train acc: 0.740964, test acc: 0.745763\n",
      "4615, train loss: 0.563759, test loss: 0.58331, train acc: 0.740964, test acc: 0.745763\n",
      "4616, train loss: 0.563751, test loss: 0.5833, train acc: 0.740964, test acc: 0.745763\n",
      "4617, train loss: 0.563742, test loss: 0.58329, train acc: 0.740964, test acc: 0.745763\n",
      "4618, train loss: 0.563734, test loss: 0.583279, train acc: 0.740964, test acc: 0.745763\n",
      "4619, train loss: 0.563725, test loss: 0.583269, train acc: 0.740964, test acc: 0.745763\n",
      "4620, train loss: 0.563717, test loss: 0.583258, train acc: 0.740964, test acc: 0.745763\n",
      "4621, train loss: 0.563708, test loss: 0.583248, train acc: 0.740964, test acc: 0.745763\n",
      "4622, train loss: 0.5637, test loss: 0.583237, train acc: 0.740964, test acc: 0.745763\n",
      "4623, train loss: 0.563691, test loss: 0.583227, train acc: 0.740964, test acc: 0.745763\n",
      "4624, train loss: 0.563683, test loss: 0.583216, train acc: 0.740964, test acc: 0.745763\n",
      "4625, train loss: 0.563675, test loss: 0.583206, train acc: 0.740964, test acc: 0.745763\n",
      "4626, train loss: 0.563666, test loss: 0.583196, train acc: 0.740964, test acc: 0.745763\n",
      "4627, train loss: 0.563658, test loss: 0.583185, train acc: 0.740964, test acc: 0.745763\n",
      "4628, train loss: 0.563649, test loss: 0.583175, train acc: 0.740964, test acc: 0.745763\n",
      "4629, train loss: 0.563641, test loss: 0.583164, train acc: 0.740964, test acc: 0.745763\n",
      "4630, train loss: 0.563632, test loss: 0.583154, train acc: 0.740964, test acc: 0.745763\n",
      "4631, train loss: 0.563624, test loss: 0.583143, train acc: 0.740964, test acc: 0.745763\n",
      "4632, train loss: 0.563615, test loss: 0.583133, train acc: 0.740964, test acc: 0.745763\n",
      "4633, train loss: 0.563607, test loss: 0.583123, train acc: 0.740964, test acc: 0.745763\n",
      "4634, train loss: 0.563599, test loss: 0.583112, train acc: 0.740964, test acc: 0.745763\n",
      "4635, train loss: 0.56359, test loss: 0.583102, train acc: 0.740964, test acc: 0.745763\n",
      "4636, train loss: 0.563582, test loss: 0.583091, train acc: 0.740964, test acc: 0.745763\n",
      "4637, train loss: 0.563573, test loss: 0.583081, train acc: 0.740964, test acc: 0.745763\n",
      "4638, train loss: 0.563565, test loss: 0.58307, train acc: 0.740964, test acc: 0.745763\n",
      "4639, train loss: 0.563556, test loss: 0.58306, train acc: 0.740964, test acc: 0.745763\n",
      "4640, train loss: 0.563548, test loss: 0.58305, train acc: 0.740964, test acc: 0.745763\n",
      "4641, train loss: 0.56354, test loss: 0.583039, train acc: 0.740964, test acc: 0.745763\n",
      "4642, train loss: 0.563531, test loss: 0.583029, train acc: 0.740964, test acc: 0.745763\n",
      "4643, train loss: 0.563523, test loss: 0.583018, train acc: 0.740964, test acc: 0.745763\n",
      "4644, train loss: 0.563514, test loss: 0.583008, train acc: 0.740964, test acc: 0.745763\n",
      "4645, train loss: 0.563506, test loss: 0.582998, train acc: 0.740964, test acc: 0.745763\n",
      "4646, train loss: 0.563497, test loss: 0.582987, train acc: 0.740964, test acc: 0.745763\n",
      "4647, train loss: 0.563489, test loss: 0.582977, train acc: 0.740964, test acc: 0.745763\n",
      "4648, train loss: 0.56348, test loss: 0.582966, train acc: 0.740964, test acc: 0.745763\n",
      "4649, train loss: 0.563472, test loss: 0.582956, train acc: 0.740964, test acc: 0.745763\n",
      "4650, train loss: 0.563464, test loss: 0.582945, train acc: 0.740964, test acc: 0.745763\n",
      "4651, train loss: 0.563455, test loss: 0.582935, train acc: 0.740964, test acc: 0.745763\n",
      "4652, train loss: 0.563447, test loss: 0.582925, train acc: 0.740964, test acc: 0.745763\n",
      "4653, train loss: 0.563438, test loss: 0.582914, train acc: 0.740964, test acc: 0.745763\n",
      "4654, train loss: 0.56343, test loss: 0.582904, train acc: 0.740964, test acc: 0.745763\n",
      "4655, train loss: 0.563421, test loss: 0.582893, train acc: 0.740964, test acc: 0.745763\n",
      "4656, train loss: 0.563413, test loss: 0.582883, train acc: 0.740964, test acc: 0.745763\n",
      "4657, train loss: 0.563405, test loss: 0.582873, train acc: 0.740964, test acc: 0.745763\n",
      "4658, train loss: 0.563396, test loss: 0.582862, train acc: 0.740964, test acc: 0.745763\n",
      "4659, train loss: 0.563388, test loss: 0.582852, train acc: 0.740964, test acc: 0.745763\n",
      "4660, train loss: 0.563379, test loss: 0.582841, train acc: 0.740964, test acc: 0.745763\n",
      "4661, train loss: 0.563371, test loss: 0.582831, train acc: 0.740964, test acc: 0.745763\n",
      "4662, train loss: 0.563363, test loss: 0.582821, train acc: 0.740964, test acc: 0.745763\n",
      "4663, train loss: 0.563354, test loss: 0.58281, train acc: 0.740964, test acc: 0.745763\n",
      "4664, train loss: 0.563346, test loss: 0.5828, train acc: 0.740964, test acc: 0.745763\n",
      "4665, train loss: 0.563337, test loss: 0.582789, train acc: 0.740964, test acc: 0.745763\n",
      "4666, train loss: 0.563329, test loss: 0.582779, train acc: 0.740964, test acc: 0.745763\n",
      "4667, train loss: 0.563321, test loss: 0.582769, train acc: 0.740964, test acc: 0.745763\n",
      "4668, train loss: 0.563312, test loss: 0.582758, train acc: 0.740964, test acc: 0.745763\n",
      "4669, train loss: 0.563304, test loss: 0.582748, train acc: 0.740964, test acc: 0.745763\n",
      "4670, train loss: 0.563295, test loss: 0.582738, train acc: 0.740964, test acc: 0.745763\n",
      "4671, train loss: 0.563287, test loss: 0.582727, train acc: 0.740964, test acc: 0.745763\n",
      "4672, train loss: 0.563278, test loss: 0.582717, train acc: 0.740964, test acc: 0.745763\n",
      "4673, train loss: 0.56327, test loss: 0.582706, train acc: 0.740964, test acc: 0.745763\n",
      "4674, train loss: 0.563262, test loss: 0.582696, train acc: 0.740964, test acc: 0.745763\n",
      "4675, train loss: 0.563253, test loss: 0.582686, train acc: 0.740964, test acc: 0.745763\n",
      "4676, train loss: 0.563245, test loss: 0.582675, train acc: 0.740964, test acc: 0.745763\n",
      "4677, train loss: 0.563237, test loss: 0.582665, train acc: 0.740964, test acc: 0.745763\n",
      "4678, train loss: 0.563228, test loss: 0.582654, train acc: 0.740964, test acc: 0.745763\n",
      "4679, train loss: 0.56322, test loss: 0.582644, train acc: 0.740964, test acc: 0.745763\n",
      "4680, train loss: 0.563211, test loss: 0.582634, train acc: 0.740964, test acc: 0.745763\n",
      "4681, train loss: 0.563203, test loss: 0.582623, train acc: 0.740964, test acc: 0.745763\n",
      "4682, train loss: 0.563195, test loss: 0.582613, train acc: 0.740964, test acc: 0.745763\n",
      "4683, train loss: 0.563186, test loss: 0.582603, train acc: 0.740964, test acc: 0.745763\n",
      "4684, train loss: 0.563178, test loss: 0.582592, train acc: 0.740964, test acc: 0.745763\n",
      "4685, train loss: 0.563169, test loss: 0.582582, train acc: 0.740964, test acc: 0.745763\n",
      "4686, train loss: 0.563161, test loss: 0.582571, train acc: 0.740964, test acc: 0.745763\n",
      "4687, train loss: 0.563153, test loss: 0.582561, train acc: 0.740964, test acc: 0.745763\n",
      "4688, train loss: 0.563144, test loss: 0.582551, train acc: 0.740964, test acc: 0.745763\n",
      "4689, train loss: 0.563136, test loss: 0.58254, train acc: 0.740964, test acc: 0.745763\n",
      "4690, train loss: 0.563128, test loss: 0.58253, train acc: 0.740964, test acc: 0.745763\n",
      "4691, train loss: 0.563119, test loss: 0.58252, train acc: 0.740964, test acc: 0.745763\n",
      "4692, train loss: 0.563111, test loss: 0.582509, train acc: 0.740964, test acc: 0.745763\n",
      "4693, train loss: 0.563102, test loss: 0.582499, train acc: 0.740964, test acc: 0.745763\n",
      "4694, train loss: 0.563094, test loss: 0.582489, train acc: 0.740964, test acc: 0.745763\n",
      "4695, train loss: 0.563086, test loss: 0.582478, train acc: 0.740964, test acc: 0.745763\n",
      "4696, train loss: 0.563077, test loss: 0.582468, train acc: 0.740964, test acc: 0.745763\n",
      "4697, train loss: 0.563069, test loss: 0.582457, train acc: 0.740964, test acc: 0.745763\n",
      "4698, train loss: 0.56306, test loss: 0.582447, train acc: 0.740964, test acc: 0.745763\n",
      "4699, train loss: 0.563052, test loss: 0.582437, train acc: 0.740964, test acc: 0.745763\n",
      "4700, train loss: 0.563044, test loss: 0.582426, train acc: 0.740964, test acc: 0.745763\n",
      "4701, train loss: 0.563035, test loss: 0.582416, train acc: 0.740964, test acc: 0.745763\n",
      "4702, train loss: 0.563027, test loss: 0.582406, train acc: 0.740964, test acc: 0.745763\n",
      "4703, train loss: 0.563019, test loss: 0.582395, train acc: 0.740964, test acc: 0.745763\n",
      "4704, train loss: 0.56301, test loss: 0.582385, train acc: 0.740964, test acc: 0.745763\n",
      "4705, train loss: 0.563002, test loss: 0.582375, train acc: 0.740964, test acc: 0.745763\n",
      "4706, train loss: 0.562994, test loss: 0.582364, train acc: 0.740964, test acc: 0.745763\n",
      "4707, train loss: 0.562985, test loss: 0.582354, train acc: 0.740964, test acc: 0.745763\n",
      "4708, train loss: 0.562977, test loss: 0.582344, train acc: 0.740964, test acc: 0.745763\n",
      "4709, train loss: 0.562968, test loss: 0.582333, train acc: 0.740964, test acc: 0.745763\n",
      "4710, train loss: 0.56296, test loss: 0.582323, train acc: 0.740964, test acc: 0.745763\n",
      "4711, train loss: 0.562952, test loss: 0.582313, train acc: 0.740964, test acc: 0.745763\n",
      "4712, train loss: 0.562943, test loss: 0.582302, train acc: 0.740964, test acc: 0.745763\n",
      "4713, train loss: 0.562935, test loss: 0.582292, train acc: 0.740964, test acc: 0.745763\n",
      "4714, train loss: 0.562927, test loss: 0.582282, train acc: 0.740964, test acc: 0.745763\n",
      "4715, train loss: 0.562918, test loss: 0.582271, train acc: 0.740964, test acc: 0.745763\n",
      "4716, train loss: 0.56291, test loss: 0.582261, train acc: 0.740964, test acc: 0.745763\n",
      "4717, train loss: 0.562902, test loss: 0.582251, train acc: 0.740964, test acc: 0.745763\n",
      "4718, train loss: 0.562893, test loss: 0.58224, train acc: 0.740964, test acc: 0.745763\n",
      "4719, train loss: 0.562885, test loss: 0.58223, train acc: 0.740964, test acc: 0.745763\n",
      "4720, train loss: 0.562877, test loss: 0.58222, train acc: 0.740964, test acc: 0.745763\n",
      "4721, train loss: 0.562868, test loss: 0.582209, train acc: 0.740964, test acc: 0.745763\n",
      "4722, train loss: 0.56286, test loss: 0.582199, train acc: 0.740964, test acc: 0.745763\n",
      "4723, train loss: 0.562852, test loss: 0.582188, train acc: 0.740964, test acc: 0.745763\n",
      "4724, train loss: 0.562843, test loss: 0.582178, train acc: 0.740964, test acc: 0.745763\n",
      "4725, train loss: 0.562835, test loss: 0.582168, train acc: 0.740964, test acc: 0.745763\n",
      "4726, train loss: 0.562827, test loss: 0.582158, train acc: 0.740964, test acc: 0.745763\n",
      "4727, train loss: 0.562818, test loss: 0.582147, train acc: 0.740964, test acc: 0.745763\n",
      "4728, train loss: 0.56281, test loss: 0.582137, train acc: 0.740964, test acc: 0.745763\n",
      "4729, train loss: 0.562801, test loss: 0.582127, train acc: 0.740964, test acc: 0.745763\n",
      "4730, train loss: 0.562793, test loss: 0.582116, train acc: 0.740964, test acc: 0.745763\n",
      "4731, train loss: 0.562785, test loss: 0.582106, train acc: 0.740964, test acc: 0.745763\n",
      "4732, train loss: 0.562777, test loss: 0.582096, train acc: 0.740964, test acc: 0.745763\n",
      "4733, train loss: 0.562768, test loss: 0.582085, train acc: 0.740964, test acc: 0.745763\n",
      "4734, train loss: 0.56276, test loss: 0.582075, train acc: 0.740964, test acc: 0.745763\n",
      "4735, train loss: 0.562751, test loss: 0.582065, train acc: 0.740964, test acc: 0.745763\n",
      "4736, train loss: 0.562743, test loss: 0.582054, train acc: 0.740964, test acc: 0.745763\n",
      "4737, train loss: 0.562735, test loss: 0.582044, train acc: 0.740964, test acc: 0.745763\n",
      "4738, train loss: 0.562727, test loss: 0.582034, train acc: 0.740964, test acc: 0.745763\n",
      "4739, train loss: 0.562718, test loss: 0.582023, train acc: 0.740964, test acc: 0.745763\n",
      "4740, train loss: 0.56271, test loss: 0.582013, train acc: 0.740964, test acc: 0.745763\n",
      "4741, train loss: 0.562702, test loss: 0.582003, train acc: 0.740964, test acc: 0.745763\n",
      "4742, train loss: 0.562693, test loss: 0.581992, train acc: 0.740964, test acc: 0.745763\n",
      "4743, train loss: 0.562685, test loss: 0.581982, train acc: 0.740964, test acc: 0.745763\n",
      "4744, train loss: 0.562677, test loss: 0.581972, train acc: 0.740964, test acc: 0.745763\n",
      "4745, train loss: 0.562668, test loss: 0.581962, train acc: 0.740964, test acc: 0.745763\n",
      "4746, train loss: 0.56266, test loss: 0.581951, train acc: 0.740964, test acc: 0.745763\n",
      "4747, train loss: 0.562652, test loss: 0.581941, train acc: 0.740964, test acc: 0.745763\n",
      "4748, train loss: 0.562643, test loss: 0.581931, train acc: 0.740964, test acc: 0.745763\n",
      "4749, train loss: 0.562635, test loss: 0.58192, train acc: 0.740964, test acc: 0.745763\n",
      "4750, train loss: 0.562627, test loss: 0.58191, train acc: 0.740964, test acc: 0.745763\n",
      "4751, train loss: 0.562618, test loss: 0.5819, train acc: 0.740964, test acc: 0.745763\n",
      "4752, train loss: 0.56261, test loss: 0.58189, train acc: 0.740964, test acc: 0.745763\n",
      "4753, train loss: 0.562602, test loss: 0.581879, train acc: 0.740964, test acc: 0.745763\n",
      "4754, train loss: 0.562593, test loss: 0.581869, train acc: 0.740964, test acc: 0.745763\n",
      "4755, train loss: 0.562585, test loss: 0.581859, train acc: 0.740964, test acc: 0.745763\n",
      "4756, train loss: 0.562577, test loss: 0.581848, train acc: 0.740964, test acc: 0.745763\n",
      "4757, train loss: 0.562569, test loss: 0.581838, train acc: 0.740964, test acc: 0.745763\n",
      "4758, train loss: 0.56256, test loss: 0.581828, train acc: 0.740964, test acc: 0.745763\n",
      "4759, train loss: 0.562552, test loss: 0.581817, train acc: 0.740964, test acc: 0.745763\n",
      "4760, train loss: 0.562544, test loss: 0.581807, train acc: 0.740964, test acc: 0.745763\n",
      "4761, train loss: 0.562535, test loss: 0.581797, train acc: 0.740964, test acc: 0.745763\n",
      "4762, train loss: 0.562527, test loss: 0.581787, train acc: 0.740964, test acc: 0.745763\n",
      "4763, train loss: 0.562519, test loss: 0.581776, train acc: 0.740964, test acc: 0.745763\n",
      "4764, train loss: 0.56251, test loss: 0.581766, train acc: 0.740964, test acc: 0.745763\n",
      "4765, train loss: 0.562502, test loss: 0.581756, train acc: 0.740964, test acc: 0.745763\n",
      "4766, train loss: 0.562494, test loss: 0.581745, train acc: 0.740964, test acc: 0.745763\n",
      "4767, train loss: 0.562486, test loss: 0.581735, train acc: 0.740964, test acc: 0.745763\n",
      "4768, train loss: 0.562477, test loss: 0.581725, train acc: 0.740964, test acc: 0.745763\n",
      "4769, train loss: 0.562469, test loss: 0.581715, train acc: 0.740964, test acc: 0.745763\n",
      "4770, train loss: 0.562461, test loss: 0.581704, train acc: 0.740964, test acc: 0.745763\n",
      "4771, train loss: 0.562452, test loss: 0.581694, train acc: 0.740964, test acc: 0.745763\n",
      "4772, train loss: 0.562444, test loss: 0.581684, train acc: 0.740964, test acc: 0.745763\n",
      "4773, train loss: 0.562436, test loss: 0.581673, train acc: 0.740964, test acc: 0.745763\n",
      "4774, train loss: 0.562427, test loss: 0.581663, train acc: 0.740964, test acc: 0.745763\n",
      "4775, train loss: 0.562419, test loss: 0.581653, train acc: 0.740964, test acc: 0.745763\n",
      "4776, train loss: 0.562411, test loss: 0.581643, train acc: 0.740964, test acc: 0.745763\n",
      "4777, train loss: 0.562403, test loss: 0.581632, train acc: 0.740964, test acc: 0.745763\n",
      "4778, train loss: 0.562394, test loss: 0.581622, train acc: 0.740964, test acc: 0.745763\n",
      "4779, train loss: 0.562386, test loss: 0.581612, train acc: 0.740964, test acc: 0.745763\n",
      "4780, train loss: 0.562378, test loss: 0.581602, train acc: 0.740964, test acc: 0.745763\n",
      "4781, train loss: 0.56237, test loss: 0.581591, train acc: 0.740964, test acc: 0.745763\n",
      "4782, train loss: 0.562361, test loss: 0.581581, train acc: 0.740964, test acc: 0.745763\n",
      "4783, train loss: 0.562353, test loss: 0.581571, train acc: 0.740964, test acc: 0.745763\n",
      "4784, train loss: 0.562345, test loss: 0.58156, train acc: 0.740964, test acc: 0.745763\n",
      "4785, train loss: 0.562336, test loss: 0.58155, train acc: 0.740964, test acc: 0.745763\n",
      "4786, train loss: 0.562328, test loss: 0.58154, train acc: 0.740964, test acc: 0.745763\n",
      "4787, train loss: 0.56232, test loss: 0.58153, train acc: 0.740964, test acc: 0.745763\n",
      "4788, train loss: 0.562312, test loss: 0.581519, train acc: 0.740964, test acc: 0.745763\n",
      "4789, train loss: 0.562303, test loss: 0.581509, train acc: 0.740964, test acc: 0.745763\n",
      "4790, train loss: 0.562295, test loss: 0.581499, train acc: 0.740964, test acc: 0.745763\n",
      "4791, train loss: 0.562287, test loss: 0.581489, train acc: 0.740964, test acc: 0.745763\n",
      "4792, train loss: 0.562279, test loss: 0.581478, train acc: 0.740964, test acc: 0.745763\n",
      "4793, train loss: 0.56227, test loss: 0.581468, train acc: 0.740964, test acc: 0.745763\n",
      "4794, train loss: 0.562262, test loss: 0.581458, train acc: 0.740964, test acc: 0.745763\n",
      "4795, train loss: 0.562254, test loss: 0.581448, train acc: 0.740964, test acc: 0.745763\n",
      "4796, train loss: 0.562246, test loss: 0.581438, train acc: 0.740964, test acc: 0.745763\n",
      "4797, train loss: 0.562237, test loss: 0.581427, train acc: 0.740964, test acc: 0.745763\n",
      "4798, train loss: 0.562229, test loss: 0.581417, train acc: 0.740964, test acc: 0.745763\n",
      "4799, train loss: 0.562221, test loss: 0.581407, train acc: 0.740964, test acc: 0.745763\n",
      "4800, train loss: 0.562212, test loss: 0.581397, train acc: 0.740964, test acc: 0.745763\n",
      "4801, train loss: 0.562204, test loss: 0.581386, train acc: 0.740964, test acc: 0.745763\n",
      "4802, train loss: 0.562196, test loss: 0.581376, train acc: 0.740964, test acc: 0.745763\n",
      "4803, train loss: 0.562188, test loss: 0.581366, train acc: 0.740964, test acc: 0.745763\n",
      "4804, train loss: 0.562179, test loss: 0.581355, train acc: 0.740964, test acc: 0.745763\n",
      "4805, train loss: 0.562171, test loss: 0.581345, train acc: 0.740964, test acc: 0.745763\n",
      "4806, train loss: 0.562163, test loss: 0.581335, train acc: 0.740964, test acc: 0.745763\n",
      "4807, train loss: 0.562155, test loss: 0.581325, train acc: 0.740964, test acc: 0.745763\n",
      "4808, train loss: 0.562146, test loss: 0.581315, train acc: 0.740964, test acc: 0.745763\n",
      "4809, train loss: 0.562138, test loss: 0.581304, train acc: 0.740964, test acc: 0.745763\n",
      "4810, train loss: 0.56213, test loss: 0.581294, train acc: 0.740964, test acc: 0.745763\n",
      "4811, train loss: 0.562122, test loss: 0.581284, train acc: 0.740964, test acc: 0.745763\n",
      "4812, train loss: 0.562113, test loss: 0.581274, train acc: 0.740964, test acc: 0.745763\n",
      "4813, train loss: 0.562105, test loss: 0.581263, train acc: 0.740964, test acc: 0.745763\n",
      "4814, train loss: 0.562097, test loss: 0.581253, train acc: 0.740964, test acc: 0.745763\n",
      "4815, train loss: 0.562089, test loss: 0.581243, train acc: 0.740964, test acc: 0.745763\n",
      "4816, train loss: 0.562081, test loss: 0.581233, train acc: 0.740964, test acc: 0.745763\n",
      "4817, train loss: 0.562072, test loss: 0.581222, train acc: 0.740964, test acc: 0.745763\n",
      "4818, train loss: 0.562064, test loss: 0.581212, train acc: 0.740964, test acc: 0.745763\n",
      "4819, train loss: 0.562056, test loss: 0.581202, train acc: 0.740964, test acc: 0.745763\n",
      "4820, train loss: 0.562048, test loss: 0.581192, train acc: 0.740964, test acc: 0.745763\n",
      "4821, train loss: 0.562039, test loss: 0.581182, train acc: 0.740964, test acc: 0.745763\n",
      "4822, train loss: 0.562031, test loss: 0.581171, train acc: 0.740964, test acc: 0.745763\n",
      "4823, train loss: 0.562023, test loss: 0.581161, train acc: 0.740964, test acc: 0.745763\n",
      "4824, train loss: 0.562015, test loss: 0.581151, train acc: 0.740964, test acc: 0.745763\n",
      "4825, train loss: 0.562006, test loss: 0.581141, train acc: 0.740964, test acc: 0.745763\n",
      "4826, train loss: 0.561998, test loss: 0.58113, train acc: 0.740964, test acc: 0.745763\n",
      "4827, train loss: 0.56199, test loss: 0.58112, train acc: 0.740964, test acc: 0.745763\n",
      "4828, train loss: 0.561982, test loss: 0.58111, train acc: 0.740964, test acc: 0.745763\n",
      "4829, train loss: 0.561973, test loss: 0.5811, train acc: 0.740964, test acc: 0.745763\n",
      "4830, train loss: 0.561965, test loss: 0.58109, train acc: 0.740964, test acc: 0.745763\n",
      "4831, train loss: 0.561957, test loss: 0.581079, train acc: 0.740964, test acc: 0.745763\n",
      "4832, train loss: 0.561949, test loss: 0.581069, train acc: 0.740964, test acc: 0.745763\n",
      "4833, train loss: 0.561941, test loss: 0.581059, train acc: 0.740964, test acc: 0.745763\n",
      "4834, train loss: 0.561932, test loss: 0.581049, train acc: 0.740964, test acc: 0.745763\n",
      "4835, train loss: 0.561924, test loss: 0.581039, train acc: 0.740964, test acc: 0.745763\n",
      "4836, train loss: 0.561916, test loss: 0.581028, train acc: 0.740964, test acc: 0.745763\n",
      "4837, train loss: 0.561908, test loss: 0.581018, train acc: 0.740964, test acc: 0.745763\n",
      "4838, train loss: 0.561899, test loss: 0.581008, train acc: 0.740964, test acc: 0.745763\n",
      "4839, train loss: 0.561891, test loss: 0.580998, train acc: 0.740964, test acc: 0.745763\n",
      "4840, train loss: 0.561883, test loss: 0.580988, train acc: 0.740964, test acc: 0.745763\n",
      "4841, train loss: 0.561875, test loss: 0.580977, train acc: 0.740964, test acc: 0.745763\n",
      "4842, train loss: 0.561867, test loss: 0.580967, train acc: 0.740964, test acc: 0.745763\n",
      "4843, train loss: 0.561858, test loss: 0.580957, train acc: 0.740964, test acc: 0.745763\n",
      "4844, train loss: 0.56185, test loss: 0.580947, train acc: 0.740964, test acc: 0.745763\n",
      "4845, train loss: 0.561842, test loss: 0.580936, train acc: 0.740964, test acc: 0.745763\n",
      "4846, train loss: 0.561834, test loss: 0.580926, train acc: 0.740964, test acc: 0.745763\n",
      "4847, train loss: 0.561826, test loss: 0.580916, train acc: 0.740964, test acc: 0.745763\n",
      "4848, train loss: 0.561817, test loss: 0.580906, train acc: 0.740964, test acc: 0.745763\n",
      "4849, train loss: 0.561809, test loss: 0.580896, train acc: 0.740964, test acc: 0.745763\n",
      "4850, train loss: 0.561801, test loss: 0.580885, train acc: 0.740964, test acc: 0.745763\n",
      "4851, train loss: 0.561793, test loss: 0.580875, train acc: 0.740964, test acc: 0.745763\n",
      "4852, train loss: 0.561785, test loss: 0.580865, train acc: 0.740964, test acc: 0.745763\n",
      "4853, train loss: 0.561776, test loss: 0.580855, train acc: 0.740964, test acc: 0.745763\n",
      "4854, train loss: 0.561768, test loss: 0.580845, train acc: 0.740964, test acc: 0.745763\n",
      "4855, train loss: 0.56176, test loss: 0.580835, train acc: 0.740964, test acc: 0.745763\n",
      "4856, train loss: 0.561752, test loss: 0.580824, train acc: 0.740964, test acc: 0.745763\n",
      "4857, train loss: 0.561744, test loss: 0.580814, train acc: 0.740964, test acc: 0.745763\n",
      "4858, train loss: 0.561735, test loss: 0.580804, train acc: 0.740964, test acc: 0.745763\n",
      "4859, train loss: 0.561727, test loss: 0.580794, train acc: 0.740964, test acc: 0.745763\n",
      "4860, train loss: 0.561719, test loss: 0.580784, train acc: 0.740964, test acc: 0.745763\n",
      "4861, train loss: 0.561711, test loss: 0.580773, train acc: 0.740964, test acc: 0.745763\n",
      "4862, train loss: 0.561703, test loss: 0.580763, train acc: 0.740964, test acc: 0.745763\n",
      "4863, train loss: 0.561694, test loss: 0.580753, train acc: 0.740964, test acc: 0.745763\n",
      "4864, train loss: 0.561686, test loss: 0.580743, train acc: 0.740964, test acc: 0.745763\n",
      "4865, train loss: 0.561678, test loss: 0.580733, train acc: 0.740964, test acc: 0.745763\n",
      "4866, train loss: 0.56167, test loss: 0.580723, train acc: 0.740964, test acc: 0.745763\n",
      "4867, train loss: 0.561662, test loss: 0.580712, train acc: 0.740964, test acc: 0.745763\n",
      "4868, train loss: 0.561653, test loss: 0.580702, train acc: 0.740964, test acc: 0.745763\n",
      "4869, train loss: 0.561645, test loss: 0.580692, train acc: 0.740964, test acc: 0.745763\n",
      "4870, train loss: 0.561637, test loss: 0.580682, train acc: 0.740964, test acc: 0.745763\n",
      "4871, train loss: 0.561629, test loss: 0.580672, train acc: 0.740964, test acc: 0.745763\n",
      "4872, train loss: 0.561621, test loss: 0.580662, train acc: 0.740964, test acc: 0.745763\n",
      "4873, train loss: 0.561613, test loss: 0.580651, train acc: 0.740964, test acc: 0.745763\n",
      "4874, train loss: 0.561604, test loss: 0.580641, train acc: 0.740964, test acc: 0.745763\n",
      "4875, train loss: 0.561596, test loss: 0.580631, train acc: 0.740964, test acc: 0.745763\n",
      "4876, train loss: 0.561588, test loss: 0.580621, train acc: 0.740964, test acc: 0.745763\n",
      "4877, train loss: 0.56158, test loss: 0.580611, train acc: 0.740964, test acc: 0.745763\n",
      "4878, train loss: 0.561572, test loss: 0.580601, train acc: 0.740964, test acc: 0.745763\n",
      "4879, train loss: 0.561563, test loss: 0.58059, train acc: 0.740964, test acc: 0.745763\n",
      "4880, train loss: 0.561555, test loss: 0.58058, train acc: 0.740964, test acc: 0.745763\n",
      "4881, train loss: 0.561547, test loss: 0.58057, train acc: 0.740964, test acc: 0.745763\n",
      "4882, train loss: 0.561539, test loss: 0.58056, train acc: 0.740964, test acc: 0.745763\n",
      "4883, train loss: 0.561531, test loss: 0.58055, train acc: 0.740964, test acc: 0.745763\n",
      "4884, train loss: 0.561523, test loss: 0.58054, train acc: 0.740964, test acc: 0.745763\n",
      "4885, train loss: 0.561514, test loss: 0.580529, train acc: 0.740964, test acc: 0.745763\n",
      "4886, train loss: 0.561506, test loss: 0.580519, train acc: 0.740964, test acc: 0.745763\n",
      "4887, train loss: 0.561498, test loss: 0.580509, train acc: 0.740964, test acc: 0.745763\n",
      "4888, train loss: 0.56149, test loss: 0.580499, train acc: 0.740964, test acc: 0.745763\n",
      "4889, train loss: 0.561482, test loss: 0.580489, train acc: 0.740964, test acc: 0.745763\n",
      "4890, train loss: 0.561474, test loss: 0.580479, train acc: 0.740964, test acc: 0.745763\n",
      "4891, train loss: 0.561466, test loss: 0.580469, train acc: 0.740964, test acc: 0.745763\n",
      "4892, train loss: 0.561457, test loss: 0.580458, train acc: 0.740964, test acc: 0.745763\n",
      "4893, train loss: 0.561449, test loss: 0.580448, train acc: 0.740964, test acc: 0.745763\n",
      "4894, train loss: 0.561441, test loss: 0.580438, train acc: 0.740964, test acc: 0.745763\n",
      "4895, train loss: 0.561433, test loss: 0.580428, train acc: 0.740964, test acc: 0.745763\n",
      "4896, train loss: 0.561425, test loss: 0.580418, train acc: 0.740964, test acc: 0.745763\n",
      "4897, train loss: 0.561417, test loss: 0.580408, train acc: 0.740964, test acc: 0.745763\n",
      "4898, train loss: 0.561409, test loss: 0.580397, train acc: 0.740964, test acc: 0.745763\n",
      "4899, train loss: 0.5614, test loss: 0.580387, train acc: 0.740964, test acc: 0.745763\n",
      "4900, train loss: 0.561392, test loss: 0.580377, train acc: 0.740964, test acc: 0.745763\n",
      "4901, train loss: 0.561384, test loss: 0.580367, train acc: 0.740964, test acc: 0.745763\n",
      "4902, train loss: 0.561376, test loss: 0.580357, train acc: 0.740964, test acc: 0.745763\n",
      "4903, train loss: 0.561368, test loss: 0.580347, train acc: 0.740964, test acc: 0.745763\n",
      "4904, train loss: 0.56136, test loss: 0.580337, train acc: 0.740964, test acc: 0.745763\n",
      "4905, train loss: 0.561351, test loss: 0.580327, train acc: 0.740964, test acc: 0.745763\n",
      "4906, train loss: 0.561343, test loss: 0.580316, train acc: 0.740964, test acc: 0.745763\n",
      "4907, train loss: 0.561335, test loss: 0.580306, train acc: 0.740964, test acc: 0.745763\n",
      "4908, train loss: 0.561327, test loss: 0.580296, train acc: 0.740964, test acc: 0.745763\n",
      "4909, train loss: 0.561319, test loss: 0.580286, train acc: 0.740964, test acc: 0.745763\n",
      "4910, train loss: 0.561311, test loss: 0.580276, train acc: 0.740964, test acc: 0.745763\n",
      "4911, train loss: 0.561303, test loss: 0.580266, train acc: 0.740964, test acc: 0.745763\n",
      "4912, train loss: 0.561294, test loss: 0.580256, train acc: 0.740964, test acc: 0.745763\n",
      "4913, train loss: 0.561286, test loss: 0.580245, train acc: 0.740964, test acc: 0.745763\n",
      "4914, train loss: 0.561278, test loss: 0.580235, train acc: 0.740964, test acc: 0.745763\n",
      "4915, train loss: 0.56127, test loss: 0.580225, train acc: 0.740964, test acc: 0.745763\n",
      "4916, train loss: 0.561262, test loss: 0.580215, train acc: 0.740964, test acc: 0.745763\n",
      "4917, train loss: 0.561254, test loss: 0.580205, train acc: 0.740964, test acc: 0.745763\n",
      "4918, train loss: 0.561246, test loss: 0.580195, train acc: 0.740964, test acc: 0.745763\n",
      "4919, train loss: 0.561237, test loss: 0.580185, train acc: 0.740964, test acc: 0.745763\n",
      "4920, train loss: 0.561229, test loss: 0.580175, train acc: 0.740964, test acc: 0.745763\n",
      "4921, train loss: 0.561221, test loss: 0.580165, train acc: 0.740964, test acc: 0.745763\n",
      "4922, train loss: 0.561213, test loss: 0.580154, train acc: 0.740964, test acc: 0.745763\n",
      "4923, train loss: 0.561205, test loss: 0.580144, train acc: 0.740964, test acc: 0.745763\n",
      "4924, train loss: 0.561197, test loss: 0.580134, train acc: 0.740964, test acc: 0.745763\n",
      "4925, train loss: 0.561189, test loss: 0.580124, train acc: 0.740964, test acc: 0.745763\n",
      "4926, train loss: 0.561181, test loss: 0.580114, train acc: 0.740964, test acc: 0.745763\n",
      "4927, train loss: 0.561173, test loss: 0.580104, train acc: 0.740964, test acc: 0.745763\n",
      "4928, train loss: 0.561164, test loss: 0.580094, train acc: 0.740964, test acc: 0.745763\n",
      "4929, train loss: 0.561156, test loss: 0.580084, train acc: 0.740964, test acc: 0.745763\n",
      "4930, train loss: 0.561148, test loss: 0.580073, train acc: 0.740964, test acc: 0.745763\n",
      "4931, train loss: 0.56114, test loss: 0.580063, train acc: 0.740964, test acc: 0.745763\n",
      "4932, train loss: 0.561132, test loss: 0.580053, train acc: 0.740964, test acc: 0.745763\n",
      "4933, train loss: 0.561124, test loss: 0.580043, train acc: 0.740964, test acc: 0.745763\n",
      "4934, train loss: 0.561116, test loss: 0.580033, train acc: 0.740964, test acc: 0.745763\n",
      "4935, train loss: 0.561108, test loss: 0.580023, train acc: 0.740964, test acc: 0.745763\n",
      "4936, train loss: 0.5611, test loss: 0.580013, train acc: 0.740964, test acc: 0.745763\n",
      "4937, train loss: 0.561091, test loss: 0.580003, train acc: 0.740964, test acc: 0.745763\n",
      "4938, train loss: 0.561083, test loss: 0.579993, train acc: 0.740964, test acc: 0.745763\n",
      "4939, train loss: 0.561075, test loss: 0.579983, train acc: 0.740964, test acc: 0.745763\n",
      "4940, train loss: 0.561067, test loss: 0.579973, train acc: 0.740964, test acc: 0.745763\n",
      "4941, train loss: 0.561059, test loss: 0.579962, train acc: 0.740964, test acc: 0.745763\n",
      "4942, train loss: 0.561051, test loss: 0.579952, train acc: 0.740964, test acc: 0.745763\n",
      "4943, train loss: 0.561043, test loss: 0.579942, train acc: 0.740964, test acc: 0.762712\n",
      "4944, train loss: 0.561035, test loss: 0.579932, train acc: 0.740964, test acc: 0.762712\n",
      "4945, train loss: 0.561026, test loss: 0.579922, train acc: 0.740964, test acc: 0.762712\n",
      "4946, train loss: 0.561018, test loss: 0.579912, train acc: 0.740964, test acc: 0.762712\n",
      "4947, train loss: 0.56101, test loss: 0.579902, train acc: 0.740964, test acc: 0.762712\n",
      "4948, train loss: 0.561002, test loss: 0.579892, train acc: 0.740964, test acc: 0.762712\n",
      "4949, train loss: 0.560994, test loss: 0.579882, train acc: 0.740964, test acc: 0.762712\n",
      "4950, train loss: 0.560986, test loss: 0.579872, train acc: 0.740964, test acc: 0.762712\n",
      "4951, train loss: 0.560978, test loss: 0.579861, train acc: 0.740964, test acc: 0.762712\n",
      "4952, train loss: 0.56097, test loss: 0.579851, train acc: 0.740964, test acc: 0.762712\n",
      "4953, train loss: 0.560962, test loss: 0.579841, train acc: 0.740964, test acc: 0.762712\n",
      "4954, train loss: 0.560954, test loss: 0.579831, train acc: 0.740964, test acc: 0.762712\n",
      "4955, train loss: 0.560946, test loss: 0.579821, train acc: 0.740964, test acc: 0.762712\n",
      "4956, train loss: 0.560937, test loss: 0.579811, train acc: 0.740964, test acc: 0.762712\n",
      "4957, train loss: 0.560929, test loss: 0.579801, train acc: 0.740964, test acc: 0.762712\n",
      "4958, train loss: 0.560921, test loss: 0.579791, train acc: 0.740964, test acc: 0.762712\n",
      "4959, train loss: 0.560913, test loss: 0.579781, train acc: 0.740964, test acc: 0.762712\n",
      "4960, train loss: 0.560905, test loss: 0.579771, train acc: 0.740964, test acc: 0.762712\n",
      "4961, train loss: 0.560897, test loss: 0.579761, train acc: 0.740964, test acc: 0.762712\n",
      "4962, train loss: 0.560889, test loss: 0.579751, train acc: 0.740964, test acc: 0.762712\n",
      "4963, train loss: 0.560881, test loss: 0.57974, train acc: 0.740964, test acc: 0.762712\n",
      "4964, train loss: 0.560873, test loss: 0.57973, train acc: 0.740964, test acc: 0.762712\n",
      "4965, train loss: 0.560865, test loss: 0.57972, train acc: 0.740964, test acc: 0.762712\n",
      "4966, train loss: 0.560857, test loss: 0.57971, train acc: 0.740964, test acc: 0.762712\n",
      "4967, train loss: 0.560849, test loss: 0.5797, train acc: 0.740964, test acc: 0.762712\n",
      "4968, train loss: 0.56084, test loss: 0.57969, train acc: 0.740964, test acc: 0.762712\n",
      "4969, train loss: 0.560832, test loss: 0.57968, train acc: 0.740964, test acc: 0.762712\n",
      "4970, train loss: 0.560824, test loss: 0.57967, train acc: 0.740964, test acc: 0.762712\n",
      "4971, train loss: 0.560816, test loss: 0.57966, train acc: 0.740964, test acc: 0.762712\n",
      "4972, train loss: 0.560808, test loss: 0.57965, train acc: 0.740964, test acc: 0.762712\n",
      "4973, train loss: 0.5608, test loss: 0.57964, train acc: 0.740964, test acc: 0.762712\n",
      "4974, train loss: 0.560792, test loss: 0.57963, train acc: 0.740964, test acc: 0.762712\n",
      "4975, train loss: 0.560784, test loss: 0.57962, train acc: 0.740964, test acc: 0.762712\n",
      "4976, train loss: 0.560776, test loss: 0.57961, train acc: 0.740964, test acc: 0.762712\n",
      "4977, train loss: 0.560768, test loss: 0.5796, train acc: 0.740964, test acc: 0.762712\n",
      "4978, train loss: 0.56076, test loss: 0.57959, train acc: 0.740964, test acc: 0.762712\n",
      "4979, train loss: 0.560752, test loss: 0.579579, train acc: 0.740964, test acc: 0.762712\n",
      "4980, train loss: 0.560744, test loss: 0.579569, train acc: 0.740964, test acc: 0.762712\n",
      "4981, train loss: 0.560736, test loss: 0.579559, train acc: 0.740964, test acc: 0.762712\n",
      "4982, train loss: 0.560727, test loss: 0.579549, train acc: 0.740964, test acc: 0.762712\n",
      "4983, train loss: 0.560719, test loss: 0.579539, train acc: 0.740964, test acc: 0.762712\n",
      "4984, train loss: 0.560711, test loss: 0.579529, train acc: 0.740964, test acc: 0.762712\n",
      "4985, train loss: 0.560703, test loss: 0.579519, train acc: 0.740964, test acc: 0.762712\n",
      "4986, train loss: 0.560695, test loss: 0.579509, train acc: 0.740964, test acc: 0.762712\n",
      "4987, train loss: 0.560687, test loss: 0.579499, train acc: 0.740964, test acc: 0.762712\n",
      "4988, train loss: 0.560679, test loss: 0.579489, train acc: 0.740964, test acc: 0.762712\n",
      "4989, train loss: 0.560671, test loss: 0.579479, train acc: 0.740964, test acc: 0.762712\n",
      "4990, train loss: 0.560663, test loss: 0.579469, train acc: 0.740964, test acc: 0.762712\n",
      "4991, train loss: 0.560655, test loss: 0.579459, train acc: 0.740964, test acc: 0.762712\n",
      "4992, train loss: 0.560647, test loss: 0.579449, train acc: 0.740964, test acc: 0.762712\n",
      "4993, train loss: 0.560639, test loss: 0.579439, train acc: 0.740964, test acc: 0.762712\n",
      "4994, train loss: 0.560631, test loss: 0.579429, train acc: 0.740964, test acc: 0.762712\n",
      "4995, train loss: 0.560623, test loss: 0.579419, train acc: 0.740964, test acc: 0.762712\n",
      "4996, train loss: 0.560615, test loss: 0.579409, train acc: 0.740964, test acc: 0.762712\n",
      "4997, train loss: 0.560607, test loss: 0.579399, train acc: 0.740964, test acc: 0.762712\n",
      "4998, train loss: 0.560599, test loss: 0.579389, train acc: 0.740964, test acc: 0.762712\n",
      "4999, train loss: 0.560591, test loss: 0.579379, train acc: 0.740964, test acc: 0.762712\n"
     ]
    }
   ],
   "source": [
    "# Introduco l'ottimizzatore SGD e scrivo il ciclo di ottimizzazione\n",
    "\n",
    "optimizer = torch.optim.SGD(lin.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(5000):\n",
    "    output = lin(Xt_train)\n",
    "    outtest = lin(Xt_test).detach()\n",
    "\n",
    "    test_loss = criterion(outtest, zt_test)\n",
    "    train_accuracy = (output.detach().argmax(dim=1) == zt_train.argmax(dim=1)).float().mean()\n",
    "    test_accuracy  = (outtest.argmax(dim=1) == zt_test.argmax(dim=1)).float().mean()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss = criterion(output, zt_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'{epoch}, train loss: {loss.item():g}, test loss: {test_loss:g}, train acc: {train_accuracy:g}, test acc: {test_accuracy:g}', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "11796295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.4487,  0.3409,  1.3740, -0.4157,  0.2694, -0.1289,  0.3048],\n",
       "         [ 0.1475, -0.5732, -0.5662,  0.0252,  0.6189, -0.0137, -0.4156],\n",
       "         [ 0.4051, -0.1149, -0.4891,  0.4592, -0.5195, -0.1258, -0.0861]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 1.4971, -0.5720, -0.8024], requires_grad=True)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(lin.parameters())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
